#,ID/Reference,Sentence,Comments,Classification
1,DNN1 (Tensorflow),"In addition, often in close collaboration with the Google Brain team, more than 50 teams at Google and other Alphabet companies have deployed deep neural networks using DistBelief in a wide variety of products, including Google Search [11], our advertising products, our speech recognition systems [50, 6, 46], Google Photos [43], Google Maps and StreetView [19], Google Translate [18], YouTube, and many others.",The need for a scalable system for deploying deep neural networks across a wide range of applications at Google,Key Motivating Factors
2,DNN1 (Tensorflow),"A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards","Emphasizes the portability and adaptability of the TensorFlow framework, highlighting the importance of a system that can handle different hardware setups. ",Critical Factors and Guidelines
3,DNN1 (Tensorflow),"In addition, often in close collaboration with the Google Brain team, more than 50 teams at Google and other Alphabet companies have deployed deep neural networks using DistBelief in a wide variety of products, including Google Search [11], our advertising products, our speech recognition systems [50, 6, 46], Google Photos [43], Google Maps and StreetView [19], Google Translate [18], YouTube, and many others.","Talks about the deployment of deep neural networks using TensorFlow's predecessor, DistBelief, across a diverse range of Google products. This highlights the practical relevance and scalability of the underlying distributed system.",Practical Evaluation Scenarios
4,DNN1 (Tensorflow),"Once a system has multiple devices, there are two main complications: deciding which device to place the computation for each node in the graph, and then managing the required communication of data across device boundaries implied by these placement decisions. (...) A future version of this white paper will have a comprehensive performance evaluation section of both the single machine and distributed implementations.",The need for an algorithm for mapping computations to devices and managing communication. These were ongoing limitations at the time the paper was written.,Tool Limitations and Challenges
5,DNN2 (MXNet),"The scale and complexity of machine learning (ML) algorithms are becoming increasingly large. Almost all recent ImageNet challenge [12] winners employ neural networks with very deep layers, requiring billions of floating-point operations to process one single sample. The rise of structural and computational complexity poses interesting challenges to ML system design and implementation.","The system aims to bridge the gap between imperative and declarative programming, allowing the user to express computations in a variety of styles.",Key Motivating Factors
6,DNN2 (MXNet),"Most ML systems embed a domain-specific language (DSL) into a host language (e.g. Python, Lua, C++). Possible programming paradigms range from imperative, where the user specifies exactly “how” computation needs to be performed, and declarative, where the user specification focuses on “what” to be done.","A critical factor is the support for both imperative and declarative programming styles, which allows users to choose the most suitable approach for their needs",Critical Factors and Guidelines
7,DNN2 (MXNet),not applicable,,Practical Evaluation Scenarios
8,DNN2 (MXNet),"Most ML systems embed a domain-specific language (DSL) into a host language (e.g. Python, Lua, C++).  (...)  Comparing to other open-source ML systems, MXNet provides a superset programming interface to Torch7, Theano, Chainer and Caffe, and supports more systems such as GPU clusters.","Offers a flexible programming model. MXNet blends imperative and declarative approaches, providing efficient memory allocation and targeting heterogeneous systems. MXNet distinguishes itself through its support for multiple host languages and its ability to fuse execution across different programming paradigms to the same backend",Tool Limitations and Challenges
9,DNN3 (GPipe),"We scale the architecture along two dimensions to stress the flexibility of GPipe: (i) along the depth by increasing the number of layers in the model and (ii) along the width by increasing the hidden dimension in the feed-forward layers and the number of attention heads (...) We notice that increasing the model capacity, from 400M params (T (6, 8192, 16)) to 1.3B (T (24, 8192, 16)), and further, to 6B (T (64, 16384, 32)), leads to significant quality improvements across all languages.","The desire to push the boundaries of model size in order to improve accuracy, showing a trend towards larger models with greater capabilities.",Key Motivating Factors
10,DNN3 (GPipe),"In many cases, increasing model capacity beyond the memory limit of a single accelerator has required developing special algorithms or infrastructure. These solutions are often architecture-specific and do not transfer to other tasks. To address the need for efficient and task-independent model parallelism, we introduce GPipe, a pipeline parallelism library that allows scaling any network that can be expressed as a sequence of layers.","The necessity for efficient model parallelism to overcome memory limitations when training large models, and seeks to provide an approach that is not specific to a single architecture or task.",Critical Factors and Guidelines
11,DNN3 (GPipe),"We demonstrate the advantages of GPipe by training large-scale neural networks on two different tasks with distinct network architectures: (i) Image Classification: We train a 557-million-parameter AmoebaNet model and attain a top-1 accuracy of 84.4% on ImageNet-2012, (ii) Multilingual Neural Machine Translation: We train a single 6-billion-parameter, 128-layer Transformer model on a corpus spanning over 100 languages and achieve better quality than all bilingual models.","Provides concrete practical scenarios of training large models with GPipe. It includes image classification with a 557 million parameter model and multilingual machine translation with a 6 billion parameter model. The evaluation included measuring accuracy on ImageNet and translation quality on a 100 language corpus. The evaluation also studied scalability, efficiency and communication cost",Practical Evaluation Scenarios
12,DNN3 (GPipe),"The naive model parallelism strategy leads to severe under-utilization due to the sequential dependency of the network. (...) In many cases, increasing model capacity beyond the memory limit of a single accelerator has required developing special algorithms or infrastructure. These solutions are often architecture-specific and do not transfer to other tasks.","This paper focuses on the limitations of naive model parallelism, highlighting under-utilization and lack of flexibility. GPipe addresses these by introducing pipeline parallelism with micro-batching to improve utilization and enable scaling, which provides a more flexible approach than prior architecture specific methods",Tool Limitations and Challenges
13,DNN4 (BytePS),"Data center clusters that run DNN training jobs are inherently heterogeneous. They have GPUs and CPUs for computation and network bandwidth for distributed training. However, existing distributed DNN training architectures, all-reduce and Parameter Server (PS), cannot fully utilize such heterogeneous resources. In this paper, we present a new distributed DNN training architecture called BytePS. BytePS can leverage spare CPU and bandwidth resources in the cluster to accelerate distributed DNN training tasks running on GPUs.",The motivation is rooted in the observation that existing methods are not optimal. It focuses on performance enhancements by using the resources more effectively by using both CPUs and GPUs.,Key Motivating Factors
14,DNN4 (BytePS),BytePS is a unified distributed DNN training acceleration system that achieves optimal communication efficiency in heterogeneous GPU/CPU clusters,"This paper focuses on communication efficiency in distributed training, particularly in heterogeneous environments. It introduces BytePS as a system that unifies different communication strategies, such as all-reduce and parameter servers, to achieve optimal performance. It also focuses on improving efficiency by offloading some operations to the CPU.",Critical Factors and Guidelines
15,DNN4 (BytePS),"We evaluate BytePS using six DNN models and three training frameworks in production data centers. The results show that with 256 GPUs, BytePS consistently outperform existing all-reduce and PS solutions by up to 84% and 245%, respectively.","This paper evaluates BytePS on six different DNN models (ResNet-50, VGG-16, Transformer, BERT, UGATIT, GPT-2) and three frameworks (TensorFlow, PyTorch, MXNet) using 8 to 256 GPUs. The evaluation was conducted in a production environment, demonstrating the real-world benefits of their approach. The key metric is the training speed compared to All-Reduce and Parameter Server baselines",Practical Evaluation Scenarios
16,DNN4 (BytePS),"For distributed training, there are two families of data parallelism approaches, i.e., all-reduce and Parameter Server (PS). In what follows, we introduce all-reduce and PS and analyze their communication overheads. (...) In all-reduce, no additional CPU machine is involved. Ring is the most popular all-reduce algorithm. All-reduce has been optimized for many years, and most state-of-the-art training speed records are achieved using all-reduce, including classical CNN-based ImageNet tasks [27, 36, 49, 73], RNN-based language modeling tasks [56], and the pre-training of BERT [26, 74]. (...) All-reduce has no way to utilize additional non-worker nodes, since it was designed for homogeneous setup.","This paper points out the inefficiencies of relying solely on GPUs for gradient aggregation, particularly when CPU resources and network bandwidth are underutilized. It introduces BytePS to make better use of these resources, noting that even without additional CPUs, BytePS outperforms all-reduce in practice due to a better intra-machine communication strategy",Tool Limitations and Challenges
17,DNN5 (GShard),"Neural network scaling has been critical for improving the model quality in many real-world machine learning applications with vast amounts of training data and compute. Although this trend of scaling is affirmed to be a sure-fire approach for better model quality, there are challenges on the path such as the computation cost, ease of programming, and efficient implementation on parallel devices. (...) In this section, we advocate how conditional computation [45, 46] with sparsely gated mixture of experts [16] fits into the above detailed desiderata and show its efficacy by scaling neural machine translation models beyond 1 trillion parameters, while keeping the training time of such massive networks practical.","The motivation is to make very large models feasible to train, and the desire to improve the quality of machine translation through the use of massive models, particularly for the challenge of translating between many different languages.",Key Motivating Factors
18,DNN5 (GShard),"Neural network scaling has been critical for improving the model quality in many real-world machine learning applications with vast amounts of training data and compute. Although this trend of scaling is affirmed to be a sure-fire approach for better model quality, there are challenges on the path such as the computation cost, ease of programming, and efficient implementation on parallel devices.","Emphasizes the challenges of scaling models and the need for efficient, automatic sharding to make the training of very large models feasible. It also highlights the importance of conditional computation with sparsely gated mixture of experts to keep training time practical. The paper also touches on the importance of training efficiency.",Critical Factors and Guidelines
19,DNN5 (GShard),GShard enabled us to scale up multilingual neural machine translation Transformer model with Sparsely-Gated Mixture-of-Experts beyond 600 billion parameters using automatic sharding. We demonstrate that such a giant model can efficienctly be trained on 2048 TPU v3 accelerators in 4 days to achieve far superior quality for translation from 100 languages to English compared to the prior art.,"The practical scenario involves training a 600 billion parameter multilingual translation model on 2048 TPU v3 accelerators within four days. The evaluation focuses on the model quality for translation between 100 languages and English. This paper explores the trade-offs between model size, training efficiency, and translation quality",Practical Evaluation Scenarios
20,DNN5 (GShard),There is a lack of support for efficient model parallelism algorithms under commonly used deep learning frameworks such as TensorFlow [21] and PyTorch [22]. Naive model parallelism with graph partition is supported but it would lead to severe under-utilization due to the sequential dependency of the network and gradient based optimization.,The paper identifies the lack of good model parallelism support in mainstream frameworks and points out that naive graph partitioning is insufficient for training very large models. GShard tackles this by providing a more flexible system through lightweight annotations that allow diverse parallelization strategies.,Tool Limitations and Challenges
21,DNN6 (Pytorch DDP),"Deep Neural Networks (DNN) have powered a wide spectrum of applications, ranging from image recognition [20], language translation [15], anomaly detection [16], content recommendation [38], to drug discovery [33], art generation [28], game play [18], and self-driving cars [13]. Many applications pursue higher intelligence by optimizing larger models using larger datasets, craving advances in distributed training systems. Among existing solutions, distributed data parallel is a dominant strategy due to its minimally intrusive nature. (...) During the past year, we have seen significant adoption both internally and externally. Within Facebook, a workload study from 05/11/20 to 06/05/20 shows that more than 60% of production GPU hours during that period were spent on the PyTorch distributed data parallel package across a wide variety of applications, including speech, vision, mobile vision, translation, etc.","Motivated by the need for a practical solution to distributed training that is easy to use, evidenced by its wide use within Facebook. The focus is less on theoretical advancement and more on practical implementation in Pytorch.",Key Motivating Factors
22,DNN6 (Pytorch DDP),"Recent advances in deep learning argue for the value of large datasets and large models, which necessitates the ability to scale out model training to more computational resources. Data parallelism has emerged as a popular solution for distributed training thanks to its straightforward principle and broad applicability.","This paper focuses on optimizing data parallelism in PyTorch, highlighting techniques such as bucketing gradients, overlapping communication with computation, and skipping gradient synchronization. It emphasizes the need for minimal intrusion on the user's code.",Critical Factors and Guidelines
23,DNN6 (Pytorch DDP),"As of v1.5, PyTorch natively provides several techniques to accelerate distributed data parallel, including bucketing gradients, overlapping computation with communication, and skipping gradient synchronization. Evaluations show that, when configured appropriately, the PyTorch distributed data parallel module attains near-linear scalability using 256 GPUs. (...) All 4 servers reside in the same rack, and each server is equipped with 8 NVIDIA Tesla V100 GPUs. Fig. 5 shows the interconnection of the 8 GPUs within the same server. We only use the shared entitlement when a set of experiments require more than 32 GPUs. (...) We measure DDP per iteration latency and scalability using two popular models, ResNet50 [20] and BERT [15], to represent typical vision and NLP applications.",This paper evaluates the PyTorch DDP module using ResNet50 and BERT models on a 32 GPU cluster and a shared entitlement with up to 256 GPUs. The key evaluation metrics include per-iteration latency and scalability. The paper also examines the impact of various optimization techniques such as bucketing gradients and overlapping computation with communication. The evaluation aims to demonstrate that PyTorch DDP is capable of achieving near-linear scalability when properly configured,Practical Evaluation Scenarios
24,DNN6 (Pytorch DDP),"Despite the conceptual simplicity of the technique, the subtle dependencies between computation and communication make it non-trivial to optimize the distributed training efficiency. (...) Based on our observations, there is no single configuration that would work for all use cases, as it would highly depend on the model size, model structure, network link bandwidth, etc.","This paper highlights the challenges in optimizing distributed data parallel training due to the complex interactions between computation and communication, which varies depending on model architecture and available hardware. It emphasizes the need for empirical tuning, and also mentions that layer dropping techniques are not easily accelerated in distributed data parallel training due to fixed bucket mappings",Tool Limitations and Challenges
25,DNN7 (Colossal AI),"Methods such as PipeDream [25], GPipe [16], and Chimera [20] were proposed to split the model into several chunks of consecutive layers and each chunk is allocated to a device as shown in Figure 3c. Intermediate activations and gradients are passed between pipeline stages to complete the forward and backward pass. As a result, this method reduces cross-node communication. Pipeline parallelism allows multiple devices to compute simultaneously, leading to a higher throughput. (...) Inspired by Alpa, Colossal-AI has included an experimental automatic parallelism feature to improve upon the Alpa project.","To create a unified system for large-scale parallel training, addressing the limitations of existing methods by incorporating various parallelism techniques and automated features.",Key Motivating Factors
26,DNN7 (Colossal AI),"Methods such as PipeDream [25], GPipe [16], and Chimera [20] were proposed to split the model into several chunks of consecutive layers and each chunk is allocated to a device as shown in Figure 3c. Intermediate activations and gradients are passed between pipeline stages to complete the forward and backward pass. As a result, this method reduces cross-node communication. Pipeline parallelism allows multiple devices to compute simultaneously, leading to a higher throughput. (...) DeepSpeed’s static policy will still offload all model data to the CPU memory, leading to low memory efficiency and high communication overhead. Instead, Colossal-AI will dynamically determine whether a tensor should be placed on GPU or CPU depending on the memory availability.","Colossal-AI as a framework for unified large-scale parallel training, encompassing data, pipeline, and tensor parallelism. The emphasis is on dynamic memory management (sharding and offloading) and automatic parallelization, addressing the limitations of existing solutions like DeepSpeed.",Critical Factors and Guidelines
27,DNN7 (Colossal AI),"To demonstrate the capability of dynamic tensor placement in ColossalAI, we trained GPT-2 model with 10 billion parameters on the Wikipedia dataset on System II. We set the batch size to 4 and scaled the data parallel training from 1 GPU to 8 GPU.",This paper evaluates Colossal-AI using a variety of experiments including training a 10 billion parameter GPT-2 model on the Wikipedia dataset and training a 13 billion parameter OPT model. The experiments aim to show the effectiveness of dynamic tensor placement and its advantage over static offloading methods like DeepSpeed.,Practical Evaluation Scenarios
28,DNN7 (Colossal AI),"One drawback of pipeline parallel training is that there will be some bubble time, where some devices are idle when others are engaged in computation, leading to the waste of computational resources. (...) DeepSpeed’s static policy will still offload all model data to the CPU memory, leading to low memory efficiency and high communication over-head. (...) During heterogeneous training, Colossal-AI’s hybrid Adam optimizer monitors the available memory space on the GPU. It does not statically keep all FP32 weights in the CPU memory, instead, it dynamically keeps part of parameters and gradients on the GPU as long as there is space left.","This paper points out the limitations of pipeline parallelism due to potential resource under-utilization, and static memory management policies. Colossal-AI tries to address these issues by dynamically determining where a tensor should be placed based on memory availability.",Tool Limitations and Challenges
29,DNN8 (Ray),"In our evaluation, we study the following questions: 1. How well does Ray meet the latency, scalability, and fault tolerance requirements listed in Section 2? (Section 5.1) 2. What overheads are imposed on distributed primitives (e.g., allreduce) written using Ray’s API? (Section 5.1) 3. In the context of RL workloads, how does Ray compare against specialized systems for training, serving, and simulation? (Section 5.2) 4. What advantages does Ray provide for RL applications, compared to custom systems? (Section 5.3)","The need for a general-purpose distributed framework to support the diverse requirements of emerging AI applications such as reinforcement learning, which require a combination of training, serving, and simulation",Key Motivating Factors
30,DNN8 (Ray),"To learn a policy, an agent typically employs a two-step process: (1) policy evaluation and (2) policy improvement. To evaluate the policy, the agent interacts with the environment (e.g., with a simulation of the environment) to generate trajectories, where a trajectory consists of a sequence of (state, reward) tuples produced by the current policy.","Ray is a distributed framework designed to support diverse AI workloads, particularly reinforcement learning. It highlights the importance of fault tolerance and general workload handling as critical factors. It's a more general framework than the others, not strictly focused on training efficiency.",Critical Factors and Guidelines
31,DNN8 (Ray),"In our experiments, we demon-strate scaling beyond 1.8 million tasks per second and better performance than existing specialized systems for several challenging reinforcement learning applications","Demonstrates Ray's capabilities in handling large-scale, diverse AI workloads by achieving 1.8 million tasks per second. It evaluates Ray's performance and scalability with benchmarks of distributed primitives, such as allreduce. It further compares Ray against specialized systems in the context of reinforcement learning training, serving and simulation.",Practical Evaluation Scenarios
32,DNN8 (Ray),"While in principle one could develop an end-to-end solution by stitching together several existing systems (e.g., Horovod [53] for distributed training, Clipper [19] for serving, and CIEL [40] for simulation), in practice this approach is untenable due to the tight coupling of these components within applications. As a result, researchers and practitioners today build one-off systems for specialized RL applications [58, 41, 54, 44, 49, 5]. (...) To satisfy these requirements, Ray implements a unified interface that can express both task-parallel and actorbased computations.","The Ray paper identifies that the complexity of combining existing systems for distributed AI applications is untenable and that scheduling is difficult without full knowledge of the computation. Ray seeks to address these by providing a unified interface for distributed computing, including scheduling, fault tolerance and data movement",Tool Limitations and Challenges
33,DNN9 (DeepSpeed),"DeepSpeed is compatible with PyTorch. One piece of our library, called ZeRO, is a new parallelized optimizer that greatly reduces the resources needed for model and data parallelism while massively increasing the number of parameters that can be trained. Researchers have used these breakthroughs to create Turing Natural Language Generation (Turing-NLG), which at the time of its release was the largest publicly known language model at 17 billion parameters.",To improve the performance and usability of deep learning training for large models (to make large models practical).,Key Motivating Factors
34,DNN9 (DeepSpeed),"The latest trend in AI is that larger natural language models provide better accuracy; however, larger models are difficult to train because of cost, time, and ease of code integration. With the goal of advancing large model training by improving scale, speed, cost, and usability for model developers across the world, Microsoft made the DeepSpeed library open source in February of 2020. Since then, the DeepSpeed team has been hard at work extending the library to continue pushing the boundaries of scale and speed of deep learning training.","DeepSpeed is a tool for training large models, emphasizing the need for system-level optimizations to improve scalability, speed, and cost. The paper shows the library's capability.",Critical Factors and Guidelines
35,DNN9 (DeepSpeed),not applicable,,Practical Evaluation Scenarios
36,DNN9 (DeepSpeed),not applicable,,Tool Limitations and Challenges
37,DNN10 (Horovod),"Existing methods for enabling multi-GPU training under the TensorFlow library entail non-negligible communication overhead and require users to heavily modify their model-building code, leading many researchers to avoid the whole mess and stick with slower single-GPU training. In this paper we introduce Horovod, an open source library that improves on both obstructions to scaling: it employs efficient inter-GPU communication via ring reduction and requires only a few lines of modification to user code, enabling faster, easier distributed training in TensorFlow.",This is motivated by the need to address the complexity of distributed deep learning in existing frameworks and make it more accessible to researchers and practitioners by using HPC techniques.,Key Motivating Factors
38,DNN10 (Horovod),"In early 2017 Baidu published an article [8] evangelizing a different algorithm for averaging gradients and communicating those gradients to all nodes (Steps 2 and 3 above), called ring-allreduce (...) allows worker nodes to average gradients and disperse them to all nodes without the need for a parameter server (...)  This algorithm is bandwidth-optimal, meaning that if the buffer is large enough, it will optimally utilize the available network.","Focuses on the ring-allreduce algorithm as a way to improve communication efficiency (avoids using a parameter server, making it easier to set up and use distributed training)",Critical Factors and Guidelines
39,DNN10 (Horovod),not applicable,,Practical Evaluation Scenarios
40,DNN10 (Horovod),"There are a few areas that we are actively working on to improve Horovod, including: Collecting and sharing learnings about adjusting model parameters for distributed deep learning: Facebook’s paper [6] describes the adjustments needed to model hyperparameters to achieve the same or greater accuracy in a distributed training job compared to training the same model on a single GPU, demonstrating the feasibility of training a TensorFlow model on 256 GPUs. We believe this area of deep learning research is still in its early stages and hope to collaborate with other teams about approaches to further scale deep learning training.",This paper acknowledges that hyperparameter tuning for distributed training is complex and not fully understood.,Tool Limitations and Challenges
41,DNN11 (Megatron-LM),"Natural Language Processing (NLP) is advancing quickly in part due to an increase in available compute and dataset size. The abundance of compute and data enables training increas-ingly larger language models via unsupervised pretraining... Empirical evi-dence indicates that larger language models are dramatically more useful for NLP tasks such as article completion, ques-tion answering, and natural language inference (...) In summary, our approach as de-scribed above is simple to implement, requiring only a few extra all-reduce operations added to the forward and back-ward pass. It does not require a compiler, and is orthogonal and complementary to the pipeline model parallelism advo-cated by approaches such as (Huang et al., 2018).","The need to train large language models to advance natural language processing tasks, and the limitations of data parallelism for very large models. Model parallelism is overcomes those limitations.",Key Motivating Factors
42,DNN11 (Megatron-LM),"Our approach is to utilize model parallelism to split the model across multiple accelerators. This not only alleviates the memory pressure, but also increases the amount of parallelism independently of the microbatch size. (...) By increasing the minibatch size proportionally to the number of available workers (i.e. weak scaling), one observes near linear scaling in training data throughput. (...) We show that careful attention to the placement of layer normalization in BERT-like models is critical to achieving increased accuracies as the model grows. (...) We exploit the inherent structure in transformer based language models to make a simple model-parallel implementation that trains efficiently in PyTorch, with no custom C++ code or compiler required.","This paper introduces a hybrid model and data parallelism technique to train large language models, showing that scaling model size improves accuracy. It touches on challenges of model parallelism and provides a framework for large model training without the need for a compiler.",Critical Factors and Guidelines
43,DNN11 (Megatron-LM),Using the GPT-2 model we achieve SOTA results on the WikiText103 (10.8 compared to SOTA perplexity of 15.8) and LAMBADA (66.5% compared to SOTA accuracy of 63.2%) datasets. Our BERT model achieves SOTA results on the RACE dataset (90.9% compared to SOTA accuracy of 89.4%) (...) We demonstrate that scaling the model size results in improved accuracies for both GPT-2 (studied up to 8.3 billion parameters) and BERT (studied up to 3.9B parameters) models.,This paper evaluates model parallelism by training GPT-2 (up to 8.3B parameters) and BERT (up to 3.9B parameters) models and showcasing improved accuracy with increased size. Evaluation includes perplexity on WikiText103 and accuracy on LAMBADA and RACE datasets.,Practical Evaluation Scenarios
44,DNN11 (Megatron-LM),"However, large batch training introduces complications into the optimization process that can result in reduced accuracy or longer time to convergence, offsetting the benefit of increased training throughput. (...) for BERT models, careful attention to the placement of layer normalization in BERT-like models is critical to achieving increased accuracies as the model size increases.","This paper highlights the challenges of large batch training and the importance of model architecture and configurations when scaling up, specifically for BERT models and the correct placement of layer normalisation.",Tool Limitations and Challenges
45,DNN12 (Transformers),"An increasingly important goal of Transformers is to make it easy to efficiently deploy model to pro-duction. Different users have different production needs, and deployment often requires solving sig-nificantly different challenges than training. The library thereforce allows for several different strate-gies for production deployment.
One core propery of the libary is that models are available both in PyTorch and TensorFlow, and there is interoperability between both frameworks. ","The need for a user-friendly open-source library for state-of-the-art NLP models, making them easily accessible for downloading, fine-tuning and production use",Key Motivating Factors
46,DNN12 (Transformers),"Each model is made up of a Tokenizer, Transformer, and Head. The model is pretrained with a fixed head and can then be further fine-tuned with alternate heads for different tasks.","The need for a unified library that provides state-of-the-art Transformer models, tokenizers, and tools for NLP, making it easy for researchers and practitioners to access and use these models.",Critical Factors and Guidelines
47,DNN12 (Transformers),not applicable,,Practical Evaluation Scenarios
48,DNN12 (Transformers),not applicable,,Tool Limitations and Challenges
,,,,
,,,,
,,,,
,,,,

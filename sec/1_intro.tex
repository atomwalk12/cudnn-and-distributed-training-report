% ===== STEP 1: Getting Started =====
% This section covers:
% - Goals and expected outputs
% - Initial constraints
% - Search terms and keywords
\section{Introduction}
\label{sec:intro}

\begin{table*}[htbp]
	\centering
	\caption{A summary of existing surveys on the two topics.}
	\label{tab:related_surveys}
	\begin{tabular}{|p{5cm}|p{11cm}|}
		\toprule
		\textbf{Survey Name}                                                                                & \textbf{Survey Description}                                                                                                                         \\
		\midrule
		{\small Deep Learning frameworks for large-scale data mining~\cite{nguyen_machine_2019}}            & This study emphasizes the connection between deep learning and massively parallelism to efficiently handle Big Data computations.                   \\
		\midrule
		{\small From Distributed Machine to Distributed Deep Learning~\cite{dehghani_distributed_2023}}     & This study emphasizes the connections between distributed machine learning and traditional machine learning methods.                                \\
		\midrule
		{\small Hitchhikerâ€™s Guide On Distributed Training of Deep NNs~\cite{chahal_hitchhikers_2018}}      & This study focuses on the relevant implementation details for training distributed neural networks. It details how data and model parallelism work. \\
		\midrule
		{\small Survey on Distributed Deep Learning Frameworks for Big Data~\cite{berloco_systematic_2022}} & The review compares distributed deep learning approaches based on parallelism techniques, hardware support and framework compatibility.             \\
		\bottomrule
	\end{tabular}
\end{table*}
\subsection{Background}

\textbf{Deep Neural Networks.}
DNNs have powered a wide range of applications in areas such as computer vision,
natural language processing, audio processing, graph knowledge representation and time series
analysis \cite{noauthor_papers_nodate}. One of the key driving factors that enable the application
of deep learning models in such a wide range of domains, while still ensuring impressive
performance, is the ability to scale raw computational power and datasets to impressive amounts
\cite{ben-nun_demystifying_2020, dehghani_distributed_2023, langer_distributed_2020}. Distributed
Neural Networks (DNNs) provide techniques that enable scaling both these factors, effectively
enabling models to learn complex relationships and excel in a wide range of domains.

\textbf{GPU Programming.}
At the heart of distributed training stand frameworks that enable efficient calculations of matrix
multiplication operations in parallel through the use of specialized GPUs. At the dawn of the
machine learning revolution, AlexNet \cite{krizhevsky_imagenet_2012} (also called ConvNet), showed
that deep learning models can effectively outperform human-level performance on the ImageNet
dataset. They used two GTX 580 GPUs to train a network over a period of five to six days. The
network achieved top-1 error rate of 37.5\%, outperforming previous state-of-the-art results at the
time, while their success sparked significant interest in the community and spurred a wave of
research.

\textbf{Importance of the Topic.}
It follows that the effectiveness of the deep neural networks is influenced by breakthroughs in both domains,
which in turn have an impact on both research and industry applications \cite{chahal_hitchhikers_2018, xing_strategies_2015}.
To help create a more unified view of each area, in this study I will highlight the most significant frameworks
in both domains, identify trends over a 13 year timeline and to ease-out the introduction by providing
two end-to-end examples, which will hopefully facilitate a more intuitive understanding of the topics.

% TODO Razvan
% \begin{enumerate}
%     \item What distributed techniques have been developed for parallelizing stochastic descent backpropagation?
%     \begin{itemize}
%         \item How does CUDA enable data parallelization on single-machine GPUs?
%         \item What methods are available for parallelizing learning across PC clusters, particularly using PyTorch's DDP?
%     \end{itemize}

%     \item How do these techniques compare in terms of convergence speed and computational efficiency?
%     \begin{itemize}
%         \item What are the performance differences between CUDA-based GPU implementations and CPU-only approaches?
%         \item How does data parallelization scale in GPU clusters, and what are its limitations?
%     \end{itemize}

%     \item What are the primary challenges and solutions in implementing distributed training systems?
%     \begin{itemize}
%         \item What are the key considerations when implementing cluster parallelization using DDP?
%         \item How can data parallelization be effectively implemented on GPUs using CUDA?
%     \end{itemize}
% \end{enumerate}

% First introduce the workflow

% TODO Razvan
% Our investigation includes practical implementations and experimental work:
% \begin{itemize}
%     \item Cluster parallelization demonstrations using PyTorch's DDP module
%     \item GPU implementations with CUDA, including:
%     \begin{itemize}
%         \item Code examples showing data parallelization on GPUs
%         \item Comparative implementations using CPUs
%         \item Performance analysis and benchmarking
%     \end{itemize}
% \end{itemize}

\paragraph{Paper Organization.}
The remainder of this paper is organized as follows: Section \ref{sec:related_work} introduces
similar work already present in the literature, Section \ref{sec:protocol} presents this study's
systematic review protocol documenting primarily the search process and the selection of relevant
studies, Section \ref{sec:reading-studies} describes the methodology used to identify key
information from the selected studies. Section \ref{sec:translate-concepts} presents the
identification of the relationships between the two topics, Section \ref{sec:practical-evaluation}
briefly presents the experiments which were performed to evaluate the selected frameworks, Section
\ref{sec:report-findings} reports the findings with respect to the research questions, and finally
Section \ref{sec:conclusion} concludes with future work and learning outcomes.
To improve survey clarity, this paper's methodology is outlined in Figure \ref{fig:workflow}.

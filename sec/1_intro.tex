% ===== STEP 1: Getting Started =====
% This section covers:
% - Goals and expected outputs
% - Initial constraints
% - Search terms and keywords
\section{Introduction}
\label{sec:intro}

\subsection{Background}
Machine learning (ML) has become essential for extracting knowledge from data across diverse
applications. Deep learning, a subfield of ML using artificial neural networks, is increasingly
important, especially with the massive amounts of data now available
\cite{ben-nun_demystifying_2020}, \cite{dehghani_distributed_2023}, \cite{langer_distributed_2020}.
Distributed deep learning (DDL) has become crucial due to the increasing size of datasets and model
complexity \cite{berloco_systematic_2022}, \cite{dehghani_distributed_2023}. This literature review
focuses on the core concepts, techniques, and frameworks used to implement and optimise DDL.
Challenges in scaling deep learning include distributing ML programs, bridging computation with
communication, and determining what to communicate between machines \cite{xing_strategies_2015}.
This systematic review will provide a comprehensive overview of the current state of DDL and
address these questions, identify gaps, and give direction for future research
\cite{langer_distributed_2020}, \cite{ben-nun_demystifying_2020}, \cite{dehghani_distributed_2023},
\cite{verbraeken_survey_2021} \TODO{Review citations}.

\subsection{Importance of the Topic}
\label{sec:importance_of_topic}
The increasing volume of data necessitates advanced analysis techniques, making distributed deep
learning essential. Efficiently training deep learning models is vital for research and development
\cite{dehghani_distributed_2023}, \cite{chahal_hitchhikers_2018}, \cite{xing_strategies_2015}. This
review is important for both practitioners and researchers, providing an overview of techniques and
frameworks for implementing DDL solutions \cite{dehghani_distributed_2023},
\cite{berloco_systematic_2022} and also highlighting current research trends and future
opportunities \cite{berloco_systematic_2022}, \cite{ben-nun_demystifying_2020}. This review
addresses gaps in existing surveys by focusing on practical implementation and a detailed analysis
of the performance and efficiency of different DDL techniques \cite{berloco_systematic_2022}
\TODO{review citation}. The outcomes of this review will aid software engineers in selecting and
implementing appropriate DDL techniques.

\subsection{Research Questions}
\label{sec:initial_research_questions}
A set of research questions were determined prior to initiating the review process. However, the questions
were subsequently refined as a result of piloting the review protocol. The initial questions are listed below,
however will be expanded on in Section \ref{sec:research_questions}.

\begin{itemize}
	\item How does data parallelization work in GPU clusters and what are the limitations of these tools?
	\item How effective is the learning algorithm when parallelized using CUDA compared to relying solely on CPUs?
\end{itemize}


% TODO Razvan
% \begin{enumerate}
%     \item What distributed techniques have been developed for parallelizing stochastic descent backpropagation?
%     \begin{itemize}
%         \item How does CUDA enable data parallelization on single-machine GPUs?
%         \item What methods are available for parallelizing learning across PC clusters, particularly using PyTorch's DDP?
%     \end{itemize}

%     \item How do these techniques compare in terms of convergence speed and computational efficiency?
%     \begin{itemize}
%         \item What are the performance differences between CUDA-based GPU implementations and CPU-only approaches?
%         \item How does data parallelization scale in GPU clusters, and what are its limitations?
%     \end{itemize}

%     \item What are the primary challenges and solutions in implementing distributed training systems?
%     \begin{itemize}
%         \item What are the key considerations when implementing cluster parallelization using DDP?
%         \item How can data parallelization be effectively implemented on GPUs using CUDA?
%     \end{itemize}
% \end{enumerate}

\subsection{Scope and Methodology}
% First introduce the workflow
This systematic review follows a rigorous methodology outlined in Figure \ref{fig:workflow}. The
workflow consists of seven main steps, starting with getting started (Step 1), through to the final
wrap-up of the report (Step 7). Reproducibility and transparency of the review process is a
priority. As a result multiple artifacts will be produced at each phase. These artifacts are
reported separately in the appendix.

\TODO{Too many subsections}

% TODO Razvan
% Our investigation includes practical implementations and experimental work:
% \begin{itemize}
%     \item Cluster parallelization demonstrations using PyTorch's DDP module
%     \item GPU implementations with CUDA, including:
%     \begin{itemize}
%         \item Code examples showing data parallelization on GPUs
%         \item Comparative implementations using CPUs
%         \item Performance analysis and benchmarking
%     \end{itemize}
% \end{itemize}

\subsection{Paper Organization}
The remainder of this paper is organized as follows: Section \ref{sec:protocol} presents our
systematic review protocol including the study selection framework and search process
documentation, Section \ref{sec:methods} describes our systematic review methodology, including the
search strategy across digital libraries, inclusion/exclusion criteria, and quality assessment
protocol. Section \ref{sec:results} presents our findings, Section \ref{sec:discussion} discusses
the implications, and Section \ref{sec:conclusion} concludes with future research directions. This
structure ensures a logical flow from background to findings and recommendations.

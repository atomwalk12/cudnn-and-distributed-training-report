% ===== STEP 1: Getting Started =====
% This section covers:
% - Goals and expected outputs
% - Initial constraints
% - Search terms and keywords
\section{Introduction}
\label{sec:intro}

\begin{table*}[htbp]
	\centering
	\caption{A summary of existing surveys on the two topics.}
	\label{tab:related_surveys}
	\begin{tabular}{|p{5cm}|p{11cm}|}
		\toprule
		\textbf{Survey Name} & \textbf{Survey Description}  \\
		\midrule
		{\small Deep Learning frameworks for large-scale data mining~\cite{nguyen_machine_2019}} & This study emphasizes the connection between deep learning and massively parallelism to efficiently handle Big Data computations. \\
		\midrule
		{\small From Distributed Machine to Distributed Deep Learning~\cite{dehghani_distributed_2023}} & This study emphasizes the connections between distributed machine learning and traditional machine learning methods.  \\
		\midrule
		{\small Hitchhikerâ€™s Guide On Distributed Training of Deep NNs~\cite{chahal_hitchhikers_2018}} & This study focuses on the relevant implementation details for training distributed neural networks. It details how data and model parallelism work.  \\
		\midrule
		{\small Survey on Distributed Deep Learning Frameworks for Big Data~\cite{berloco_systematic_2022}} & The review compares distributed deep learning approaches based on parallelism techniques, hardware support and framework compatibility.\\
		\bottomrule
	\end{tabular}
\end{table*}
\subsection{Background}
Deep Neural Networks have powered a wide spectrum of applications in areas such as computer vision,
natural language processing, audio progressing, graph knowledge representation and time series
analysis \cite{noauthor_papers_nodate}. One of the key driving factors that enable the application
of deep learning models in such a wide range of domains, while still ensuring formidable
state-of-the-art performance, is the ability to scale-out models to millions or billions of
parameters. It follows that efficient training techniques to distribute the training process across
multiple machines are essential. One constraint to using such large models is that the amount of
data required to train them must be diverse, high-quality and in large quantities
\cite{ben-nun_demystifying_2020, dehghani_distributed_2023, langer_distributed_2020}. Distributed
Neural Networks (DNNs) enable the training of large models in distributed environments, effectively
parallelizing both the model as well as the data in a way that makes the training process tractable
to a huge number of parameters or a large amount of data.

On the other hand, it is valuable to inquire what methods were initially used to parallelize model
training on GPUs. For instance, \cite{krizhevsky_imagenet_2012} showed how AlexNet, a deep learning
model for image classification, can parallelize training using two GTX 580 GPUs in five to six
days. The network achieved top-1 error rate of 37.5\% on the ImageNet dataset, significantly better
than previous state-of-the-art results at the time. The success of AlexNet sparked significant
interest in the community and spurred a wave of research.

This literature review focuses on the core concepts and libraries used to implement and optimise
DNNs. At the same time, key proprietary frameworks are identified (primarily from NVIDIA, AMD and
Intel) that provide the building blocks for parallelizing the training process on GPUs. I will also
provide practical guidance on running a small neural network on the GPU using the \texttt{cuDNN}
primitives, included in the CUDA toolkit. Furthermore, will also demonstrate how it is possible to
simulate a multi-GPU distributed environment for executing multi-node training using Docker, which
will be done using the PyTorch Distributed Data Parallel (\texttt{DDP}) library.


\subsection{Importance of the Topic}
\label{sec:importance_of_topic}
Efficiently training deep learning models is vital both in academic as well as industry settings
\cite{chahal_hitchhikers_2018, xing_strategies_2015}. The impetus for conducting a review
is due to multiple factors. As far as GPU programming is concerned, sheer curiosity for the topic is
the driving motivation; however, the Distributed Neural Networks (DNN) task has more practical benefits.
Specifically, understanding the underlying technologies can help practitioners and researchers make more
informed decisions when conducting their work.

\subsection{Research Questions}
\label{sec:initial_research_questions}
A pair of research questions were determined prior to initiating the review. However, the questions
were subsequently refined as a result of piloting the review protocol. The initial questions are listed below,
however will be expanded on in Section \ref{sec:research_questions}.

\begin{itemize}
	\item How does data parallelization work in GPU clusters and what are the limitations of these tools?
	\item How effective is the learning algorithm when parallelized using CUDA compared to relying solely on
	      CPUs?
\end{itemize}

% TODO Razvan
% \begin{enumerate}
%     \item What distributed techniques have been developed for parallelizing stochastic descent backpropagation?
%     \begin{itemize}
%         \item How does CUDA enable data parallelization on single-machine GPUs?
%         \item What methods are available for parallelizing learning across PC clusters, particularly using PyTorch's DDP?
%     \end{itemize}

%     \item How do these techniques compare in terms of convergence speed and computational efficiency?
%     \begin{itemize}
%         \item What are the performance differences between CUDA-based GPU implementations and CPU-only approaches?
%         \item How does data parallelization scale in GPU clusters, and what are its limitations?
%     \end{itemize}

%     \item What are the primary challenges and solutions in implementing distributed training systems?
%     \begin{itemize}
%         \item What are the key considerations when implementing cluster parallelization using DDP?
%         \item How can data parallelization be effectively implemented on GPUs using CUDA?
%     \end{itemize}
% \end{enumerate}

\subsection{Scope and Methodology}
% First introduce the workflow
This systematic review follows a rigorous methodology outlined in Figure \ref{fig:workflow}. In
order to ensure partial reproducibility many artifacts are reproduced throughout the review
process. These artifacts are reported separately in the appendix.

% TODO Razvan
% Our investigation includes practical implementations and experimental work:
% \begin{itemize}
%     \item Cluster parallelization demonstrations using PyTorch's DDP module
%     \item GPU implementations with CUDA, including:
%     \begin{itemize}
%         \item Code examples showing data parallelization on GPUs
%         \item Comparative implementations using CPUs
%         \item Performance analysis and benchmarking
%     \end{itemize}
% \end{itemize}

\paragraph{Paper Organization.}
The remainder of this paper is organized as follows: Section \ref{sec:related_work} introduces
similar work already present in the literature, Section \ref{sec:protocol} presents our systematic
review protocol (documenting primarily the search process), Section \ref{sec:methods} describes our
systematic review methodology, including the search strategy across digital libraries,
inclusion/exclusion criteria, and quality assessment protocol. Section \ref{sec:results} presents
our findings, Section \ref{sec:discussion} discusses the implications, and Section
\ref{sec:conclusion} concludes with future research directions. This structure ensures a logical
flow from background to findings and recommendations. \TODO{...}


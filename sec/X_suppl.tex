\newpage
\onecolumn
\setcounter{page}{1}
\maketitlesupplementary


% TODO Razvan: remove this table
% \begin{table*}[htbp]
%     \centering
%     \caption{Search Terms by Category for Distributed Deep Learning and GPU Programming}
%     \label{tab:search_terms}
%     \begin{tabularx}{\textwidth}{|l|X|X|}
%         \hline
%         \textbf{Facet} & \textbf{Distributed Deep Learning Terms} & \textbf{GPU Programming Terms} \\
%         \hline
%         Core Concept & 
%         \textbf{"Distributed Deep Learning"}, \textbf{"Parallel Deep Learning"}, 
%         "Deep Learning on Clusters", "Large-Scale Deep Learning", 
%         "Scalable Deep Learning" &
%         \textbf{"GPU Programming"}, \textbf{"General-Purpose GPU Programming"}, 
%         \textbf{"GPGPU Programming"} \\
%         \hline
%         Specific Technology / 
%         Parallelization Techniques & 
%         \textbf{"Data Parallelism"}, \textbf{"Model Parallelism"}, 
%         \textbf{"Hybrid Parallelism"}, "Data-Parallel", "Model-Parallel" &
%         \textbf{"CUDA"}, \textbf{"CUDA Programming"}, "Nvidia CUDA", 
%         "Compute Unified Device Architecture" \\
%         \hline
%         Training Methods / 
%         Programming Aspects & 
%         \textbf{"Stochastic Gradient Descent"}, \textbf{"SGD"}, "Mini-batch SGD", 
%         "Asynchronous SGD", "Synchronous SGD", "Distributed Stochastic Gradient Descent", 
%         "Elastic Averaging SGD", "Byzantine-tolerant gradient descent" &
%         "Parallel Computing", "Parallel Programming", "High-Performance Computing", 
%         "Kernel Programming", "GPU Memory Management", "GPU Optimisation", 
%         "CUDA Libraries" \\
%         \hline
%         Communication Strategies & 
%         \textbf{"Parameter Server"}, \textbf{"All-Reduce"}, 
%         \textbf{"Collective Communication"}, "Decentralized Optimization", 
%         "Decentralized Parameter Sharing", "Gradient Compression", 
%         "Sparse Communication" & -- \\
%         \hline
%         Frameworks & 
%         "TensorFlow", "PyTorch", "Horovod", "DistBelief", "Parameter Server", 
%         "SparkNet", "Petuum", "BigDL", "MXNet", "CaffeOnSpark" &
%         \textbf{"CUDA Toolkit"}, \textbf{"cuDNN"}, "TensorRT", "Thrust", 
%         "OpenACC", "RAPIDS", "PyCUDA", "Numba", "JAX", "TensorFlow with CUDA", 
%         "PyTorch with CUDA", "Caffe with CUDA", "Theano with CUDA", 
%         "MxNet with CUDA", "Darknet with CUDA" \\
%         \hline
%         Hardware & 
%         "GPUs", "CPUs", "Accelerators", "Cluster Computing", "Supercomputers", 
%         "Multi-GPU" &
%         "GPUs", "Nvidia GPUs", "Multi-GPU", "CUDA-enabled GPUs" \\
%         \hline
%         Performance Aspects & 
%         "Scalability", "Convergence", "Latency", "Communication Overhead", 
%         "Fault Tolerance" &
%         "GPU Acceleration", "Parallel Speedup", "Throughput", 
%         "Memory Bandwidth", "Latency", "Performance Optimisation" \\
%         \hline
%     \end{tabularx}
%     \caption*{Note: Bold terms indicate primary search terms that will be prioritized in the search strategy. A dash (--) indicates no specific terms for that category.}
% \end{table*}


\input{tables/passages_dnn}

\input{tables/passages_gpu}

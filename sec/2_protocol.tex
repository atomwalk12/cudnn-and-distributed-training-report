% ===== STEP 2: Define Search Strategy =====
% This section covers:
% - Search strategy development
% - Documentation of search process

\section{Related Work}
\label{sec:related_work}
For documenting the review process, this study follows primarily the guidelines laid out in
\cite{keele_systematic_2007}, however advice for conducting the review is synthesized from a wider
range of related articles
\cite{brereton_lessons_2007-1,kitchenham_procedures_nodate,budgen_reporting_2018,dos_santos_sustainable_2024}.

\textbf{Limitations of existing work.}
A summary of existing work in the field is summarized in Table \ref{tab:related_surveys}.
Our study differs from previous surveys in a few different aspects. First, concerning DNNs,
related work focuses on techniques and algorithms for training models across multiple machines
\cite{dehghani_distributed_2023, chahal_hitchhikers_2018, berloco_systematic_2022}, where themes
such as data and model parallelization techniques and communication protocols are explored. As a
result, existing literature tackles architectural patterns and design choices, none focusing
explicitly on providing a broad review of the available frameworks. Secondly, although existing
repositories do provide examples on how to use the CUDA library
\cite{noauthor_nvidiacuda-samples_2025}, as well as DDP
\cite{noauthor_examplesdistributedddpreadmemd_nodate}, none provide end-to-end implementations that
would allow the community to easily build on. This study attempts to answer both of these concerns.

Another relevant article which aims to provide an overview of massive parallel frameworks available
for deep learning is \cite{nguyen_machine_2019}. It describes specialized tools for hardware
accelerators (GPUs, FPGAs, TPUs), however does not focus on auxiliary libraries for linear algebra,
numerical computing and GPU communication, gap which this study aims to fill.

\begin{figure*}[th]
	\centering
	\includegraphics[width=\linewidth]{figures/workflow2}
	\caption{The workflow is divided into three main phases: main workflow (top), studies selection (bottom left), and
		evaluation (bottom right). Dashed lines indicate documentation and communication flows.}
	\label{fig:workflow}
\end{figure*}

\paragraph{Key concepts.}
Below are summarized some of the key DNN concepts that the rest of this review builds on:

\begin{itemize}
	\item \textbf{Data Parallelism:}
	      The dataset is divided across multiple nodes, with each node training a complete copy of the
	      model on its portion of data. Gradients from all nodes are then combined to update the model parameters.
	      This approach can be implemented either synchronously (all nodes wait for each other) or asynchronously (nodes work independently).

	\item \textbf{Model Parallelism:}
	      The neural network model itself is divided across different nodes, with each node responsible
	      for computing a specific portion of the model architecture. This strategy is particularly useful
	      when the model is too large to fit on a single machine.

	\item \textbf{Pipeline Parallelism:}
	      The training process is divided into sequential stages, similar to an assembly line,
	      where the output of one stage becomes the input for the next. This allows different parts
	      of the model to train simultaneously while maintaining dependencies.

	\item \textbf{Hybrid Parallelism:}
	      This approach combines multiple parallelization strategies to optimize training efficiency.
	      For example, model parallelism might be used to distribute a large model across GPUs, while
	      data parallelism is applied to each model segment.
\end{itemize}

These approaches can be further enhanced through techniques such as gradient compression, mixed
precision training, and tensor fusion \cite{dehghani_distributed_2023}. The choice of specific
techniques depends on factors including model architecture, available hardware, and training
requirements. For a comprehensive review of these techniques and their implementations, readers are
referred to \cite{chahal_hitchhikers_2018}.

\section{Research method}
\label{sec:protocol}

Multiple studies emphasize that a literature survey should be both transparent and replicable
\cite{keele_systematic_2007, dos_santos_sustainable_2024-1}, as this can ensure that reviewer bias
is minimized. Generally, a literature survey is conducted by a group of people working together to
assess the literature and involves multiple iterations. In this paper, steps have been taken to
mitigate this problem, nonetheless it is acknowledged that it is not possible to eliminate bias
completely. To ensure this, the process is documented and most of the resulting artifacts are
shown.

% TODO these are all good but was hoping to minimize the number of citations
% TODO \cite{keele_systematic_2007,brereton_lessons_2007-1,budgen_reporting_2018, dos_santos_sustainable_2024-1},

% Now detail Step 1 content
The main workflow is displayed in Figure \ref{fig:workflow} where the key phases are annotated as
follows: Getting Started (M.1, M.2), Planning the Review (M.3, M.4), Conducting the Review (M.5,
M.6), and Reporting the Review (M.7, M.8).

\paragraph{M.1 -- The need for a survey}
\label{sec:need_for_survey}
The importance follows from the need for a survey that discusses both general frameworks as well as
the connection between the two topics. Also, by supplying practical end-to-end implementations, this
survey can prove useful to students or practitioners that would like to apply the techniques in practice.
It has been shown in Section \ref{sec:related_work} that a survey that focuses on both topics is lacking
in the literature.

\begin{figure*}[th]
	\centering
	\includegraphics[width=\linewidth]{figures/survey-dnn.pdf}
	\caption{The diagram shows the series of steps carried out in the planning and study selection
		phases for the DNNs survey.}.
	\label{fig:workflow-study-dnn}
\end{figure*}

\textbf{Learning outcomes.}
Conducting a review is also a good way of becoming accustomed with the two topics. By following a
systematic approach, it becomes easier to find relevant frameworks and research publications which
is a skill that can be applied to other domains as well.

\paragraph{M.2 -- Getting started}
\label{sec:research_questions}
The main goal is to analyze parallelization frameworks in DNNs and GPU programming. This entails two distinct
constraints: (i) consider only frameworks that introduce GPU programming and DNN frameworks and (ii) include
only primary studies.

\subsection{M.3 -- Selection of Relevant Studies}
This step identified relevant papers and is composed of two distinct processes: (i) S.1. literature
survey on DNN frameworks and (ii) S.2. literature survey on GPU programming libraries.

\subsubsection{S.1 -- Literature Survey on DNN libraries}
The steps involved in this process are shown in Figure \ref{fig:workflow-study-dnn}.

\paragraph{S.1.1 -- Problem definition.}
In order to identify relevant studies, I conducted a secondary study\footnote{A secondary study
	synthesizes primary papers to provide a comprehensive overview. This contrasts with tertiary
	studies which analyze secondary studies.}. This decision was made in the problem definition step
(S.1.1). To ensure a methodical approach, this section defines a research protocol which formally
defines the key attributes of the search process. This basically entails defining the research
questions and expanding on the search strategy.

\paragraph{S.1.2 -- Research questions.}
After an initial literature review, the research questions (RQ) are defined using the guidelines
from \cite{kitchenham_evidence-based_2015} and \cite{keele_systematic_2007}. Specifically, the
PICOC (Population, Intervention, Comparison, Outcome, Context) criterion is used to write the
questions into a format that ensures them to be specific, measurable and well-defined.

% TODO: remove this
% \begin{itemize}
% 	\item \textbf{RQ\textsubscript{1}} What are the most common frameworks currently available for 
% 		  GPU programming with CUDA, and how do their usability compare?
% 		  % NOTE implementing distributed deep learning, and how does their usability compare?
% 	      % NOTE \cite{berloco_systematic_2022, ben-nun_demystifying_2020, langer_distributed_2020}?
% 	      % NOTE \item How do parameter update strategies impact distributed deep learning systems (e.g., Parameter Server and decentralised approaches) \cite{ben-nun_demystifying_2020,berloco_systematic_2022,langer_distributed_2020}?
% 	\item How is stochastic gradient descent (SGD) computed in distributed environments
% 	      \cite{berloco_systematic_2022,ben-nun_demystifying_2020,langer_distributed_2020,verbraeken_survey_2021}? % NOTE and what are the associated challenges 
% 	\item What are the key frameworks currently available for implementing DDL, and how do their features 
% 	      compare \cite{berloco_systematic_2022}?
%  	\item In what ways are the techniques used in DDL also useful in GPU parallelization?
% \end{itemize}

% TODO: \TODO{RQ1: ease of learning, ease of use and documentation compare?}
\label{sec:research_questions_refined}
\begin{itemize}
	\item \textbf{RQ\textsubscript{1}:} What are the most commonly cited
	      frameworks for distributed neural network training, and how do their communities vary in size? \\
	      \textit{Rationale:} By identifying the most common frameworks, we can trace the years in which they were published
	      and form a unified timeline of the evolution of the field.

	\item \textbf{RQ\textsubscript{2}:} What are the most frequently cited
	      frameworks for GPU programming, and how do their communities differ in size?\\
	      \textit{Rationale:} By identifying the most common frameworks, we can identify which are the gaps
	      in the literature they cover and which are the most promising areas for future research.

	\item \textbf{RQ\textsubscript{3}:} Which are the overlaps and shared limitations of these areas? \\
	      \textit{Rationale:} By identifying the overlaps, this can lead to a more comprehensive understanding of the field.

	\item \textbf{RQ\textsubscript{4}:} How can these technologies be applied in practice? \\
	      \textit{Rationale:} This can yield hands-on experience on the topic which is helpful for practical applications.

\end{itemize}

\paragraph{S.1.3 -- Search Strategy.}
The search strategy represents a systematic approach for identifying relevant studies that
adequately answer the research questions.

\textbf{Databases.}
The process involves a manual search of three citation databases --
\href{https://www.scopus.com/}{Scopus}, \href{https://www.semanticscholar.org/}{Semantic Scholar}
and \href{https://arxiv.org/}{arXiv}\footnote{Other relevant databases that could have been used
	include: \href{https://ieeexplore.ieee.org/}{IEEE Xplore}, \href{https://dl.acm.org/}{ACM Digital
		Library} and \href{https://www.sciencedirect.com/}{Science Direct}.} -- that include conference
proceedings and journal papers, considering three metadata fields (title, abstract, and keywords).

\textbf{Inclusion/Exclusion Criteria.}
There were defined three inclusion criteria (IC) and three exclusion criteria (EC). In particular, I
decided to select only primary studies, however secondary studies were mentioned in Section
\ref{sec:related_work}. The identification of secondary studies was useful since the selected
studies synthesize evidence and can make it possible to access primary studies:

\begin{itemize}
	\item \textbf{IC\textsubscript{1}}: Study is a primary study.
	\item \textbf{IC\textsubscript{2}}: Study addresses distributed frameworks in DL.
	\item \textbf{IC\textsubscript{3}}: Study introduces a library or a framework. \\
	\item \textbf{EC\textsubscript{1}}: Study does not discuss implementation details.
	\item \textbf{EC\textsubscript{2}}: Study is not a primary study.
	\item \textbf{EC\textsubscript{3}}: Study is not written in English.
\end{itemize}

\textbf{Search terms.}
In Step S.1.3, I inquired about the literature using the following search string:
\begin{quote}
	\textit{( "machine learning" OR "deep learning" )
		AND
		( "Data parallelism" OR "model parallelism")
		AND
		( "framework" OR "implementation" )}
\end{quote}

\begin{figure*}[th]
	\centering
	\includegraphics[width=\linewidth]{figures/survey-cuda3.pdf}
	\caption{The diagram shows the series of steps carried out in the planning and study selection
		phases for the GPU programming survey.}.
	\label{fig:workflow-study-cuda}
\end{figure*}

\paragraph{Publication year criteria.}
For the DDL task, papers were considered between the time period $\yearstartddl$-$\yearendddl$. The
start year ($\yearstartddl$) was chosen due to being the year where there was a shift towards
resource conservation, which resulted in a focus on concurrency within mini-batches
\cite{ben-nun_demystifying_2020}. This is the year by which the effectiveness of deep learning
algorithms was more widely recognized and more research was published that focused on scalability.

\paragraph{S1.4 - S1.5 -- Semi-Automatic selection.}
After an initial database search (Step S1.4), 182 studies were retrieved. Subsequently, as part of
S.1.5, two methods were applied for reducing the number of studies to a more manageable set. Below,
I describe each approach, including my evaluation regarding their effectiveness.

\textbf{Swift-Review.}
As suggested by \cite{bolanos_artificial_2024},
I applied Swift-Review \cite{Howard2016SWIFTReviewAT}, a machine learning classifier to filter out
irrelevant studies (Step S.1.5). The technique is semi-automatic
with a focus on screening and extraction of relevant information. Prior to initiating the classification,
I had to manually identify 10 positive and 10 negative examples. These
acted as input seed samples for the classifier, which helped pinpoint other useful material. Then
the classifier identified other relevant papers by screening the title, abstract and keywords of
each study. All papers with a confidence score above 0.5 were selected.

\textbf{Classification using Gemini.}
The downside of the previous approach is that it still involves a lot of work to manually identify key
studies. To address this, \cite{bolanos_artificial_2024} suggests many emerging tools that use Large Language Models (LLMs)
to automatically classify relevant material. However, one important factor that limits their usability and effectiveness is the
context window being relatively small to handle a large corpus of text. Nonetheless, Gemini \cite{team_gemini_2024}
is an effective tool to classify the studies, a particularly strong choice due to its massive
context window (over 2M tokens). The drawback is that Gemini does not provide citations when using the web interface
\cite{noauthor_gemini_nodate}. However, NotebookLM \cite{notebooklm_google_2024} solves the issue by providing references
to portions of the text that are relevant to the query. This initial screening is an iterative process that involves
reading the abstracts and keywords of each study to ascertain about the reliability of each response.

\textbf{Results.}
Swift-Review is a more traditional method while NotebookLM is a more recent tool. The former used
to be a viable screening tool before the advent of LLMs. However, Gemini being a much larger model,
provides a faster and more reliable screening process by giving the user the ability to actively
check the results.

\paragraph{S.1.6 - S.1.8 -- Manual selection.}
Step S.1.6 involved a manual inspection over the the title, abstract and keywords. This resulted in
a total of 13 studies that appeared to be relevant. After checking their full text, this number was
reduced to 4. I performed backward snowballing \cite{jalali_systematic_2012} by revisiting the
references of the 4 studies, as well as checking available preprint articles and finally identified
other 8 studies (Step 1.8). Hence, a total of 12 studies (4 + 8) were selected. A subsequent 5
frameworks were found with no accompanying papers by reviewing the references. All 17 libraries are
listed in Table \ref{tab:dnn_papers}.

\paragraph{S.1.9 -- Quality assessment.}
To evaluate the quality of these studies (Step S.1.9), I adapted the quality appraisal instrument
suggested by \cite{zhou_map_2016}, considering 2 main aspects: report and relevance. Regarding
report, I checked whether the studies clearly tackled the problem, research questions and
inclusion/exclusion criteria defined in Steps S.1.1, S.1.2 and S.1.3 respectively. Concerning
relevance, I verified whether the studies presented relevant information to ensure their value for
students and practitioners. Finally, all 17 studies identified in the previous steps passed the
quality checks.

\subsubsection{S.2 -- Survey on GPU programming libraries}
\label{sec:gpu-programming-libraries}
This step involved identifying popular frameworks that facilitate programming on the GPU. The
workflow that was followed is shown in Figure \ref{fig:workflow-study-cuda}.

\paragraph{S.2.1 -- Problem definition.}
It was not possible to perform a systematic review in the traditional sense due to the nature of
the available libraries. The available frameworks are frequently proprietary and are rarely
accompanied by academic papers. Also, the libraries are implementation-focused with documentation
and tutorials being the main source of information. As a result, I had to widen out the type of
articles to include in the review by referencing useful tutorials and key documentation pages.

\paragraph{S.2.2 -- Research Questions.}
The relevant research questions -- RQ\textsubscript{2}, RQ\textsubscript{3} and RQ\textsubscript{4}
-- are listed in Section \ref{sec:research_questions_refined}.

\paragraph{S.2.3 -- Search strategy.}
To find relevant materials (Github repositories, documentation pages and tutorials),
\href{https://github.com/search/advanced}{Advanced Github Search} is used by searching for keywords
related to the main GPU manufacturers: AMD, Intel and NVIDIA.

\input{tables/dnn_studies.tex}
\input{tables/gpu_studies.tex}
\textbf{Inclusion/Exclusion criteria.}
The following criteria were identified to guide the selection process:

\begin{itemize}
	\item \textbf{IC\textsubscript{1}}: The material addresses GPU programming.
	\item \textbf{IC\textsubscript{2}}: The material is official documentation/repository.
	\item \textbf{IC\textsubscript{3}}: The material is a tutorial.
	\item \textbf{IC\textsubscript{4}}: The material introduces a library or framework. \\
	\item \textbf{EC\textsubscript{1}}: The material is not a primary source.
	\item \textbf{EC\textsubscript{2}}: The material is not written in English.
\end{itemize}
\input{tables/timeline.tex}
\textbf{Search terms.}
Search terms similar to the following ones are used to refine the search process:

\begin{quote}
	\textit{user:ROCm user:oneAPI-SRC user:NVIDIA}
\end{quote}

\textbf{Publication year criteria.}
The GPU programming search was restricted to papers published between
$\yearstartcuda$-$\yearendcuda$. The start year ($\yearstartcuda$) was chosen as the baseline due
to being the year when AlexNet \cite{krizhevsky_imagenet_2012} was published. This paper
revolutionized research in neural networks by allowing advanced AI models to be trained on GPUs.

\paragraph{S.2.4 - S.2.8 -- Study Selection Framework.}
\label{sec:ai-screening}
At this step, I used DeepSeek \cite{noauthor_deepseek_nodate} to identify relevant GPU programming
keywords and categories (Step S.2.4). This led to identifying \cite{noauthor_enccsgpu-programming_nodate}, which
offers an excellent introduction to the topic covering general aspects of GPU programming as well
as specific frameworks.

% ===== STEP 3: Selection of Relevant Studies =====
% This section details: 
% - Study 1: Distributed learning techniques
% - Study 2: CUDA implementations

Given the relevant keywords, I followed an iterative approach using search engines, Github
repositories and documentation pages to identify relevant material (Step S.2.5). This lead to
identifying 21 libraries (S.2.6). The libraries had only 2 accompanying papers
\cite{chetlur_cudnn_2014,okuta_cupy_2017} in the literature (S.2.7). The quality assessment step
(S.2.8) ensured that the libraries are relevant to training neural networks and the resulting
papers are shown in Table \ref{tab:gpu_papers}. The categories that were included belong to the
main GPU manufacturers: AMD, Intel and NVIDIA. Across each category, libraries for algebraic
operations (matrix multiplications) include \cite{noauthor_cublas_nodate,noauthor_rocmrocblas_2025,
	noauthor_uxlfoundationonemath_2025}. These are used as building blocks for more complex deep
learning primitives as shown in
\cite{chetlur_cudnn_2014,noauthor_rocmmiopen_2025,onednn_contributors_oneapi_2025}. Existing
approaches that bridge the gap between GPU programming and DNNs are related to communication
protocols such as
\cite{noauthor_nvidianccl_2025,noauthor_rocmrccl_2025,noauthor_uxlfoundationoneccl_2025}. These
frameworks are used in distributed training to synchronize calculations by providing a low-level
APIs that manages cross-GPU communication. Moreover, cross-platform and high-level libraries are
also reported as they allow to effectively build upon core libraries in a faster and more efficient
manner.

The column concerning the NN libraries \cite{Jia.EtAl_2014a,krizhevsky_imagenet_2012,
	Goodfellow.EtAl_2013,Collobert.EtAl_} represent pioneering frameworks that implemented GPU
acceleration prior to the advent of optimized frameworks such as cuDNN.

\section{M.4 -- Reading the studies}
\label{sec:reading-studies}
Figure \ref{fig:timeline} shows a timeline of the evolution of the field. Also, Figure \ref{fig:taxonomy}
shows a taxonomy of the libraries in a unified, hierarchical view.

\subsection{S.1 -- Distributed Neural Networks}
\label{sec:dnn-studies}

In order to more easily extract useful information from the studies, I identified four key criteria
which aim to facilitate answering the research questions defined in Section
\ref{sec:research_questions_refined}.

\begin{itemize}
	\item C1: Key Motivating Factors
	\item C2: Critical Factors and Guidelines
	\item C3: Practical Evaluation Scenarios
	\item C4: Tool Limitations and challenges
\end{itemize}

The following text draws conclusions from various passages present in the studies, and for
convenience the resulting artifacts are available in the supplementary material in Table
\ref{tab:dnn_passages}. Each contributing factor is associated with an exact passage and various
codes (categories) were used to classify the passage. The studies are summarized below.

\textbf{Motivating Factors.}
The motivating factors for training DNNs include the pursuit of better performance through more
efficient training and increased usability in practical scenarios. It has been widely regarded that
by scaling the architectures to a large number of parameters and leveraging larger datasets, the
evaluation accuracy on many benchmarks would be improved \cite{hestness_deep_2017}. However, it was
recently demonstrated by Deepseek R1
\cite{deepseekai2025deepseekr1incentivizingreasoningcapability} that scaling up the computational
power is not the only possible method of progress. As a result, future research is likely to focus
not only on distributed training, but also on innovating existing architectures.

Nonetheless, optimizing speed is crucial because by training larger models, performance is likely
to improve \cellrefs{D103} and as a result many applications become viable \cellrefs{D105}, which
otherwise would be infeasible to train. \input{tables/taxonomy} It has been shown through empirical
evidence that increasing the size of the model or dataset, leads to better performance
\cellrefs{D102,D105,D111}. This has been applied in industrial settings \cellrefs{D101,D106}, and
large scale training requirements have been formalized in \cellrefs{D109,D110}.

Apart from scalability, performance is also a key motivating factor \cellrefs{D103,D105}. Some
studies have shown that increasing the depth of the network overcomes performance bottlenecks
\cellrefs{D103}, ensuring applicability in a wide range of domains. Lastly, resource utilization
also plays an important role in practical scenarios where clusters typically contain heterogeneous
hardware \cellrefs{D104}.

Beyond performance and scale, ease of programming and accessability to a wide range of uses also
play a crucial role. This involves simplifying workflows and enabling access to advanced techniques
to a wider audience \cellrefs{D110,D111,D112}. Nonetheless, one obstacle for usability is the
requirement to evolve existing frameworks and support an increasing number of applications as user
requirements change \cellrefs{D106}. Furthermore, cross-platform and cross-framework support remain
crucial for wider adoption and flexibility \cellrefs{D109,D112}, while at the same time enforcing
innovation through scientific curiosity \cellrefs{D102}.

\textbf{Critical Factors.}
There were considered several critical factors to effectively develop large machine learning systems.
Performance and scalability are among the central concerns \cellrefs{D203,D206,D207,D209,D212}. These
can further be trimmed down to efficiency (not waste resources), speed and the ability to handle
a wide range of architectural choices in distributed environments.

Usability and ease of use are also increasingly considered important for broader adaption and
developer productivity, which includes intuitive programming paradigms and simplified workflows
\cellrefs{D202,D205,D209,D211,D212}.

There are also other factors such as cost and communication efficiency which were also considered
\cellrefs{D209}. Particularly, they have been shown to represent a bottleneck \cellrefs{D204,D210},
and thus are also important.

To facilitate all these factors, effective hardware utilization (using both the CPU and GPU) plays
an important role \cellrefs{D201}, as well as software engineering principles like separation of
concerns \cellrefs{D205}, further contribute to robust library design.

\textbf{Evaluation Scenarios.}
Generally, evaluation is done by assessing task performance metrics, which includes accuracy
across various tasks like image classification, vision or NLP \cellrefs{D303,D305,D306,D308,D311}.
Although the measurements regarding the speed, efficiency or resource utilization are important
\cellrefs{D306,D307,D308,D311}, scalability is a key evaluation metric that is broadly used in the
literature \cellrefs{D306,D308,D311}. In particular, studies such as \cellrefs{D301} assess
deployment in real-world scenarios.

Lately, some algorithms have also focused on cross-framework evaluation to ensure broader
applicability \cellrefs{D304}.

\textbf{Limitations and Challenges.}
The main challenges faced by DNNs include communication overhead and resource under-utilization
\cellrefs{D401,D403,D404,D405,D407,D410}. The lack of standardized tools and frameworks lead
to difficulties regarding programming complexity and ease of use \cellrefs{D402,D403,D408}.

There exist high-level optimization challenges that prevent achieving peak performance, due to
different architectures and hardware configurations \cellrefs{D406,D411}. Moreover, there exist
also algorithmic limitations, tight coupling, and issues related to debugging code
\cellrefs{D405,D406,D408,D411}, which shows that there is still large room for improvement.

Finally, the requirement to manually tune applications to find out optimal parameter configurations
points towards the need to create more user-friendly machine learning frameworks \cellrefs{D411}.

\subsection{S.2 -- GPU Programming}
\label{sec:gpu-studies}
\paragraph{Motivating Factors.}
Below are summarized the motivating factors for the development of GPU programming libraries,
making use of the characteristics displayed in Table \ref{tab:gpu_passages}.

\begin{figure*}
	\centering
	\begin{subfigure}{0.48\linewidth}
		\includegraphics[width=\textwidth]{figures/mindmap}
		\caption{Accuracy for the EasyVQA training split.}
		\label{fig:base-a}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.48\linewidth}
		\includegraphics[width=\textwidth]{figures/mindmap-cuda}
		\caption{Accuracy for the DAQUAR training split.}
		\label{fig:base-b}
	\end{subfigure}
	\caption{Accuracy during training/validation on the baseline generator models.}
	\label{fig:base}
\end{figure*}
\textbf{Scalability and performance optimizations.}
The development of GPU programming libraries is primarily driven by the need to increase
performance \cellrefs{G1013,G1031,G1051}. This is most often done through focused
optimizations involving the learning kernels, which relate to the ability to perform
large matrix multiplications quicker and reducing the required amount of auxiliary memory. These
improvements are intrinsically linked to scalability and the ability to develop larger and more
powerful architectures \cellrefs{G1011,G1012,G1071}. 

\textbf{Compatibility.}
Another important concern is related to the integration and compatibility of GPU libraries with
existing frameworks \cellrefs{G1013,G1014,G1015,G1062}. This is particularly important as seamless
compatibility streamlines development and promotes wider adoption. As an example, Caffe \cite{Jia.EtAl_2014a}
particularly emphasizes how time-consuming developing optimized code for individual architectures is.
To combat this, frameworks such as CuDNN~\cite{chetlur_cudnn_2014} aim to provide optimized primitives
for NVidia GPUs across multiple GPU architectures.

\textbf{Usability and programming experience.}
Another important aspect is usability and its effects on programming productivity \cellrefs{G1031,G1071}.
Usability emphasizes the creation of tools accessible to a broader audience, which in turn leads to
being able to build more complex applications \cellrefs{G1012,G1016,G1017,G1061}.

\paragraph{Critical factors.}
Performance remains as the key concern encompassing not only raw speed but also portability across
different hardware architectures \cellrefs{G2011,G2012,G2021}.

\textbf{Scalability.}
Scalability is equally important for handling increasingly complex models and datasets \cellrefs{G2011,G2041}.
One factor that greatly contributes to scalability is the ability to fully utilize the available resources
across heterogeneous hardware. This poses significant design constraints as it requires to integrate
both CPUs and GPUs simultaneously during training \cellrefs{G2021,G2041}.

Another important factor that influences scalability is to create modular code through the
separation of concerns as some libraries have highlighted \cellrefs{G2012,G2041}.

One aspect that plays a role in scalability is how efficient inter-GPU communication is in
multi-GPU setups \cellrefs{G2051}. As we shall see this is tightly linked to the choice of the
communication library and is a key concern for both GPU programming and DNN frameworks
\cite{noauthor_uxlfoundationoneccl_2025,noauthor_nvidianccl_2025,noauthor_rocmrccl_2025}.

\textbf{Usability.}
Moreover, broader adoption requires libraries that are easy to use, as this ensures developer productivity
\cellrefs{G2041}.

Also, usability is facilitated by the adoption of declarative programming styles and a focus on
high-level design which excluded boiler-plate code as much as possible \cellrefs{G2012,G2061}.

\paragraph{Evaluation.}
Both quantitative and qualitative studies are performed to assess the performance.

\textbf{Quantitative evaluation.}
Specifically, quantitative metrics assess solutions based on measured performance like convolution speed
and general throughput, frequently benchmarked against established methods \cellrefs{G3011,G3013}.
One approach to assess raw performance involves varying mini-batch sizes against popular neural network
architectures \cellrefs{G3011}. This is related also to portability as it allows to reach top
performance without imposing constraints on architecture choices or the hyperparameters used
\cellrefs{G3012,G3013,G3061}. This is a metric to assess generalization capabilities.

\textbf{Generalization capabilities.}
Given that generalization is important, the documents also touch on the effectiveness of the neural
networks in different domains (i.e. reinforcement learning, computer vision, NLP, etc.), which
ensures broad applicability~\cellrefs{G3012,G3061}.

To ensure fair assessment, the libraries are often evaluated in deployed environments and serve
practical real world applications~\cellrefs{G3041}.

\textbf{Qualitative evaluation.}
These methods are used to assess subjective aspects like the visual quality of the results, as it was
seen in AlexNet~\cellrefs{G3051}. Lastly, it is crucial to keep in mind the context of the evaluation,
as they are usually tailored to specific model architectures and application areas. As a result,
there is always a certain degree of subjectivity involved in the assessment~\cellrefs{G3031}.

\paragraph{Limitations and Challenges.}
Despite progress, GPU programming faces multiple challenges, which arguably can be attributed to
the lack of a large open-source ecosystem and the use of closed-source proprietary hardware.

\textbf{Developer effort.}
One significant concern is that the kernel optimizations are time-consuming and require specialized
expertise about the dedicated GPU architecture \cellrefs{G4012,G4041}. Also, unless using standardized
libraries (i.e. cuDNN~\cite{chetlur_cudnn_2014}), the replication of results can be challenging as
execution times can vary significantly \cellrefs{G4041}.

\textbf{Performance bottlenecks.}
Performance bottlenecks are due to small matrix operations, which are generally inefficient \cellrefs{G4051,G4061},
but also due to communication overhead in cross-GPU communication \cellrefs{G4051}.

Algorithmic and memory constraints are also present, with some algorithms being limited by high
memory usage \cellrefs{G4013}, and others requiring specialized implementations for various corner
cases \cellrefs{G4013}.

\textbf{Evolving architectures.}
The field also faces ongoing challenges with evolving architectures, which involves both hardware advancements
but also new neural network innovations, requiring continuous adaptation~\cellrefs{G4011}.

\section{M.5 -- Translate concepts}
\label{sec:translate-concepts}
The translated results that highlight the relationships between the two topics are shown in Tables
\ref{tab:translations_motivating_factors}, \ref{tab:translations_critical_factors} and
\ref{tab:translations_evaluation_metrics}.

\subsection{M.6 -- Relationship between concepts}

\paragraph{Motivating factors.}\
Below I analyze the relationships between the two topics, while leveraging rows MF1-MF7 from Table
\ref{tab:translations_motivating_factors}.

\textbf{MF1. Scalability.}
The connection is that scalability is a major shared motivating factor for both DNNs and GPU programming.
The increasing scale of data and complexity of DNNs necessitates scalable solutions. GPU programming is
motivated by providing the tools and optimizations needed to achieve this scalability, enabling DNNs to
handle larger workloads, improve productivity, and become more cost-effective.

\textbf{MF2. Complexity and performance.}
The key shared motivator is to manage computational complexity while at the same time ensure higher accuracy
in common applications (i.e. NLP tasks). The increased accuracy is due to the guarantee that performance
is likely to increase thanks to the scaling laws that neural networks exhibit.
GPU programming directly provides the necessary primitives to facilitate the scaling laws and DNNs build on top of them.

\textbf{MF3. Critical in many domains.}
GPU programming itself might be considered a more specialized domain, but its motivation is connected to the
critical applicability of DNNs across many fields. GPU programming enable DNNs to function efficiently in
these domains by providing the building blocks. In this case, NVidia provides the optimized libraries, as
they know the hardware better than the general community.

\textbf{MF4. Heterogeneous hardware.}
While DNNs are motivated to use heterogenous hardware to ensure broader applicability and performance, GPU programming
acknowledge its importance by providing C APIs for CPU-GPU communication. Nonetheless, there do exist limitations
due to latency and sub-optimal bandwidth utilization in both domains. A related concern is that GPU libraries like cuDNN do not provide
integrated support for multi-GPU training, which must be achieved manually by the user. As a result there exist areas
for improvement in both domains.

\textbf{MF5. Applications.}
A shared motivation is to simplify development and improve the practical utility of both DNNs and GPU programming.
Both fields are driven by the need to make life easier for developers to leverage parallel hardware effectively,
and the motivation to fulfill practical requirements of simplified deployment and reproducible research.

\textbf{MF6. Leveraging existing tools.}
DNN development benefits from open-source frameworks, ensuring community-driven innovation. Conversely, GPU programming,
while often proprietary, builds on-top of other low-level libraries like cuBLAS. This shows a reliance on other
proprietary software, which although ensures high-performance, can arguably hinder innovation in the long term.

\textbf{MF7. Cross-framework use.}
Open-source DNN framework aim for cross-framework compatibility and usability to foster community innovation.
GPU programming libraries show a trade-off between low-level, highly optimized primitives (like cuDNN), and offering
user-friendly interfaces (like CuPy and Torch7). Code sharing is important in both areas, however there is limited
scope for GPU programming due to proprietary software.

\paragraph{Critical factors.}
Below I analyze rows CF1-CF7 from Table \ref{tab:translations_critical_factors}.

\textbf{CF1. Paradigms, programming ease.}
DNNs are generally flexible and often use popular interpreted programming languages like Python to facilitate
ease of use.
On the other hand, GPU programming usually relies on C++ and CUDA, which are critical in areas
where speed is a key concern. There has been broad concern about programming ease in GPU programming
as well, as there have slowly been built new frameworks that provide bindings to the dominant paradigms
(Python). This ensures a lowering entry barrier to GPU programming for DNN practitioners who are
often more familiar with Python.

\textbf{CF2. Scalability.}
Both domains address scalability by implementing a modular programming style. DNN frameworks abstract away
the distributed infrastructure complexity (by being able to easily select distributed strategies), while
GPU programming abstract away low-level hardware details, allow developers to focus on the application logic.

\textbf{CF3. Performance.}
Performance is a key concern for both DNNs and GPU programming. DNNs are
motivated by the need to minimize network bandwidth latency and achieve better scalability (as larger
models would likely achieve better performance), while GPU programming provides optimized deep-learning
primitives by optimizing large-matrix operations. To facilitate this, thorough knowledge of the GPU
architecture is required.
\textbf{CF4. Network and hardware communication.}
Multi-GPU training is particularly relevant in DNNs. There exist multiple algorithms to minimize
network latency, however GPU programming frameworks still struggle with multi-GPU communication
training, as libraries like cuDNN leave this management to the user. This indicates an ongoing challenge
in this area.

\textbf{CF5. Ease of use and hardware flexibility.}
The main challenge here resides between ease of use to the developer and hardware flexibility.
Considering the broad community of developers, DNN libraries prioritize modularity and ease of
extension to facilitate usability. Some GPU programming libraries sacrifice ease of use for lower-level control
and potentially higher performance (cuDNN), while others strive for more user-friendly API and easier
hardware switching (CuPy, Caffe).
\paragraph{Evaluation.}
Finally, these are the evaluation metrics (EM) for each domain, together with the challenges and
ongoing limiting factors (LF) for each domain.

\textbf{EM1. Deployment.}
Deployment is an important aspect for both DNNs and GPU programming. DNN libraries follow a staged
deployment process for safe evaluation (i.e. Google products), which offers a degree of safety in
real world scenarios. Conversely, GPU programming frameworks are designed with configurable deployment
in mind, providing features like compile-time flags to integrate more easily into ML frameworks.

\textbf{EM2. Model architectures.}
In some evaluating scenarios, special care was payed to assess performance across different architectures.
For DNNs, this involves assessing scalability with increasingly complex model architectures. On the other hand,
for GPU programming evaluation is geared towards optimizing performance by providing efficient primitives
for the most popular operations (convolutions, self-attention, fully connected layers, etc.).

\textbf{EM3. Task domains.}
Evaluation metrics are strongly dependent on the task domains. DNN benchmarks are performed across
a broad spectrum of deep learning tasks to show general applicability. GPU programming libraries,
while generally geared towards deep learning tasks (like cuDNN), some tools also aim for broader
use (like CuPy in scientific computing). As a result, GPU programming focuses on depth by ensuring
high-performance in a focused field, while DNNs aim for breadth by showing general applicability.

\include{tables/translation_motivating_factors}
\include{tables/translation_critical_factors}
\include{tables/translation_evaluation_metrics}

\textbf{EM4. Evaluation}
DNN evaluation focuses on overall performance gains and broad applicability in different domains (vision,
NLP, etc.). GPU programming, on the other hand, focuses on the potential performance gains achievable through
hardware-specific optimization. This shows that older approaches are continuously refined when new hardware
becomes available, which in turn affects the design of DNNs.

\textbf{LF1. Usability.}
Usability is a limitation factor that affects both areas, with repercussions around reproducibility and developer
productivity. DNN libraries attempt to improve usability through common APIs to facilitate broader
experiment reproducibility. On the other hand, although closed-source GPU programming frameworks
demonstrate better performance due to a more through understanding of the underlying hardware, this sacrifices
developer productivity. For example, debug information is often hidden to the user to avoid leaking
internal implementation details. This is often combated through the use of complex workflows using
profiling tools.

\textbf{LF2. Algorithmic limitations.}
The primary algorithmic limitation concerns memory management in both domains.
DNNs pose problems related to data and model parallelism, while GPU programming faces issues
related to matrix multiplication algorithms and hyperparameter choices being suboptimal in some edge cases
(i.e. small batch size). This shows that advancements are needed in both areas.

\textbf{LF3. Communication Overhead and Scalability.}
Another limitation is the communication overhead and its impact on scalability. Both areas struggle
with this, leading to performance bottlenecks that are challenging to overcome. There are not universally
optimal solutions, as the best approaches are dependent on model architectures and hardware configurations.
Community involvement is essential for progress as this has proven to promote innovation and faster progress.

\section{M.7 -- Practical evaluation}
\label{sec:practical-evaluation}
The study contained also a practical evaluation of \texttt{cuDNN}, \texttt{cuBLAS} and \texttt{Pytorch DDP}.
This is a more pragmatic approach that is meant to provide a more intimate understanding of what
these libraries are capable of.

\paragraph{V.1 -- GPU programming.}
The GPU experiments implement a toy neural network that is meant to be a proof-of-concept for the
capabilities of the utilized libraries (cuDNN and cuBLAS). The following components were
implemented:

\begin{itemize}
	\item The \textbf{forward} and \textbf{backward} training steps.
	\item The \textbf{backward step} is performed via \textbf{two distinct operations}: w.r.t. the weights
	      and w.r.t. the inputs.
	\item The \textbf{gradients can be nulled} in a similar way to Pytorch.
	\item The \textbf{updateWeights} operation has configurable learning rate.
	\item The \textbf{MSELoss function} is used to calculate the mean squared error loss after the forward
	      pass.
	\item Three types of layers:
	      \begin{itemize}
		      \item \textbf{Convolutional}: Uses cuDNN for the convolution operation.
		      \item \textbf{Linear}: Uses cuBLAS for the matrix multiplication.
		      \item \textbf{ReLU}: Uses manual kernel implementation.
	      \end{itemize}
\end{itemize}

\textbf{GPU Kernels.}
There were developed two GPU kernels that run code directly on the GPU. The first one is used
to compute the forward and backward pass for the ReLU activation function. The second one is used
for the MSELoss after the forward pass.

\paragraph{V.2 - V.3 -- Distributed Training.}
The distributed training experiments allow to:

\begin{itemize}
	\item Simulate multi-GPU training on a single machine.
	\item Simulate multi-node training across multiple machines.
	\item Performance metrics are collected using Weights and Biases.
	\item Facilitate experimentation through Docker containers.
\end{itemize}

The code is written in a way to dynamically span multiple GPU ids corresponding to the number of
GPUs used in the launch command (see \texttt{torchrun --nproc\_per\_node}. This conveniently allows
the code to use more memory on a single GPU, effectively allowing to simulate multi-GPU training on
a single machine. In a similar fashion, the \texttt{--nnodes} flag allows to simulate multi-node
training across multiple machines from a single machine.

\textbf{Limitations.}
It should be noted that although the seamless integration with Docker represents a way to simulate
the distributed training environment, it does not provide a true multi-GPU and multi-node performance.
The reason is that using a single GPU, it is not possible to run two code instances truly in a concurrent manner.
Due to the context switching mechanism, the performance can even be even lower than when using a single GPU.
This effectively allows to deploy multiple GPU/node code while relying on a single machine. This should
translate to a more realistic environment with minimal code changes.

\section{M.8 Report findings}
\label{sec:report-findings}
Now, to wrap-up, I will report the key characteristics that answer the research questions defined
in Section \ref{sec:research_questions}.

\textbf{RQ1. DNN frameworks and communities.}
The most common frameworks for distributed neural network training are listed in Table
\ref{tab:dnn_papers} including the number of citations and stars as an indicator for the community
size. The DNN frameworks are characterized by larger communities, two important examples being
Tensorflow and Pytorch, where each pays special attention to usability to promote wider adoption.
The dominant programming language is Python, which harnesses the open-source ecosystem to support a
wide range of applications. Also, the timeline of major frameworks is shown in Figure
\ref{fig:timeline}.

\textbf{RQ2. GPU programming libraries and communities.}
Popular frameworks in the GPU programming community are listed in Table \ref{tab:gpu_papers}. Due
to the fact that most frameworks are closed-source, the communities are generally smaller and more
specialized compared to DNNs. The area is dominated by NVidia frameworks such as cuDNN and cuBLAS
as they currently offer state-of-the-art performance in deep learning applications. Nonetheless,
AMD and Intel are making an effort to provide alternatives with a joint effort for cross-platform
compatibility, where AMD particularly focuses on enlarging the ecosystem through open-source
contributions.

\textbf{RQ3. Overlaps and shared limitations.}
Scalability is the main shared motivation in both domains, where managing the computational
complexity is an important challenge. There are three main ways to manage increased complexity: 1)
by increasing the raw computational power, 2) by decreasing the amount of memory used, and 3) by
reducing the latency and communication overhead between interconnected parts.

GPU programming primarily focuses on squeezing more performance out of the existing hardware, by
providing efficient primitives for matrix multiplication and convolution operations. DNNs, on the
other hand, focus on reducing the memory consumption and the communication overhead. Both areas
face algorithmic challenges. DNNs have the advantage of being able to leverage the large
open-source ecosystem which promotes innovation and a larger community involvement. On the other
hand, GPU programming requires a deeper understanding of the GPU architectures, and while optimal
solutions exist in the open-source community, maintainability is a problem as GPU architectures
evolve. This is why closed-source solutions are dominant, as companies can afford to have
specialized teams to support their hardware.

\section{Conclusion}
\label{sec:conclusion}
In conclusion, the results of the survey provided experience in conducting a systematic review. The
systematic mapping of existing programming frameworks shows that while GPU programming libraries
are a precursor to DNNs, over time they have become more integrated with the DNN frameworks.

GPU programming is a more niche field, where fewer experts are involved with. On the other hand,
DNNs have a larger community and are more widely used. There exist a plethora of frameworks
available for DNNs, characteristic largely due to the large open-source ecosystem. On the other
hand, GPU programming is a more specialized field, with fewer libraries available. This is a
natural consequence to the fact that developing fast primitives requires deep knowledge of the
hardware, which is not required for DNNs.

\paragraph{Future work.}
The GPU training experiments could be expanded to more realistic architectures. Although, the
existing code works and shows that the network effectively learns, it would be instructive to
implement other primitives such as pooling layers, dropout and batch normalization modules, which
could expand the scope and usefulness of the existing primitives.

\paragraph{Learning outcomes.}
The study helped me develop experience with reading and summarizing scientific literature. This
would likely be useful for finding out relevant literature in future work, as by documenting the
workflow, it is easier to replicate and adapt the search for future topics.

The practical experience with libraries such as cuDNN and cuBLAS provides a more thorough
understanding of how neural networks learn and how they are implemented in practice. Similarly, the
distributed training experiments actually demonstrate that the workflows are not at all complicated
and even from a single machine, multi-GPU and multi-node training can be simulated.

\begin{itemize}
	\item \textbf{Goals:}
	\item To analyze parallelization frameworks in DDL.
	\item To evaluate libraries that support CUDA programming.
	\item To find out how these concepts are intertwined.
	\item To get practical experience with popular frameworks. \\
	\item \textbf{Expected Outputs:}
	\item Experience for conducting a systematic review.
	\item Systematic mapping of programming frameworks.
	\item Comparative analysis with strengths and weaknesses.
	\item Identification of research gaps. \\
\end{itemize}


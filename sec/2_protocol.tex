% ===== STEP 2: Define Search Strategy =====
% This section covers:
% - Search strategy development
% - Documentation of search process

\section{Related Work}
\label{sec:related_work}
For documenting the review process, this study follows primarily the guidelines laid out in
\cite{keele_systematic_2007}, however advice for conducting the review is synthesized from a wider
range of related articles
\cite{brereton_lessons_2007-1,kitchenham_procedures_nodate,budgen_reporting_2018,dos_santos_sustainable_2024}.

A summary of existing work in the field is summarized in Table \ref{tab:related_surveys}. Our study
differs from previous surveys in a few different aspects. First, concerning DNNs, related work
focuses on techniques and algorithms for training models across multiple machines
\cite{dehghani_distributed_2023, chahal_hitchhikers_2018, berloco_systematic_2022}, where themes
such as data and model parallelization techniques and communication protocols are explored. As a
result, existing literature tackles architectural patterns and design choices, none focusing
explicitly on providing a broad review of the available frameworks. Secondly, although existing
repositories do provide examples on how to use the CUDA library
\cite{noauthor_nvidiacuda-samples_2025}, as well as DDP
\cite{noauthor_examplesdistributedddpreadmemd_nodate}, none provide end-to-end implementations that
would allow the community to easily build on. This study aims to answer both of these concerns.

Another relevant article which aims to provide an overview of massive parallel frameworks available
for deep learning is \cite{nguyen_machine_2019}. It describes specialized tools for hardware
accelerators (GPUs, FPGAs, TPUs), however does not focus on auxiliary libraries for linear algebra,
numerical computing and GPU communication, gap which this study aims to fill.

\begin{figure*}[th]
	\centering
	\includegraphics[width=\linewidth]{figures/workflow2}
	\caption{The workflow is divided into three main phases: main workflow (top), studies selection (bottom left), and
		evaluation (bottom right). Dashed lines indicate documentation and communication flows.}
	\label{fig:workflow}
\end{figure*}

% \paragraph{Key concepts.}
% Below are summarized some of the key DNN concepts that the rest of this review builds on:

% \begin{itemize}
% 	\item \textbf{Data Parallelism:}
% 	      The dataset is divided across multiple nodes, with each node training a complete copy of the
% 	      model on its portion of data. Gradients from all nodes are then combined to update the model parameters.
% 	      This approach can be implemented either synchronously (all nodes wait for each other) or asynchronously (nodes work independently).

% 	\item \textbf{Model Parallelism:}
% 	      The neural network model itself is divided across different nodes, with each node responsible
% 	      for computing a specific portion of the model architecture. This strategy is particularly useful
% 	      when the model is too large to fit on a single machine.

% 	\item \textbf{Pipeline Parallelism:}
% 	      The training process is divided into sequential stages, similar to an assembly line,
% 	      where the output of one stage becomes the input for the next. This allows different parts
% 	      of the model to train simultaneously while maintaining dependencies.

% 	\item \textbf{Hybrid Parallelism:}
% 	      This approach combines multiple parallelization strategies to optimize training efficiency.
% 	      For example, model parallelism might be used to distribute a large model across GPUs, while
% 	      data parallelism is applied to each model segment.
% \end{itemize}

% These approaches can be further enhanced through techniques such as gradient compression, mixed
% precision training, and tensor fusion \cite{dehghani_distributed_2023}. The choice of specific
% techniques depends on factors including model architecture, available hardware, and training
% requirements. For a comprehensive review of these techniques and their implementations, readers are
% referred to \cite{chahal_hitchhikers_2018}.

\section{Research method}
\label{sec:protocol}

Multiple studies emphasize that a literature survey should be both transparent and replicable
\cite{keele_systematic_2007, dos_santos_sustainable_2024-1}, as this can ensure that reviewer bias
is minimized. Generally, a literature survey is conducted by a group of people working together to
assess the literature and involves multiple iterations. In this paper, steps have been taken to
mitigate this problem, nonetheless it is acknowledged that it is not possible to eliminate bias
completely. To ensure this, the process is documented and most of the resulting artifacts are
shown.

% TODO these are all good but was hoping to minimize the number of citations
% TODO \cite{keele_systematic_2007,brereton_lessons_2007-1,budgen_reporting_2018, dos_santos_sustainable_2024-1},

% Now detail Step 1 content
The main workflow is displayed in Figure \ref{fig:workflow} where the key phases are annotated as
follows: Getting Started (M.1, M.2), Planning the Review (M.3, M.4), Conducting the Review (M.5,
M.6), and Reporting the Review (M.7, M.8).

\paragraph{M.1 -- The need for a survey}
\label{sec:need_for_survey}
The importance follows from the need for a survey that discusses both general frameworks as well as
the connection between the two topics. Also, by supplying practical end-to-end implementations, this
survey can prove useful to students or practitioners that would like to apply the techniques in practice.
It has been shown in Section \ref{sec:related_work} that a survey that focuses on both topics is lacking
in the literature.

\textbf{Learning outcomes.}
Conducting a review is also a good way of becoming accustomed with the two topics. By following a
systematic approach, it becomes easier to find relevant frameworks and research publications which
is a skill that can be applied to other domains as well.

\paragraph{M.2 -- Getting started}
\label{sec:research_questions}
The main goal is to analyze parallelization frameworks in DNNs and GPU programming. This entails two distinct
constraints: (i) consider only frameworks that introduce GPU programming and DNN frameworks and (ii) include
only primary studies.

\subsection{M.3 -- Selection of Relevant Studies}
This step identified relevant papers and is composed of two distinct processes: (i) S.1. literature
survey on DNN frameworks and (ii) S.2. literature survey on GPU programming libraries.

\subsubsection{S.1 -- Literature Survey on DNN libraries}
The steps involved in this process are shown in Figure \ref{fig:workflow-study-dnn}.

\paragraph{S.1.1 -- Problem definition.}
In order to identify relevant studies, I conducted a secondary study\footnote{A secondary study
	synthesizes primary papers to provide a comprehensive overview. This contrasts with tertiary
	studies which analyze secondary studies.}. This decision was made in the problem definition step
(S.1.1). To ensure a methodical approach, this section defines a research protocol which formally
defines the key attributes of the search process. This basically entails defining the research
questions and expanding on the search strategy.

\paragraph{S.1.2 -- Research questions.}
After an initial literature review, the research questions (RQ) are defined using the guidelines
from \cite{kitchenham_evidence-based_2015} and \cite{keele_systematic_2007}. Specifically, the
PICOC (Population, Intervention, Comparison, Outcome, Context) criterion is used to write the
questions into a format that ensures them to be specific, measurable and well-defined.

% TODO: remove this
% \begin{itemize}
% 	\item \textbf{RQ\textsubscript{1}} What are the most common frameworks currently available for 
% 		  GPU programming with CUDA, and how do their usability compare?
% 		  % NOTE implementing distributed deep learning, and how does their usability compare?
% 	      % NOTE \cite{berloco_systematic_2022, ben-nun_demystifying_2020, langer_distributed_2020}?
% 	      % NOTE \item How do parameter update strategies impact distributed deep learning systems (e.g., Parameter Server and decentralised approaches) \cite{ben-nun_demystifying_2020,berloco_systematic_2022,langer_distributed_2020}?
% 	\item How is stochastic gradient descent (SGD) computed in distributed environments
% 	      \cite{berloco_systematic_2022,ben-nun_demystifying_2020,langer_distributed_2020,verbraeken_survey_2021}? % NOTE and what are the associated challenges 
% 	\item What are the key frameworks currently available for implementing DDL, and how do their features 
% 	      compare \cite{berloco_systematic_2022}?
%  	\item In what ways are the techniques used in DDL also useful in GPU parallelization?
% \end{itemize}

% TODO: \TODO{RQ1: ease of learning, ease of use and documentation compare?}
\label{sec:research_questions_refined}
\begin{itemize}
	\item \textbf{RQ\textsubscript{1}:} What are the most commonly cited
	      frameworks for distributed neural network training, and how do their communities vary in size? \\
	      \textit{Rationale:} By identifying the most common frameworks, we can trace the years in which they were published
	      and form a unified timeline of the evolution of the field.

	\item \textbf{RQ\textsubscript{2}:} What are the most frequently cited
	      frameworks for GPU programming, and how do their communities differ in size?\\
	      \textit{Rationale:} By identifying the most common frameworks, we can identify which are the gaps
	      in the literature they cover and which are the most promising areas for future research.

	\item \textbf{RQ\textsubscript{3}:} Which are the overlaps and shared limitations of these areas? \\
	      \textit{Rationale:} By identifying the overlaps, this can lead to a more comprehensive understanding of the field.

	\item \textbf{RQ\textsubscript{4}:} How can these technologies be applied in practice? \\
	      \textit{Rationale:} This can yield hands-on experience on the topic which is helpful for practical applications.

\end{itemize}
\begin{figure*}[th]
	\centering
	\includegraphics[width=\linewidth]{figures/survey-dnn.pdf}
	\caption{The diagram shows the series of steps carried out in the planning and study selection
		phases for the DNNs survey.}.
	\label{fig:workflow-study-dnn}
\end{figure*}
\paragraph{S.1.3 -- Search Strategy.}
The search strategy represents a systematic approach for identifying relevant studies that
adequately answer the research questions.

\textbf{Databases.}
The process involves a manual search of three citation databases --
\href{https://www.scopus.com/}{Scopus}, \href{https://www.semanticscholar.org/}{Semantic Scholar}
and \href{https://arxiv.org/}{arXiv}\footnote{Other relevant databases that could have been used
	include: \href{https://ieeexplore.ieee.org/}{IEEE Xplore}, \href{https://dl.acm.org/}{ACM Digital
		Library} and \href{https://www.sciencedirect.com/}{Science Direct}.} -- that include conference
proceedings and journal papers, considering three metadata fields (title, abstract, and keywords).

\textbf{Inclusion/Exclusion Criteria.}
There were defined three inclusion criteria (IC) and three exclusion criteria (EC). In particular, I
decided to select only primary studies, however secondary studies were mentioned in Section
\ref{sec:related_work}. The identification of secondary studies was useful since the selected
studies synthesize evidence and can make it possible to access primary studies:

\begin{itemize}
	\item \textbf{IC\textsubscript{1}}: Study is a primary study.
	\item \textbf{IC\textsubscript{2}}: Study addresses distributed frameworks in DL.
	\item \textbf{IC\textsubscript{3}}: Study introduces a library or a framework. \\
	\item \textbf{EC\textsubscript{1}}: Study does not discuss implementation details.
	\item \textbf{EC\textsubscript{2}}: Study is not a primary study.
	\item \textbf{EC\textsubscript{3}}: Study is not written in English.
\end{itemize}

\textbf{Search terms.}
In Step S.1.3, I inquired about the literature using the following search string:
\begin{quote}
	\textit{( "machine learning" OR "deep learning" )
		AND
		( "Data parallelism" OR "model parallelism")
		AND
		( "framework" OR "implementation" )}
\end{quote}

\begin{figure*}[th]
	\centering
	\includegraphics[width=\linewidth]{figures/survey-cuda3.pdf}
	\caption{The diagram shows the series of steps carried out in the planning and study selection
		phases for the GPU programming survey.}.
	\label{fig:workflow-study-cuda}
\end{figure*}

\paragraph{Publication year criteria.}
For the DDL task, papers were considered between the time period $\yearstartddl$-$\yearendddl$. The
start year ($\yearstartddl$) was chosen due to being the year where there was a shift towards
resource conservation, which resulted in a focus on concurrency within mini-batches
\cite{ben-nun_demystifying_2020}. This is the year by which the effectiveness of deep learning
algorithms was more widely recognized and more research was published that focused on scalability.

\paragraph{S1.4 - S1.5 -- Semi-Automatic selection.}
After an initial database search (Step S1.4), 182 studies were retrieved. Subsequently, as part of
S.1.5, two methods were applied for reducing the number of studies to a more manageable set. Below,
I describe each approach, including my evaluation regarding their effectiveness.

\textbf{Swift-Review.}
As suggested by \cite{bolanos_artificial_2024},
I applied Swift-Review \cite{Howard2016SWIFTReviewAT}, a machine learning classifier to filter out
irrelevant studies (Step S.1.5). The technique is semi-automatic
with a focus on screening and extraction of relevant information. Prior to initiating the classification,
I had to manually identify 10 positive and 10 negative examples. These
acted as input seed samples for the classifier, which helped pinpoint other useful material. Then
the classifier identified other relevant papers by screening the title, abstract and keywords of
each study. All papers with a confidence score above 0.5 were selected.

\textbf{Classification using Gemini.}
The downside of the previous approach is that it still involves a lot of work to manually identify key
studies. To address this, \cite{bolanos_artificial_2024} suggests many emerging tools that use Large Language Models (LLMs)
to automatically classify relevant material. However, one important factor that limits their usability and effectiveness is the
context window being relatively small to handle a large corpus of text. Nonetheless, Gemini \cite{team_gemini_2024}
is an effective tool to classify the studies, a particularly strong choice due to its massive
context window (over 2M tokens). The drawback is that Gemini does not provide citations when using the web interface
\cite{noauthor_gemini_nodate}. However, NotebookLM \cite{notebooklm_google_2024} solves the issue by providing references
to portions of the text that are relevant to the query. This initial screening is an iterative process that involves
reading the abstracts and keywords of each study to ascertain about the reliability of each response.

\textbf{Results.}
Swift-Review is a more traditional method while NotebookLM is a more recent tool. The former used
to be a viable screening tool before the advent of LLMs. However, Gemini being a much larger model,
provides a faster and more reliable screening process by giving the user the ability to actively
check the results.

\paragraph{S.1.6 - S.1.8 -- Manual selection.}
Step S.1.6 involved a manual inspection over the the title, abstract and keywords. This resulted in
a total of 13 studies that appeared to be relevant. After checking their full text, this number was
reduced to 4. I performed backward snowballing \cite{jalali_systematic_2012} by revisiting the
references of the 4 studies, as well as checking available preprint articles and finally identified
other 8 studies (Step 1.8). Hence, a total of 12 studies (4 + 8) were selected. A subsequent 5
frameworks were found with no accompanying papers by reviewing the references. All 17 libraries are
listed in Table \ref{tab:dnn_papers}.

\paragraph{S.1.9 -- Quality assessment.}
To evaluate the quality of these studies (Step S.1.9), I adapted the quality appraisal instrument
suggested by \cite{zhou_map_2016}, considering 2 main aspects: report and relevance. Regarding
report, I checked whether the studies clearly tackled the problem, research questions and
inclusion/exclusion criteria defined in Steps S.1.1, S.1.2 and S.1.3 respectively. Concerning
relevance, I verified whether the studies presented relevant information to ensure their value for
students and practitioners. Finally, all 17 studies identified in the previous steps passed the
quality checks.

\subsubsection{S.2 -- Survey on GPU programming libraries}
\label{sec:gpu-programming-libraries}
This step involved identifying popular frameworks that facilitate programming on the GPU. The
workflow that was followed is shown in Figure \ref{fig:workflow-study-cuda}.

\paragraph{S.2.1 -- Problem definition.}
It was not possible to perform a systematic review in the traditional sense due to the nature of
the available libraries. The available frameworks are frequently proprietary and are rarely
accompanied by academic papers. Also, the libraries are implementation-focused with documentation
and tutorials being the main source of information. As a result, I had to widen out the type of
articles to include in the review by referencing useful tutorials and key documentation pages.

\paragraph{S.2.2 -- Research Questions.}
The relevant research questions -- RQ\textsubscript{2}, RQ\textsubscript{3} and RQ\textsubscript{4}
-- are listed in Section \ref{sec:research_questions_refined}.

\paragraph{S.2.3 -- Search strategy.}
To find relevant materials (Github repositories, documentation pages and tutorials),
\href{https://github.com/search/advanced}{Advanced Github Search} is used by searching for keywords
related to the main GPU manufacturers: AMD, Intel and NVIDIA.

\input{tables/dnn_studies.tex}
\input{tables/gpu_studies.tex}
\textbf{Inclusion/Exclusion criteria.}
The following criteria were identified to guide the selection process:

\begin{itemize}
	\item \textbf{IC\textsubscript{1}}: The material addresses GPU programming.
	\item \textbf{IC\textsubscript{2}}: The material is official documentation/repository.
	\item \textbf{IC\textsubscript{3}}: The material is a tutorial.
	\item \textbf{IC\textsubscript{4}}: The material introduces a library or framework. \\
	\item \textbf{EC\textsubscript{1}}: The material is not a primary source.
	\item \textbf{EC\textsubscript{2}}: The material is not written in English.
\end{itemize}
\input{tables/timeline.tex}
\textbf{Search terms.}
Search terms similar to the following ones are used to refine the search process:

\begin{quote}
	\textit{user:ROCm user:oneAPI-SRC user:NVIDIA}
\end{quote}

\textbf{Publication year criteria.}
The GPU programming search was restricted to papers published between
$\yearstartcuda$-$\yearendcuda$. The start year ($\yearstartcuda$) was chosen as the baseline due
to being the year when AlexNet \cite{krizhevsky_imagenet_2012} was published. This paper
revolutionized research in neural networks by allowing advanced AI models to be trained on GPUs.

\paragraph{S.2.4 - S.2.8 -- Study Selection Framework.}
\label{sec:ai-screening}
At this step, I used DeepSeek \cite{noauthor_deepseek_nodate} to identify relevant GPU programming
keywords and categories (Step S.2.4). This led to identifying \cite{noauthor_enccsgpu-programming_nodate}, which
offers an excellent introduction to the topic covering general aspects of GPU programming as well
as specific frameworks.

% ===== STEP 3: Selection of Relevant Studies =====
% This section details: 
% - Study 1: Distributed learning techniques
% - Study 2: CUDA implementations

Given the relevant keywords, I followed an iterative approach using search engines, Github
repositories and documentation pages to identify relevant material (Step S.2.5). This lead to
identifying 21 libraries (S.2.6). The libraries had only 2 accompanying papers
\cite{chetlur_cudnn_2014,okuta_cupy_2017} in the literature (S.2.7). The quality assessment step
(S.2.8) ensured that the libraries are relevant to training neural networks and the resulting
papers are shown in Table \ref{tab:gpu_papers}. The categories that were included belong to the
main GPU manufacturers: AMD, Intel and NVIDIA. Across each category, libraries for algebraic
operations (matrix multiplications) include \cite{noauthor_cublas_nodate,noauthor_rocmrocblas_2025,
	noauthor_uxlfoundationonemath_2025}. These are used as building blocks for more complex deep
learning primitives as shown in
\cite{chetlur_cudnn_2014,noauthor_rocmmiopen_2025,onednn_contributors_oneapi_2025}. Existing
approaches that bridge the gap between GPU programming and DNNs are related to communication
protocols such as
\cite{noauthor_nvidianccl_2025,noauthor_rocmrccl_2025,noauthor_uxlfoundationoneccl_2025}. These
frameworks are used in distributed training to synchronize calculations by providing a low-level
APIs that manages cross-GPU communication. Moreover, cross-platform and high-level libraries are
also reported as they allow to effectively build upon core libraries in a faster and more efficient
manner.

The column concerning the NN libraries \cite{Jia.EtAl_2014a,krizhevsky_imagenet_2012,
	Goodfellow.EtAl_2013,Collobert.EtAl_} represent pioneering frameworks that implemented GPU
acceleration prior to the advent of optimized frameworks such as cuDNN.

\section{M.4 -- Reading the studies}
\label{sec:reading-studies}
Figure \ref{fig:timeline} shows a timeline of the evolution of the field. Also, Figure \ref{fig:taxonomy}
shows a taxonomy of the libraries in a unified, hierarchical view.

\subsection{S.1 -- Distributed Neural Networks}
\label{sec:dnn-studies}

In order to more easily extract useful information from the studies, I identified four key criteria
which aim to facilitate answering the research questions defined in Section
\ref{sec:research_questions_refined}.

\begin{itemize}
	\item C1: Key Motivating Factors
	\item C2: Critical Factors and Guidelines
	\item C3: Practical Evaluation Scenarios
	\item C4: Tool Limitations and Challenges
\end{itemize}

The following text draws conclusions from various passages present in the studies, and for
convenience the resulting artifacts are available in the supplementary material in Table
\ref{tab:dnn_passages}. Each contributing factor is associated with an exact passage and various
codes (categories) were used to classify the passage. The results are summarized below.

\paragraph{Motivating Factors.}
The motivating factors for training DNNs include the pursuit of better performance through more
efficient training and increased usability in practical scenarios.

\textbf{Scaling.}
It has been widely thought that
by scaling the architectures to a large number of parameters and leveraging larger datasets, the
evaluation accuracy on many benchmarks would be improved \cellrefs{D102,D103,D105,D111}, idea which
is supported by multiple deep learning frameworks. However, it was recently demonstrated by
Deepseek R1 \cite{deepseekai2025deepseekr1incentivizingreasoningcapability} that scaling up the
computational power is not the only possible method of progress. As a result, future research is
likely to focus not only on distributed training, but also on innovating existing architectures.
Nonetheless, through scaling, many applications become viable in industrial settings
\cellrefs{D101,D105,D106}, which otherwise would be infeasible to train due to the large training time
requirements.

\textbf{Execution speed.}
Apart from scalability, execution speed is also a key motivating factor
\cellrefs{D103,D105}. Some studies have shown that increasing the depth of the network overcomes
performance bottlenecks \cellrefs{D103}, ensuring applicability in a wide range of domains.

\textbf{Resource utilization.}
Yet another important role plays resource utilization which has influence over both scalability and raw
performance. This typically involves optimizing practical scenarios where clusters utilize
heterogeneous hardware \cellrefs{D104}.

\textbf{Accessability.}
Beyond performance and scale, ease of programming and accessability to a wide range of uses also
are important. This involves simplifying workflows and enabling access to advanced techniques to a
wider audience \cellrefs{D110,D111,D112}. However, the need to evolve existing frameworks and
support an increasing number of applications as user requirements change are constraints that make
usability challenging \cellrefs{D106}.

To facilitate broader adoption and flexibility, many frameworks provide cross-platform and
cross-framework support \cellrefs{D109,D112}. This allows researchers to leverage existing
knowledge when transitioning to new frameworks.

\paragraph{Critical Factors.}
Along the previously mentioned factors -- raw speed, scalability and usability -- there is also the
requirement to support a wide range of architectural choices in distributed environments
\cellrefs{D203}. This is a critical factor that requires extensive expertise about the underlying
hardware, and since there are also other constraints to strive for such as performance and
scalability, it becomes challenging to find a balance about what to strive for. In fact, many
frameworks make design choices that favor one property over another \cellrefs{D206,D207,D209,D212}
and it is necessary to be aware about what each library excels at.

To be more specific, Figure \ref{fig:mindmap-dnn} shows popular frameworks that are categorized by
the type of parallelism they support, namely data, model, hybrid and pipeline parallelism. Below is
a short description about what each category means:

\input{tables/taxonomy}

\begin{itemize}
	\item \textbf{Data Parallelism:}
	      The dataset is divided across multiple nodes, with each node training a complete copy of the
	      model on its portion of data. Gradients from all nodes are then combined to update the model parameters.
	      This approach can be implemented either synchronously (all nodes wait for each other) or asynchronously (nodes work independently).

	\item \textbf{Model Parallelism:}
	      The neural network model itself is divided across different nodes, with each node responsible
	      for computing a specific portion of the model architecture. This strategy is particularly useful
	      when the model is too large to fit on a single machine.

	\item \textbf{Pipeline Parallelism:}
	      The training process is divided into sequential stages, similar to an assembly line,
	      where the output of one stage becomes the input for the next. This allows different parts
	      of the model to train simultaneously while maintaining dependencies.

	\item \textbf{Hybrid Parallelism:}
	      This approach combines multiple parallelization strategies to optimize training efficiency.
	      For example, model parallelism might be used to distribute a large model across GPUs, while
	      data parallelism is applied to each model segment.
\end{itemize}

Other possible optimizations include gradient compression, mixed precision training, and tensor
fusion \cite{dehghani_distributed_2023}. The choice of the specific technique depends on factors
including model architecture, available hardware, and training requirements. For a comprehensive
review of these techniques and their implementations, the reader is referred to
\cite{chahal_hitchhikers_2018}.

% TODO: ???
%There are also other critical factors such as cost and communication efficiency \cellrefs{D204}.
%In fact, there have been developed algorithms that make optimal use of the network bandwidth, given
%that there is enough memory available \cellrefs{D210}. This shows that some areas have good solutions
%
%
%To facilitate all these factors, effective hardware utilization (using both the CPU and GPU) plays
%an important role \cellrefs{D201}, as well as software engineering principles like separation of
%concerns \cellrefs{D205}, further contribute to robust library design.

\textbf{Evaluation Scenarios.}
Evaluation is done by measuring accuracy across a wide range of tasks, including image classification,
vision and Natural Language Processing \cellrefs{D303,D305,D306,D308,D311}. In order to provide a more
fair performance assessment, many libraries are evaluated in real-world scenarios \cellrefs{D301}.
Some algorithms have also focused on cross-framework evaluation to ensure broader applicability
\cellrefs{D304}.

\textbf{Limitations and Challenges.}
The main challenges faced by DNNs include communication overhead and resource under-utilization
\cellrefs{D401,D403,D404,D405,D407,D410}. The lack of standardized tools and frameworks lead
to difficulties regarding programming complexity and ease of use \cellrefs{D402,D403,D408}.

There exist high-level optimization challenges that prevent achieving peak performance due to
different architectures and hardware configurations not being extensively supported
\cellrefs{D406,D411}. Finally, the requirement to manually tune applications to find out optimal
parameter configurations points towards the need to create more user-friendly machine learning
frameworks \cellrefs{D411}.

% TODO: ??? HERE
%Apart from this, there exist algorithmic limitations, tight coupling, and issues related to debugging code \cellrefs{D405,D406,D408,D411}, which shows that there is still large room for improvement.

\begin{figure*}
	\centering
	\begin{subfigure}{0.48\linewidth}
		\includegraphics[width=\textwidth]{figures/mindmap}
		\caption{DNN libraries}
		\label{fig:mindmap-dnn}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.48\linewidth}
		\includegraphics[width=\textwidth]{figures/mindmap-cuda}
		\caption{GPU programming libraries}
		\label{fig:mindmap-cuda}
	\end{subfigure}
	\caption{A mindmap of influential libraries}
	\label{fig:mindmap}
\end{figure*}

\subsection{S.2 -- GPU Programming}
\label{sec:gpu-studies}

Figure \ref{fig:mindmap-cuda} shows a mindmap of common GPU programming libraries being discussed
in this section. Specifically, the various categories are separated by the specific function each
provides. Some focus on linear algebra operations (i.e. cuBLAS, rocBLAS, etc) which are primarily
used by neural network frameworks (such as cuDNN, CUTLASS, MIOpen, etc.) to build optimized kernels
(i.e. convolution operations) w.r.t. the specific GPU architecture. Also, communication libraries
provide cross-GPU protocols which are heavily used in distributed training. Other tools such as
high-level languages and portability frameworks enhance usability by reducing complexity and
allowing for easier integration into existing tools.

\paragraph{Motivation and Critical Factors.}
Below are summarized the motivating factors for the development of GPU programming libraries,
making use of the characteristics displayed in Table \ref{tab:gpu_passages}.

\textbf{Scalability and performance optimizations.}
Similarly to DNNs, the development of GPU programming libraries is primarily driven by the need to increase
performance \cellrefs{G1013,G1031,G1051}. This is most often done through focused optimizations
involving the learning kernels, which consist in performing large matrix multiplications at a
faster speed and reducing the amount of auxiliary memory required. These improvements are
intrinsically linked to scalability and the ability to develop larger and more powerful
architectures \cellrefs{G1011,G1012,G1071}.

\textbf{Compatibility.}
Another important concern is the integration and compatibility of GPU libraries with existing
frameworks \cellrefs{G1013,G1014,G1015,G1062}. This is particularly important as seamless
compatibility streamlines development and promotes wider adoption. As an example, Caffe
\cite{Jia.EtAl_2014a} particularly emphasizes how time-consuming is to develop optimized code for
individual architectures. To combat this, frameworks such as CuDNN~\cite{chetlur_cudnn_2014} aim
to provide optimized primitives for Nvidia GPUs across multiple GPU architectures.

\textbf{Usability and programming experience.}
Another important aspect is usability and its effects on programming productivity \cellrefs{G1031,G1071}.
Usability emphasizes the creation of tools accessible to a broader audience, leading to increased
productivity when building complex applications \cellrefs{G1012,G1016,G1017,G1061}. A prime example
is CuPy~\cite{okuta_cupy_2017}, which is a parallel computing framework that provides a NumPy-like
interface to GPU-accelerated arrays. It adheres to principles such as separation of concerns and
provides support for both declarative and imperative programming styles to create a more user-friendly programming
experience \cellrefs{G2012,G2041}.
%\paragraph{Critical factors.}
%Although core libraries that focus on providing optimized primitives focus primarily on raw
%performance (i.e. cuDNN, MIOpen, oneDNN, etc.), there do exist frameworks that also emphasize
%usability as a way to attract a broader audience and enhance productivity \cellrefs{G2041}.

%\textbf{Scalability.}
%Scalability is equally important for handling increasingly complex models and datasets \cellrefs{G2011,G2041}.
%One factor that greatly contributes to scalability is the ability to fully utilize the available resources
%across heterogeneous hardware. This poses significant design constraints as it requires to integrate%
%both CPUs and GPUs simultaneously during training \cellrefs{G2021,G2041}.

%Another important factor that influences scalability is the creation of modular code through
%principles such as separation of concerns \cellrefs{G2012,G2041}.

%Yet another influencing factor is the effectiveness of inter-GPU communication in multi-GPU setups
%\cellrefs{G2051}. This is linked to the choice of the communication library and is a shared concern
%across common GPU programming libraries
%\cite{noauthor_uxlfoundationoneccl_2025,noauthor_nvidianccl_2025,noauthor_rocmrccl_2025}.

%\textbf{Usability.}
%Moreover, broader adoption requires libraries that are easy to use, as this ensures developer productivity
%\cellrefs{G2041}.
%Improved usability can be achieved through support for declarative and imperative programming
%styles and a focus reducing boiler-plate code as much as possible while preserving flexibility
%\cellrefs{G2012,G2061}.

\paragraph{Evaluation.}
To measure performance, both quantitative and qualitative metrics are used.

\textbf{Quantitative evaluation.}
Quantitative metrics assess solutions based on measured performance like convolution
speed and raw throughput, frequently benchmarked against established baselines
\cellrefs{G3011,G3013}. One approach to assess raw performance involves varying mini-batch sizes
against popular neural network architectures \cellrefs{G3011}.

Other types of assessments consist in verifying whether experiments are reproducible across varying
hardware configurations \cellrefs{G3012,G3013,G3061}. This is an important factor as without
deterministic results, it becomes difficult to fairly evaluate performance.

%\textbf{Generalization capabilities.}
%Given that generalization is important, the documents also touch on the effectiveness of the neural
%networks in different domains (i.e. reinforcement learning, computer vision, NLP, etc.), in order to
%ensure broad applicability~\cellrefs{G3012,G3061}.
%To ensure a more realistic assessment, libraries are often evaluated in deployed environments~\cellrefs{G3041}.

\textbf{Qualitative evaluation.}
This assessment type is more subjective as it does not involve numerical quantities to base the
evaluation on, but rather works by trying to form an intuition about the quality of the results. As
an example, this can be done by visually inspecting the results and forming an impression about
their characteristics, as demonstrated in AlexNet~\cellrefs{G3051}.

\paragraph{Limitations and Challenges.}
One significant concern is that the kernel optimizations are time-consuming and require specialized
expertise about the dedicated GPU architecture \cellrefs{G4012,G4041}. The utilization of standard
libraries is is necessary, otherwise the replication of the results would be challenging as
execution times can vary significantly across frameworks \cellrefs{G4041}.

\textbf{Algorithmic constraints.}
Performance bottlenecks are due to matrix operation computations and cross GPU-communication. The
former is an intrinsic algorithmic limitation \cellrefs{G4051,G4061}, while the later is due to the
lack of a standardized API for cross-GPU communication, where the user must more-than-often
manually manage it \cellrefs{G4051}.

Algorithmic and memory constraints are also present, with some algorithms being limited by high
memory usage \cellrefs{G4013}, and others requiring specialized implementations for various corner
cases \cellrefs{G4013}.

\textbf{Evolving architectures.}
Lastly, the field also faces ongoing challenges with evolving architectures, both in terms of hardware advancements
and new neural network innovations, requiring continuous adaptation~\cellrefs{G4011}.

\section{M.5 -- Translate concepts}
\label{sec:translate-concepts}
The translated results describe the relationships between the two topics. The following makes use of Tables \ref{tab:translations_motivating_factors},
\ref{tab:translations_critical_factors} and \ref{tab:translations_evaluation_metrics}.

\include{tables/translation_motivating_factors}
\include{tables/translation_critical_factors}
\include{tables/translation_evaluation_metrics}
% ======== CORE THEME 1: SCALABILITY ========
\section*{Scalability Relationships}
Scalability motivation (MF1) leads to modular implementation approaches (CF2), ultimately resulting in communication limitations (LF3).

\textbf{MF1. Scalability.} The connection is that scalability is a major shared motivating factor for both DNNs and GPU programming.
The increasing scale of data and complexity of DNNs necessitates scalable solutions. GPU programming is
motivated by providing the tools and optimizations needed to achieve this scalability, enabling DNNs to
handle larger workloads, improve productivity, and become more cost-effective.

\textbf{CF2. Scalability.} It is achieved by implementing a modular programming style. DNN frameworks
abstract away the distributed infrastructure complexity -- by being able to easily select distributed
strategies when executing the code -- while ML frameworks leveraging GPU acceleration to hide
low-level hardware details, allowing developers to focus on the application logic.
% Related to: MF1 (motivation), LF3 (limitation)

\textbf{LF3. Communication Overhead and Scalability.} One limitation involves the communication overhead. Both areas struggle
to effectively manage cross-GPU communication, leading to performance bottlenecks that are challenging to overcome. There are not universally
optimal solutions, as the best approaches are dependent on model architectures and hardware configurations.
Community involvement is essential for progress as this can promote innovation.
% Emerges from: CF2 implementation challenges

% ======== CORE THEME 2: HARDWARE COMMUNICATION ======== 
\subsection*{Hardware Communication Challenges}
Hardware heterogeneity motivation leads to multi-GPU challenges (CF4), ultimately creating
scalability limitations (LF3).

\textbf{MF4. Heterogeneous hardware.}
While DNNs are motivated to use heterogenous hardware to ensure broader applicability and
performance, GPU programming acknowledge its importance by providing C APIs for CPU-GPU
communication. Nonetheless, there do exist limitations due to latency and sub-optimal bandwidth
utilization in both domains. A related concern is that GPU libraries like cuDNN do not provide
integrated support for multi-GPU training, which must be achieved manually by the user. This
highlights existing challenges concerning the underlying hardware.

\textbf{CF4. Network and hardware communication.}
Concerning DNNs, there do exist multiple algorithms to optimally minimize network latency, which
indicates that the open ecosystem is effective in coming up with solutions.
% Direct progression from MF4 motivation

% ======== CORE THEME 3: PERFORMANCE OPTIMIZATION ========
\subsection*{Performance Considerations}
The key idea is that performance requires extensive architectural knowledge (CF3), and evaluation
is done through model-specific metrics (EM2).

\textbf{MF2. Complexity and performance.}
The key shared motivator is to manage computational complexity while at the same time ensure higher accuracy
in common applications (i.e. NLP tasks). The increased accuracy is due to the guarantee that performance
is likely to increase thanks to the scaling laws that neural networks exhibit.
GPU programming directly provides the necessary primitives to facilitate the scaling laws and DNNs build on
top of them to improve performance.

\textbf{CF3. Performance.}
Performance optimizations are achieved through specialized techniques. DNNs are motivated by the need to
minimize network bandwidth latency and achieve better scalability, while GPU programming provides
optimized primitives by implementing large-matrix operations, through extensive knowledge of the GPU architecture.
% Implements MF2 motivations

\textbf{EM2. Model architectures.}
In some evaluating scenarios, special attention is payed to assess performance across different
architectures to ensure thorough evaluation and fairness. For DNNs, this involves assessing
scalability with increasingly complex model architectures, while for GPU programming, assessment is
geared towards optimizing performance by providing efficient primitives for the most popular
operations (convolutions, self-attention, fully connected layers, etc.).
% Assesses CF3 implementations

% ======== CORE THEME 4: COMMUNITY & ECOSYSTEM ========
\subsection*{Community and the Available Tools}
%There are multiple motivation factors for Tool leverage (MF6) enables programming paradigms (CF1) but creates usability limitations (LF1).

%\textbf{MF3. Critical in many domains.}
%GPU programming itself might be considered a more specialized domain, but its motivation is to
%enable the use of a wide range of critical applications across the field. GPU programming enable
%DNNs to function efficiently in these domains by providing the building blocks and as a result,
%DNNs could be considered an extension of GPU programming to provide.

\textbf{MF6-MF7. Leveraging existing tools.}
DNN libraries promote an open ecosystem, and while there do exist open alternatives in the GPU programming field (i.e. CuPy),
the dominant companies promote closed-source solutions (i.e. cuDNN). There have been some effort to create
open alternatives (i.e. oneDNN), with attention to provide cross-framework compatibility to facilitate broad adoption.

%\textbf{MF7. Cross-framework use...}
%Open-source DNN framework aim for cross-framework compatibility and usability to foster community innovation.
%GPU programming libraries show a trade-off between low-level, highly optimized primitives (like cuDNN), and offering
%user-friendly interfaces (like CuPy and Torch7). Community involvement is important in both areas, however
%less so for GPU programming due to the proprietary nature of the frameworks.

\textbf{CF1. Paradigms, programming ease...}
DNNs are generally flexible and often use popular interpreted programming languages like Python to
promote ease of use. On the other hand, GPU programming usually relies on C++ and CUDA, which are
critical in areas where speed is important. Many GPU programming libraries provide bindings to
popular languages like Python to facilitate ease of use, which shows that the community has a critical
role in both areas,
% Connects to LF1

% ======== CORE THEME 5: USABILITY & FLEXIBILITY ========
\subsection*{Usability Tradeoffs}
Application motivation (MF5) drives flexibility needs (CF5) evaluated through deployment (EM1).

\textbf{MF5. Applications.}
A shared motivation is to simplify development and improve the practical utility of both DNNs and
GPU programming. Both fields are driven by the need to make life easier for developers to leverage
parallel hardware effectively, and the motivation is to fulfill practical requirements of
simplified deployment and reproducible research.

\textbf{CF5. Ease of use and hardware flexibility.}
The main challenge resides around the trade-off between ease of use and hardware flexibility.
Considering the broad community of developers, DNN libraries prioritize modularity and ease of
extension to facilitate broader community involvement. Some GPU programming libraries sacrifice
ease of use for lower-level control and potentially higher performance (i.e. cuDNN), while others
strive for more user-friendly APIs (i.e. CuPy, Caffe).

\textbf{LF1. Usability.}
DNN libraries attempt to improve usability through common APIs to facilitate broader
reproducibility. On the other hand, GPU programming frameworks have performance as a main objective.
As a result, this sacrifices usability by hiding internal implementation details. To circumvent this,
many frameworks provide profiling tools and specialized debuggers to aid developer productivity.
% Result of EM1  tensions

% ======== CORE THEME 6: DEPLOYMENT & EVALUATION ========
\subsection*{Deployment Strategies}
Domain criticality (MF3) influences task-specific evaluation (EM3).

\textbf{EM4. Evaluation.}
DNN evaluation focuses on overall performance gains and broad applicability in different domains (vision,
NLP, etc.). Conversely, GPU programming focuses on the potential performance gains achievable through
hardware-specific optimizations. There exists a continuous effort to improve both fields when new
hardware architectures become available and new deep learning algorithms are developed.

\textbf{EM1. Deployment.}
To be more specific, while DNN libraries follow a staged deployment process for safe evaluation (i.e.
Google products), offering a degree of safety in real world scenarios, GPU programming frameworks are
easily switched on and off using compiler flags, making them more configurable and easier to integrate
into ML frameworks.

% ======== CORE THEME 7: ALGORITHMIC CHALLENGES ========
\subsection*{Algorithmic Limitations}
Performance requirements (CF3) reveal algorithmic limitations (LF2).

\textbf{LF2. Algorithmic limitations.}
Algorithmic limitations are present in both domains. DNNs pose problems related to memory management in data and model parallelism,
while GPU programming faces issues related to matrix multiplication algorithms and difficulties in optimizing
algorithms for certain hyperparameter ranges (i.e. small batch size).

% Connects to CF3 (Performance)

% ======== UPDATED CROSS-THEME CONNECTIONS ========
% \begin{itemize}
% 	\item \textbf{MF1 → CF2 → LF3}: Scalability motivation (MF1) leads to modular implementation approaches (CF2), ultimately resulting in communication limitations (LF3)
% 	\item \textbf{MF4 → CF4 → LF3}: Hardware heterogeneity motivation leads to multi-GPU challenges (CF4), ultimately creating scalability limitations (LF3)
% 	\item \textbf{MF2 → CF3 → EM2}: Performance motivation requires architectural knowledge (CF3), evaluated through model-specific metrics (EM2)
% 	\item \textbf{MF6 → CF1 → LF1}: Tool leverage (MF6) enables programming paradigms (CF1) but creates usability limitations (LF1)
% 	\item \textbf{MF5 → CF5 → EM1}: Application motivation (MF5) drives flexibility needs (CF5) evaluated through deployment (EM1)
% 	\item \textbf{MF3 → EM3}: Domain criticality (MF3) influences task-specific evaluation (EM3)
% 	\item \textbf{CF3 → LF2}: Performance requirements (CF3) reveal algorithmic limitations (LF2)
% \end{itemize}

% ======== PRESERVED ORIGINAL STRUCTURE ========

\section{M.7 -- Practical evaluation}
\label{sec:practical-evaluation}
The study contained also a practical evaluation of \texttt{cuDNN}, \texttt{cuBLAS} and \texttt{Pytorch DDP}.
The following examples introduce each topic and demonstrate complete training pipelines.

\paragraph{V.1 -- GPU programming.}
The GPU experiments implement a toy neural network that is meant to be a proof-of-concept for the
capabilities of the utilized libraries (cuDNN and cuBLAS). The following components were
implemented:

\begin{itemize}
	\item The forward and backward training steps.
	\item The backward step is performed via two distinct operations: w.r.t. the weights and w.r.t. the
	      inputs.
	\item The gradients can be zeroed on request.
	\item The \texttt{updateWeights} operation has configurable learning rate.
	\item The \texttt{MSELoss} class is used to calculate the mean squared error loss after the forward pass.

\end{itemize}

Three types of layers are implemented:
\begin{itemize}
	\item \textbf{Convolutional}: Uses cuDNN for the convolution operation.
	\item \textbf{Linear}: Uses cuBLAS for the matrix multiplication.
	\item \textbf{ReLU}: Uses manual kernel implementation.
\end{itemize}

%\textbf{GPU Kernels.}
There were developed two GPU kernels that run code directly on the GPU. The first one is used to
compute the forward and backward pass for the ReLU activation function. The second one is used for
the MSELoss after the forward pass.

\paragraph{V.2 - V.3 -- Distributed Training.}
The distributed training experiments allow to:

\begin{itemize}
	\item Simulate multi-GPU training on a single machine.
	\item Simulate multi-node training across multiple machines.
	\item Collect performance metrics using Weights and Biases.
	\item Facilitate experimentation through Docker containers.
\end{itemize}

The code is written to dynamically span multiple GPU ids corresponding to the number of GPUs used
in the launch command (see \texttt{torchrun --nproc\_per\_node}). This conveniently allows the
model to use more memory on a single GPU, effectively allowing to simulate multi-GPU training on a
single machine. In a similar way, the \texttt{--nnodes} flag allows to simulate multi-node training
across multiple machines.

\textbf{Limitations.}
Although the seamless integration with Docker allows to simulate the distributed training
environment, it does not provide a true multi-GPU and multi-node performance. The reason is that
using a single GPU, it is not possible to run two code instances truly simultaneously. This is due
to the context switching mechanism inside the GPU which does not allow for true parallelism. The
code could potentially be reutilized in a cloud environment with minimal code changes.

\section{M.8 Report findings}
\label{sec:report-findings}
Now, to wrap-up, I will report the key characteristics that answer the research questions defined
in Section \ref{sec:research_questions}.

\textbf{RQ\textsubscript{1}. DNN frameworks and communities.}
The most common frameworks for distributed neural network training are listed in Table
\ref{tab:dnn_papers}, which includes the number of citations and stars as an indicator for the community
size. The DNN frameworks are characterized by larger communities, two important examples being
Tensorflow and Pytorch, where each pays special attention to usability to promote wider adoption.
The dominant programming language is Python, which harnesses the open-source ecosystem to support a
wide range of applications. Figure \ref{fig:timeline} shows the timeline of the release date of the
major frameworks.

\textbf{RQ\textsubscript{2}. GPU programming libraries and communities.}
Popular frameworks in the GPU programming community are listed in Table \ref{tab:gpu_papers}. Due
to the fact that most frameworks are closed-source, the communities are generally smaller and more
specialized compared to DNNs. The area is dominated by Nvidia libraries such as cuDNN and cuBLAS,
as they currently offer state-of-the-art performance in deep learning applications. Nonetheless,
AMD and Intel are building tools to provide alternatives with a joint effort for cross-platform
compatibility, while AMD particularly focuses on enlarging the ecosystem through open-source
contributions.

\textbf{RQ\textsubscript{3}. Overlaps and shared limitations.}
Scalability is the main shared motivation in both domains, where managing the computational
complexity is an important challenge. There are three main ways to manage increased complexity: 1)
by increasing the raw computational power, 2) by decreasing the amount of memory used, and 3) by
reducing the latency and communication overhead between interconnected parts.

GPU programming primarily focuses on squeezing more performance out of the existing hardware, by
providing efficient primitives for matrix multiplication and convolution operations. DNNs, on the
other hand, focus on reducing the memory consumption and communication overhead. Both areas face
algorithmic challenges. DNNs have the advantage of being able to leverage the large open-source
ecosystem which promotes innovation and a larger community involvement. On the other hand, GPU
programming requires a deeper understanding of the GPU architectures, and while optimal solutions
exist in the open-source community, maintainability is a problem as GPU architectures evolve. This
is why closed-source solutions are dominant, as companies can afford to have specialized teams to
support the hardware.

\section{Conclusion}
\label{sec:conclusion}
In conclusion, the systematic review summarizes key characteristics of existing programming frameworks
in each domain, while highlighting shared challenges and limitations. DNNs build on top of GPU programming
libraries and most frameworks are accompanied by academic papers that detail the underlying algorithms.
The large open-source ecosystem encourages community contributions, which promotes ease of use,
developer productivity and a wider range of specialized frameworks.

On the other hand, we have seen that GPU programming is a more specialized field, where few
libraries dominate. There exist few published papers that detail the underlying algorithms, and the
available resources are focused on providing documentation and tutorials. In some ways this stifles
innovation, and although some companies (i.e. AMD) strive for an open ecosystem, the field is currently
dominated by Nvidia GPUs.

\paragraph{Future work.}
The GPU training experiments could be expanded to more realistic architectures. Although, the
existing code shows that the network effectively learns, it would be instructive to implement other
primitives such as pooling layers, dropout and batch normalization modules, which could expand the
scope and usefulness of the possible applications.

% TODO [Possibly useful]
% \paragraph{Learning outcomes.}
% The study helped me develop experience with reading and summarizing scientific literature, skill
% which would likely be relevant for identifying relevant studies in the future. The implemented code
% provides a walkthrough of how neural networks can be implemented in practice by utilizing popular
% libraries in each domain.


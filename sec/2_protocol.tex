% ===== STEP 2: Define Search Strategy =====
% This section covers:
% - Search strategy development
% - Documentation of search process

\begin{figure*}[th]
	\centering
	\includegraphics[width=\linewidth]{figures/workflow2}
	\caption{Systematic review workflow showing the main steps, documentation artifacts, and validation processes.
		The workflow is divided into three main phases: main workflow (top), studies selection (middle), and
		validation (bottom). Dashed lines indicate documentation and communication flows. Adapted from \cite{dos_santos_sustainable_2024}.}
	\label{fig:workflow}
\end{figure*}

\section{Related Work}
\label{sec:related_work}

\TODO{Reference the related work on survying the present methods both in DDL and CUDA}

For documenting the review process, this study follows primarily the guidelines laid out in
\cite{keele_systematic_2007}, however advice for conducting the review is synthesized from a wider
range of related articles
\cite{brereton_lessons_2007-1,kitchenham_procedures_nodate,budgen_reporting_2018,dos_santos_sustainable_2024}.

\textbf{Limitations of existing work.}
Our study differs from previous surveys in a few different aspects. First, concerning the DNN part,
related work focuses on techniques and algorithms for training models across multiple machines
\cite{dehghani_distributed_2023, chahal_hitchhikers_2018, berloco_systematic_2022}, where themes
such as data and model parallelization techniques and communication protocols are explored. As a
result, existing literature tackles architectural patterns and design choices, none focusing
explicitly on providing a broad review of the available frameworks. Secondly, although existing
repositories do provide examples on how to use the CUDA library
\cite{noauthor_nvidiacuda-samples_2025}, as well as DDP
\cite{noauthor_examplesdistributedddpreadmemd_nodate}, none provide end-to-end implementations that
would allow the community to build upon. This study aims to answer both of these concerns.

Another relevant article which aims to provide an overview of massive parallel frameworks available
for deep learning is \cite{nguyen_machine_2019}. It describes specialized tools for hardware
accelerators (GPUs, FPGAs, TPUs), however does not focus on auxiliary libraries for linear algebra,
numerical computing and GPU communication, gap which this study aims to fill.

Below are summarized some of the key concepts that the rest of this review builds upon:

\begin{itemize}
	\item \textbf{Data Parallelism:}
	      The dataset is divided across multiple nodes, with each node training a complete copy of the
	      model on its portion of data. Gradients from all nodes are then combined to update the model parameters.
	      This approach can be implemented either synchronously (all nodes wait for each other) or asynchronously (nodes work independently).

	\item \textbf{Model Parallelism:}
	      The neural network model itself is divided across different nodes, with each node responsible
	      for computing a specific portion of the model architecture. This strategy is particularly useful
	      when the model is too large to fit on a single machine.

	\item \textbf{Pipeline Parallelism:}
	      The training process is divided into sequential stages, similar to an assembly line,
	      where the output of one stage becomes the input for the next. This allows different parts
	      of the model to train simultaneously while maintaining dependencies.

	\item \textbf{Hybrid Parallelism:}
	      This approach combines multiple parallelization strategies to optimize training efficiency.
	      For example, model parallelism might be used to distribute a large model across GPUs, while
	      data parallelism is applied to each model segment.
\end{itemize}

These approaches can be further enhanced through techniques such as gradient compression, mixed
precision training, and tensor fusion \cite{dehghani_distributed_2023}. The choice of specific
techniques depends on factors including model architecture, available hardware, and training
requirements. For a comprehensive review of these techniques and their implementations, readers are
referred to \cite{chahal_hitchhikers_2018}.

\section{Research method}
\label{sec:protocol}

Multiple studies emphasize that a literature survey should be both transparent and replicable
\cite{keele_systematic_2007, dos_santos_sustainable_2024-1}, as this can ensure that review bias is
minimized. This is a valid concern in general, however especially so in this survey since it is
conducted by only one person. In a broader study, bias would normally be minimized by having
multiple iterations with more than one reviewer involved. In this paper, steps have been taken to
mitigate this problem, nonetheless it is acknowledged that it is not possible to eliminate it
completely. To ensure this, the process is documented, most of the resulting artifacts being
available in the text as well as in the Appendix.

% TODO these are all good but was hoping to minimize the number of citations
% TODO \cite{keele_systematic_2007,brereton_lessons_2007-1,budgen_reporting_2018, dos_santos_sustainable_2024-1},

% Now detail Step 1 content
The process workflow is shown in Figure \ref{fig:workflow} where the key phases are annotated as
follows: Getting Started (M.1, M.2), Planning the Review (M.3, M.4), Conducting the Review (M.5,
M.6), and Reporting the Review (M.7, M.8). M.2 calls an auxiliary process composed of S.1 and S.2
to select the appropriate studies.

\subsection{M.1 -- The need for a survey}
\label{sec:need_for_survey}

\textbf{Importance of the topic.}
Distributed techniques are important in neural networks because they allow models with billions of
parameters to perform a large number of computations in a reasonable amount of time. One of the
pioneering papers in machine learning that made use of GPUs to train neural networks in parallel
(AlexNet \cite{krizhevsky_imagenet_2012}), mentioned that the network took "between five and six days to
train on two GTX 580 3GB GPUs", suggesting that the training time would have taken much longer on a
CPU.

One key property of distributing neural network architectures to a larger number of parameters, is
that one can achieve better generalization and lower error rates even when training on smaller
datasets \cite{kaplan_scaling_2020}. This has applications in many domains, among which natural
language processing, computer vision and speech recognition \cite{noauthor_papers_nodate}.

\textbf{Learning outcomes.}
As a result of conducting the survey I expect the following learning outcomes: generally get accustomed
with the frameworks that enable distributed training of neural networks and gain practical experience
conducting experiments with CUDA and Pytorch DDP.

\subsection{M.2 -- Getting started}
\label{sec:research_questions}
The main goal is to analyze parallelization frameworks in DNNs and GPU programming. This entails two distinct
constraints: (i) consider only frameworks that introduce GPU programming and DNN frameworks and (ii) include
only primary studies.

\subsection{M.3 -- Selection of Relevant Studies}
This step identified and selected studies from both areas (GPU programming in S.1 and DNN
frameworks). I collected studies of the state of the art of both GPU programming (in Step S.1) and
DNN frameworks (in Step S.2).

\subsubsection{S.1 -- Literature Survey on DNN libraries}
\paragraph{S.1.1 -- Problem definition.}
In order to identify relevant studies, I conducted a secondary study\footnote{A secondary study
	synthesizes primary papers to provide a comprehensive overview. This contrasts with tertiary
	studies which analyze secondary studies.}. This decision was made in the problem definition step
(S.1.1 in Figure \ref{fig:workflow-study-cuda}). To ensure a methodical approach, this section
defines a research protocol which formally defines the key attributes of the search process. This
basically entails expanding on the research questions and search strategy.

\begin{figure*}[th]
	\centering
	\includegraphics[width=\linewidth]{figures/survey-dnn.pdf}
	\caption{The diagram suggests the key steps for finding papers related to the CUDA programming survey. The phases
		include planning the review (aims, research questions, search strategy) and conducting the review (study selection phase). The research questions
		were transformed according to the PICOC (Population, Intervention, Comparison, Outcome, Context) criterion suggested by \cite{keele_systematic_2007}.}.
	\label{fig:workflow-study-cuda}
\end{figure*}

\paragraph{S.1.2 -- Research questions.}
After an initial literature review, the research questions (RQ) defined in Section
\ref{sec:initial_research_questions} are refined using the guidelines defined in
\cite{kitchenham_evidence-based_2015} and \cite{keele_systematic_2007}. Specifically, the PICOC
(Population, Intervention, Comparison, Outcome, Context) criterion is used to transform the
questions into a format that ensures them to be specific, measurable and well-defined. The
questions below are ordered based on their priority in the survey:

% TODO: remove this
% \begin{itemize}
% 	\item \textbf{RQ\textsubscript{1}} What are the most common frameworks currently available for 
% 		  GPU programming with CUDA, and how do their usability compare?
% 		  % NOTE implementing distributed deep learning, and how does their usability compare?
% 	      % NOTE \cite{berloco_systematic_2022, ben-nun_demystifying_2020, langer_distributed_2020}?
% 	      % NOTE \item How do parameter update strategies impact distributed deep learning systems (e.g., Parameter Server and decentralised approaches) \cite{ben-nun_demystifying_2020,berloco_systematic_2022,langer_distributed_2020}?
% 	\item How is stochastic gradient descent (SGD) computed in distributed environments
% 	      \cite{berloco_systematic_2022,ben-nun_demystifying_2020,langer_distributed_2020,verbraeken_survey_2021}? % NOTE and what are the associated challenges 
% 	\item What are the key frameworks currently available for implementing DDL, and how do their features 
% 	      compare \cite{berloco_systematic_2022}?
%  	\item In what ways are the techniques used in DDL also useful in GPU parallelization?
% \end{itemize}

% TODO: \TODO{RQ1: ease of learning, ease of use and documentation compare?}
\label{sec:research_questions_refined}
\begin{itemize}
	\item \textbf{RQ\textsubscript{1}:} In the field of Deep Learning, what are the most commonly cited
	      frameworks for distributed training of neural networks across clusters, and how do their respective communities vary in size? \\
	      \textit{Rationale:} By identifying the most common frameworks, we can trace the years in which they were published
	      and form a unified timeline of the evolution of the field. Here key motivating factors influence resulting communities.

	      % NOTE Razvan I am conducting a literature review in the field of deep learning, specifically focusing on distributed training of 
	      % NOTE neural networks across clusters. I need to identify the most commonly cited frameworks used for this purpose.
	\item \textbf{RQ\textsubscript{2}:} What practical applications of these technologies have been reported in the literature
	      and which are their limitations? \\
	      \textit{Rationale:} This can yield hands-on experience on the topic which is helpful for practical applications.

	\item \textbf{RQ\textsubscript{3}:} What are the overlaps in data/model optimization strategies used in DDL and GPU parallelization? \\
	      \textit{Rationale:} By identifying the overlaps, this can lead to a more comprehensive understanding of the field.
\end{itemize}

\input{tables/taxonomy}
The questions play a key role in guiding the search strategy, data extraction process and results
synthesis phase.

\paragraph{S.1.3 -- Search Strategy.}
The search strategy represents represents a systematic approach for identifying relevant studies
that adequately answer the research questions.

\textbf{Databases.}
The process involves a manual search of three citation databases --
\href{https://www.scopus.com/}{Scopus}, \href{https://www.semanticscholar.org/}{Semantic Scholar}
and \href{https://arxiv.org/}{arXiv}\footnote{Other relevant databases that could have been used
	include: \href{https://ieeexplore.ieee.org/}{IEEE Xplore}, \href{https://dl.acm.org/}{ACM Digital
		Library} and \href{https://www.sciencedirect.com/}{Science Direct}.} -- that include conference
proceedings and journal papers, considering three metadata fields (title, abstract, and keywords).

\textbf{Inclusion/Exclusion Criteria.}
There were defined three inclusion criteria (IC) and three exclusion criteria (EC). In particular, I
decided to select only primary studies, however secondary studies were mentioned in Section
\ref{sec:related_work}. The identification of secondary studies was useful since the selected
studies synthesize evidence and can make it possible to access primary studies:

\begin{itemize}
	\item \textbf{IC\textsubscript{1}}: Study is a primary study.
	\item \textbf{IC\textsubscript{2}}: Study addresses distributed frameworks in DL.
	\item \textbf{IC\textsubscript{3}}: Study introduces a library or a framework. \\
	\item \textbf{EC\textsubscript{1}}: Study does not discuss implementation details.
	\item \textbf{EC\textsubscript{2}}: Study is not a primary study.
	\item \textbf{EC\textsubscript{3}}: Study is not written in English.
\end{itemize}

\textbf{Search terms.}
In Step S.1.3, I inquired about the literature using the following search string:
\begin{quote}
	\textit{( "machine learning" OR "deep learning" )
		AND
		( "Data parallelism" OR "model parallelism")
		AND
		( "framework" OR "implementation" )}
\end{quote}

The details and justification to define this search string can be found in the supplementary
material in Section \ref{sec:search_strategy}. \TODO{...}

\paragraph{Publication year criteria.}
For the DDL task, papers were considered between $\yearstartddl$-$\yearendddl$. The start year
($\yearstartddl$) was chosen as at this point there was a shift towards resource conservation,
which resulted in a focus on concurrency within mini-batches \cite{ben-nun_demystifying_2020}. This
is the year by which the effectiveness of deep learning algorithms was more widely recognized and
more research was published that focused on scalability.

\paragraph{S1.4 - S1.5 -- Semi-Automatic selection.}
After an initial database search (Step S1.4), 182 studies were retrieved. Subsequently, as part of
S.1.5, two methods were applied for reducing the number of studies to a more manageable set. Below,
I describe each approach, including my evaluation regarding their effectiveness.

\textbf{Swift-Review.}
As suggest in \cite{bolanos_artificial_2024},
I applied Swift-Review \cite{Howard2016SWIFTReviewAT}, a machine learning classifier to filter out
irrelevant studies (Step S.1.5). The technique is semi-automatic
with a focus on screening and extraction of relevant information. Prior to initiating the classification,
I had to manually identify 10 positive and 10 negative examples. These
acted as input seed samples for the classifier, which helped pinpoint other useful material. Then
the classifier identified other relevant papers by screening the title, abstract and keywords of
each study. All papers with a confidence score above 0.5 are selected.

\textbf{Classification using Gemini.}
The downside of the previous approach is that it still involves a lot of work to manually identify key
studies. To address this, \cite{bolanos_artificial_2024} suggests many emerging tools that use Large Language Models (LLMs)
to automatically classify relevant material. However, in my experience, none are particularly effective
at handling a large corpus of text due to the context window being relatively small. As a result, I decided
to use Gemini \cite{team_gemini_2024} to classify the studies, which is a particularly strong choice due to its massive
context window (over 2M tokens). The drawback is that Gemini does not provide citations when using the web interface
\cite{noauthor_gemini_nodate}. However, NotebookLM \cite{notebooklm_google_2024} solves the issue by providing references
to portions of the text that are relevant to the query. My approach involved downloading the search results in textual format from the
search engines and then leveraging NotebookLM to classify the studies by utilizing the research questions
defined in Section \ref{sec:research_questions} as a prompt. This was an iterative process that involved
reading the abstracts and keywords of each study to ascertain about the reliability of each response. It is not a perfect process,
however building iteratively over the results and blending together different phases of the review process
(as we shall see in Section \ref{sec:reading-studies}), gives a good baseline for iterative improvement.

\textbf{Results.}
After investigating the results of both approaches, I found that the NotebookLM approach was more
effective at identifying key studies. This is likely due to Gemini being a more powerful
model than the classifier used by Swift-Review. The number of selected studies was 32.
\begin{table*}[th!]
	\centering
	\caption{The DNN papers included in the review. 1 (data), 2 (model), 3 (pipeline)}
	\label{tab:dnn_papers}
	\begin{tabular}{llp{8.4cm}lllc}
		\hline
		\small \textbf{\#} & \small \textbf{Ref.}                    & \small \textbf{Title}                                                                                                               & \small \textbf{Type} & \small \textbf{Year} & \small \textbf{Citations} & \small \textbf{Stars}                                                \\[1ex]
		\hline
		\small DNN1        & \small \cite{abadi_tensorflow_2016}     & \small TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems                                                & \small Data          & \small 2016          & \small 9998               & \small 187k \cite{abadi_tensorflow_2015}                             \\[1ex]
		\small DNN2        & \small \cite{chen_mxnet_2015}           & \small MXNet: A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems                               & \small Hybrid        & \small 2015          & \small 2214               & \small 20.8k \cite{noauthor_apachemxnet_2025}                        \\[1ex]
		\small DNN3        & \small \cite{huang_gpipe_2019}          & \small GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism                                                & \small Pipeline      & \small 2018          & \small 1446               & \small 2.8k \cite{noauthor_tensorflowlingvo_2025}                    \\[1ex]
		\small DNN4        & \small \cite{jiang_unified_nodate}      & \small BytePS: A Unified Architecture for Accelerating Distributed DNN Training in Heterogeneous GPU/CPU Clusters                   & \small Data          & \small 2020          & \small 338                & \small 3.7k \cite{noauthor_bytedancebyteps_2025}                     \\[1ex]
		\small DNN5        & \small \cite{lepikhin_gshard_2020}      & \small GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding                                             & \small Model         & \small 2020          & \small 931                & \small 2.8k \cite{noauthor_tensorflowlingvo_2025}                    \\[1ex]
		\small DNN6        & \small \cite{li_pytorch_2020}           & \small PyTorch Distributed: Experiences on Accelerating Data Parallel Training                                                      & \small Hybrid        & \small 2020          & \small 175                & \small 86.1k \cite{noauthor_pytorchpytorch_nodate}                   \\[1ex]
		\small DNN7        & \small \cite{li_colossal-ai_2023}       & \small Colossal AI: A Unified Deep Learning System for Large-Scale Parallel Training                                                & \small Hybrid        & \small 2023          & \small 118                & \small 39k \cite{noauthor_hpcaitechcolossalai_2025}                  \\[1ex]
		\small DNN8        & \small \cite{moritz_ray_2018}           & \small Ray: A distributed Framework for Emerging AI Applications                                                                    & \small Hybrid        & \small 2018          & \small 1108               & \small 35k \cite{noauthor_ray-projectray_2025}                       \\[1ex]
		\small DNN9        & \small \cite{rasley_deepspeed_2020}     & \small DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters                        & \small Hybrid        & \small 2020          & \small 1059               & \small 36.3k \cite{noauthor_microsoftdeepspeed_2025}                 \\[1ex]
		\small DNN10       & \small \cite{sergeev_horovod_2018}      & \small Horovod: fast and easy distributed deep learning in TensorFlow                                                               & \small Data          & \small 2018          & \small 1152               & \small 14.3k \cite{noauthor_horovodhorovod_2025}                     \\[1ex]
		\small DNN11       & \small \cite{shoeybi_megatron-lm_2020}  & \small Megatron-LM: Training Multi-Billion Parameter Models for Natural Language Processing                                         & \small Hybrid        & \small 2020          & \small 1578               & \small 11.2k \cite{noauthor_nvidiamegatron-lm_2025}                  \\[1ex]
		\small DNN12       & \small \cite{wolf_huggingfaces_2020}    & \small HuggingFace's Transformers: State-of-the-art Natural Language Processing                                                     & \small Data          & \small 2020          & \small 1444               & \small 8.2k \cite{noauthor_huggingfaceaccelerate_2025}               \\[1ex]
		\small DNN13       & \small \cite{noauthor_overview_nodate}  & \small Pytorch Lightning: The lightweight PyTorch wrapper for high-performance AI research. Scale your models, not the boilerplate. & \small Data          & \small 2019          & \small N/A                & \small 28.8k \cite{falcon_pytorch_2019}                              \\[1ex]
		\small DNN14       & \small \cite{noauthor_fairscale_nodate} & \small FairScale:  A general purpose modular PyTorch library for high performance and large scale training                          & \small Hybrid        & \small 2021          & \small N/A                & \small 3.2k \cite{FairScale2021}                                     \\[1ex]
		\small DNN15       & \small \cite{noauthor_amazon_nodate}    & \small Amazon SageMaker Platform                                                                                                    & \small Data          & \small 2017          & \small N/A                & \small 10.3k \cite{noauthor_awsamazon-sagemaker-examples_2025}       \\[1ex]
		\small DNN16       & \small \cite{sdgilley_azure_nodate}     & \small Microsoft AzureML Platform                                                                                                   & \small Data          & \small 2021          & \small N/A                & \small 1.8k \cite{noauthor_azureazureml-examples_2025}               \\[1ex]
		\small DNN17       & \small \cite{noauthor_vertex_nodate}    & \small Google Vertex AI Platform                                                                                                    & \small Data          & \small 2021          & \small N/A                & \small 178 \cite{noauthor_googlecloudplatformvertex-ai-samples_2025} \\[1ex]
		\small DNN18       & \small \cite{frostig_compiling_nodate}  & \small Jax: Compiling machine learning programs via high-level tracing                                                              & \small Hybrid        & \small 2018          & \small N/A                & \small 31k \cite{noauthor_jax-mljax_2025}                            \\[1ex]
		\hline
	\end{tabular}
\end{table*}

\paragraph{S.1.6 - S.1.8 -- Manual selection.}
In Step S.1.6, a manual inspection over the the title, abstract and keywords was made. This
resulted in a total of 13 studies that appeared to be relevant. After checking their full text,
this number was reduced to 4. I performed backward snowballing \cite{jalali_systematic_2012} by
revisiting the references of the 4 studies, as well as checking available preprint articles and
identified other 8 studies (Step 1.8). Hence, a total of 12 studies (4 + 8) were selected. A
subsequent 5 frameworks were found with no accompanying papers by reviewing the references. All 17
libraries are listed in Table \ref{tab:dnn_papers}.

\begin{table}[h!]
	\centering
	\caption{The Search Engines Used in this Survey}
	\label{tab:databases}
	\begin{tabular}{llllr}
		\hline
		ID    & Tool             & Step  & Results & Filtered \\
		\hline
		1     & Scopus           & S.1.4 & 182     & 4        \\
		2     & Semantic Scholar & S.1.8 & --      & 8        \\
		3     & Proprietary      & S.1.8 & --      & 5        \\
		\hline
		Total &                  &       &         & 17       \\
	\end{tabular}
\end{table}

\paragraph{S.1.9 -- Quality assessment.}
To evaluate the quality of these studies (Step S.1.9), I adapted the quality appraisal instrument
suggested by \cite{zhou_map_2016}, considering 2 main aspects: report and relevance. Regarding
report, I checked whether the studies clearly tackled the problem, research questions and
inclusion/exclusion criteria defined in Steps S.1.1, S.1.2 and S.1.3 respectively. Concerning
relevance, I verified whether the studies presented relevant information to ensure their value for
practitioners and researchers. Finally, all 17 studies identified in the previous steps passed the
quality checks.

\begin{figure*}[th]
	\centering
	\includegraphics[width=\linewidth]{figures/survey-cuda3.pdf}
	\caption{The diagram suggests the key steps for finding papers related to the CUDA programming survey. The phases
		include planning the review (aims, research questions, search strategy) and conducting the review (study selection phase). The research questions
		were transformed according to the PICOC (Population, Intervention, Comparison, Outcome, Context) criterion suggested by \cite{keele_systematic_2007}.}.
	\label{fig:workflow-study-dnn}
\end{figure*}
\subsubsection{S.2 -- Survey on GPU programming libraries}
\label{sec:gpu-programming-libraries}
This step involved identifying popular frameworks that facilitate programming on the GPU. The
workflow that was followed is shown in Figure \ref{fig:workflow-study-cuda}.

\paragraph{S.2.1 -- Problem definition.}
It was not possible to perform a systematic review in the traditional sense due to the nature of
the available tools. The available libraries are frequently proprietary and are rarely accompanied
by academic papers. Also, the libraries are implementation-focused with documentation and tutorials
being the main source of information. As a result, I had to widen out the type of articles to
include in the review by referencing useful tutorials and key documentation pages.

\paragraph{S.2.2 -- Research Questions.}
The research questions defined in Section \ref{sec:initial_research_questions} were refined using
the PICOC criterion suggested by \cite{keele_systematic_2007}. The following question is relevant:

\begin{itemize}
	\item \textbf{RQ\textsubscript{4}:} In the field of Deep Learning, what are the most frequently cited
	      frameworks for GPU programming using CUDA, and how do their user communities differ in size? \\
	      \textit{Rationale:} By identifying the most common frameworks, we can identify which are the gaps
	      in the literature they cover and which are the most promising areas for future research.
\end{itemize}

Moreover, questions RQ\textsubscript{2} and RQ\textsubscript{3} from Section
\ref{sec:research_questions_refined} are also relevant.

\paragraph{S.2.3 -- Search strategy.}
To find relevant materials (Github repositories, documentation pages and tutorials), the following
tools were used: \href{https://github.com/search/advanced}{Advanced Github Search} and two AI
search engines: DeepSeek Search Engine \cite{noauthor_deepseek_nodate} and Perplexity AI
\cite{noauthor_perplexity_nodate}.

\textbf{Inclusion/Exclusion criteria.}
The following criteria were identified to guide the selection process:

\begin{itemize}
	\item \textbf{IC\textsubscript{1}}: The material addresses GPU programming.
	\item \textbf{IC\textsubscript{2}}: The material is official documentation/repository.
	\item \textbf{IC\textsubscript{3}}: The material is a tutorial.
	\item \textbf{IC\textsubscript{4}}: The material introduces a library or framework. \\
	\item \textbf{EC\textsubscript{1}}: The material is not a primary source.
	\item \textbf{EC\textsubscript{2}}: The material is not written in English.
\end{itemize}

\textbf{Search terms.}
The following terms were used to process the search for relevant Github repositories, which are
linked to the main GPU manufacturers:

\begin{quote}
	\textit{user:ROCm user:oneAPI-SRC user:NVIDIA}
\end{quote}

These correspond to AMD, Intel and NVIDIA official repositories in Github.

\textbf{Publication year criteria.}
The GPU programming search was restricted to papers published between
$\yearstartcuda$-$\yearendcuda$. The start year ($\yearstartcuda$) was chosen as the baseline due
to being the year when AlexNet \cite{krizhevsky_imagenet_2012} was published. This paper
revolutionized research in neural networks by allowing advanced AI models to be trained on GPUs.

\paragraph{S.2.4 - S.2.8 -- Study Selection Framework.}
\label{sec:ai-screening}
At this step, I used DeepSeek \cite{noauthor_deepseek_nodate} to identify relevant GPU programming
keywords and categories (Step S.2.4). This led to identifying \cite{noauthor_enccsgpu-programming_nodate}, which
offers an excellent introduction to the topic covering general aspects of GPU programming as well
as specific frameworks.

\begin{table*}[htbp]
	\centering
	\caption{Critical factors of GPU Programming Libraries and Frameworks}
	\label{tab:gpu_libraries}
	\begin{tabular}{llp{8cm}lll}
		\hline
		\small \textbf{Category} & \small \textbf{ID} & \small \textbf{Library/Framework}                                                     & \small \textbf{Vendor} & \small \textbf{Type} & \small \textbf{Ref.}                                  \\
		\hline
		\multirow{4}{*}{\small NVIDIA}
		                         & \small NV1         & \small cuBLAS: GPU-accelerated BLAS library for linear algebra                        & \small NVIDIA          & \small Core          & \small \cite{noauthor_cublas_nodate}                  \\[1ex]
		                         & \small NV2         & \small cuDNN: Optimized deep neural network primitives                                & \small NVIDIA          & \small Core          & \small \cite{chetlur_cudnn_2014}                      \\[1ex]
		                         & \small NV4         & \small NCCL: Multi-GPU/multi-node communication                                       & \small NVIDIA          & \small Core          & \small \cite{noauthor_nvidianccl_2025}                \\[1ex]
		                         & \small NV5         & \small CUTLASS: Optimized C++ templates for matrix multiplication                     & \small NVIDIA          & \small Core          & \small \cite{thakkar_cutlass_2023}                    \\
		\hline
		\multirow{4}{*}{\small AMD}
		                         & \small AMD1        & \small rocBLAS: AMD's BLAS implementation                                             & \small AMD             & \small Core          & \small \cite{noauthor_rocmrocblas_2025}               \\[1ex]
		                         & \small AMD2        & \small MIOpen: Deep learning primitives                                               & \small AMD             & \small Core          & \small \cite{noauthor_rocmmiopen_2025}                \\[1ex]
		                         & \small AMD3        & \small HIP: Portable API for CUDA-like code                                           & \small Multiple        & \small Framework     & \small \cite{noauthor_rocmhip_2025}                   \\[1ex]
		                         & \small AMD4        & \small RCCL: Multi-GPU communication                                                  & \small AMD             & \small Core          & \small \cite{noauthor_rocmrccl_2025}                  \\
		\hline
		\multirow{3}{*}{\small Intel}
		                         & \small INT1        & \small oneMKL: Math Kernel Library                                                    & \small Intel           & \small Core          & \small \cite{noauthor_uxlfoundationonemath_2025}      \\[1ex]
		                         & \small INT2        & \small oneDNN: Deep learning primitives                                               & \small Intel           & \small Core          & \small \cite{onednn_contributors_oneapi_2025}         \\[1ex]
		                         & \small INT3        & \small oneCCL: Collective communication library                                       & \small Intel           & \small Core          & \small \cite{noauthor_uxlfoundationoneccl_2025}       \\
		\hline
		\multirow{3}{*}{\small Cross-Platform}
		                         & \small CP1         & \small OpenCL: Open standard for heterogeneous computing                              & \small Multiple        & \small Framework     & \small \cite{noauthor_khronosgroupopencl-sdk_2025}    \\[1ex]
		                         & \small CP2         & \small SYCL: C++-based multi-device programming                                       & \small Multiple        & \small Framework     & \small \cite{noauthor_khronosgroupsycl-docs_2025}     \\[1ex]
		                         & \small CP3         & \small Kokkos: Performance-portable C++ framework                                     & \small Multiple        & \small Framework     & \small \cite{trott_kokkos_2022}                       \\

		\hline
		\multirow{5}{*}{\small High-Level}
		                         & \small HL1         & \small CUDA.jl: Julia GPU package for NVIDIA                                          & \small NVIDIA          & \small Language      & \small \cite{noauthor_juliagpucudajl_2025}            \\[1ex]
		                         & \small HL2         & \small AMDGPU.jl: Julia GPU package for AMD                                           & \small AMD             & \small Language      & \small \cite{noauthor_juliagpuamdgpujl_2025}          \\[1ex]
		                         & \small HL3         & \small oneAPI.jl: Julia GPU package for Intel                                         & \small Intel           & \small Language      & \small \cite{besard_oneapijl_2022}                    \\[1ex]
		                         & \small HL4         & \small CuPy: NumPy-compatible GPU arrays for NVIDIA/AMD                               & \small Multiple        & \small Language      & \small \cite{okuta_cupy_2017, noauthor_cupycupy_2025} \\[1ex]
		                         & \small HL5         & \small Numba: JIT compiler for GPU acceleration (NVIDIA only)                         & \small NVIDIA          & \small Language      & \small \cite{noauthor_numbanumba_2025}                \\
		\hline
		\multirow{5}{*}{\small Tutorials}
		                         & \small T1          & \small CUDA Toolkit Samples                                                           & \small NVIDIA          & \small Tutorial      & \small \cite{noauthor_nvidiacuda-samples_2025}        \\[1ex]
		                         & \small T2          & \small AMD Lab Notes                                                                  & \small AMD             & \small Tutorial      & \small \cite{noauthor_amdamd-lab-notes_2025}          \\[1ex]
		                         & \small T3          & \small Intel Compute Samples                                                          & \small Intel           & \small Tutorial      & \small \cite{noauthor_intelcompute-samples_2025}      \\
		\hline
		\multirow{5}{*}{\small NN Libraries}
		                         & \small NN1         & \small Caffe: Convolutional Architecture for Fast Feature Embedding                   & \small BVLC            & \small Library       & \small \cite{Jia.EtAl_2014a}                          \\[1ex]
		                         & \small NN2         & \small ImageNet Classification with Deep Convolutional Neural Networks (CUDA-Convnet) & \small AlexNet         & \small Library       & \small \cite{_ag,krizhevsky_imagenet_2012}            \\[1ex]
		                         & \small NN3         & \small Pylearn2: a machine learning research library                                  & \small Pylearn2        & \small Library       & \small \cite{Goodfellow.EtAl_2013}                    \\
		                         & \small NN4         & \small Torch7: A Matlab-like Environment for Machine Learning                         & \small Torch7          & \small Library       & \small \cite{Collobert.EtAl_}                         \\
		\hline
	\end{tabular}
\end{table*}

% ===== STEP 3: Selection of Relevant Studies =====
% This section details: 
% - Study 1: Distributed learning techniques
% - Study 2: CUDA implementations

Given the relevant keywords, I followed an iterative approach using search engines, Github
repositories and documentation pages to identify relevant material (Step S.2.5). This lead to
identifying 21 libraries (S.2.6). The libraries had only 2 accompanying papers
\cite{chetlur_cudnn_2014,okuta_cupy_2017} in the academic literature (S.2.7). The quality
assessment step (S.2.8) ensured that the libraries are relevant to training neural networks and the
resulting papers are shown in Table \ref{tab:gpu_libraries}. The categories that were included
pertain to the main GPU manufacturers: AMD, Intel and NVIDIA. Across each category, libraries for
algebraic operations include \cite{noauthor_cublas_nodate,noauthor_rocmrocblas_2025,
	noauthor_uxlfoundationonemath_2025}, which are useful as building blocks for building more complex
deep learning primitives
\cite{chetlur_cudnn_2014,noauthor_rocmmiopen_2025,onednn_contributors_oneapi_2025}. Existing
approaches that bridge the gap between GPU programming and DNNs are related to communication
protocols such as
\cite{noauthor_nvidianccl_2025,noauthor_rocmrccl_2025,noauthor_uxlfoundationoneccl_2025}. These
frameworks are used in distributed training to synchronize calculations by providing a low-level
API that manages across-GPU communication. Moreover, cross-platform and high-level libraries are
also reported as they allow to effectively build upon core libraries in a faster and more efficient
manner.

\subsection{M.4 -- Reading the studies}
\label{sec:reading-studies}
\subsubsection{Distributed Neural Networks.}

In order to more easily extract useful information from the studies, I identified four key criteria
which aim to facilitate answering the research questions defined in Section
\ref{sec:research_questions_refined}.

\begin{figure*}
	\centering
	\begin{subfigure}{0.48\linewidth}
		\includegraphics[width=\textwidth]{figures/mindmap}
		\caption{Accuracy for the EasyVQA training split.}
		\label{fig:base-a}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.48\linewidth}
		\includegraphics[width=\textwidth]{figures/mindmap-cuda}
		\caption{Accuracy for the DAQUAR training split.}
		\label{fig:base-b}
	\end{subfigure}
	\caption{Accuracy during training/validation on the baseline generator models.}
	\label{fig:base}
\end{figure*}

\begin{itemize}
	\item C1: Key Motivating Factors
	\item C2: Critical Factors and Guidelines
	\item C3: Practical Evaluation Scenarios
	\item C4: Tool Limitations and challenges
\end{itemize}

The resulting artifacts are available in the supplementary material in Table
\ref{tab:dnn_passages}. Each contributing factor is associated with an exact passage and various
codes (categories) that were used to classify the passage. The studies are summarized below.

\textbf{Motivating Factors.}
The motivation for developing DNNs is to improve performance by increasing the scale and complexity
both of the datasets and model architectures. Optimizing speed is crucial because by training larger
models performance is likely to improve \cellrefs{D103} and as a result many applications become viable
\cellrefs{D105}, which otherwise would be infeasible to train.

It has been shown through empirical evidence that increasing the size of the model or dataset,
leads to better performance \cellrefs{D102,D105,D111}. This has been applied in industrial settings
\cellrefs{D101,D106}, and large scale training requirements have been formalized in
\cellrefs{D109,D110}.

Apart from scalability, performance is also a key motivating factor \cellrefs{D103,D105}. Some
studies have shown that increasing the depth of the network overcomes performance bottlenecks
\cellrefs{D103}, ensuring applicability in a wide range of domains. Lastly, resource utilization
also plays an important role in practical scenarios where clusters typically contain heterogeneous
hardware \cellrefs{D104}.

Beyond performance and scale, ease of programming and accessability to a wide range of uses also
play a crucial role. This involves simplifying workflows and enabling access to advanced techniques
to a wider audience \cellrefs{D110,D111,D112}. Nonetheless, one obstacle for usability is the
requirement to evolve existing frameworks and support an increasing number of applications as user
requirements change \cellrefs{D106}. Furthermore, cross-platform and cross-framework support remain
crucial for wider adoption and flexibility \cellrefs{D109,D112}, while at the same time enforcing
innovation through scientific curiosity \cellrefs{D102}.

\textbf{Critical Factors.}
There were considered several critical factors to effectively develop large machine learning systems.
Performance and scalability are among the central concerns \cellrefs{D203,D206,D207,D209,D212}. These
can further be trimmed down to efficiency (not waste resources), speed and the ability to handle
a wide range of architectural choices in distributed environments.

Usability and ease of use are also increasingly considered important for broader adaption and
developer productivity, which includes intuitive programming paradigms and simplified workflows
\cellrefs{D202,D205,D209,D211,D212}.

There are also other factors such as cost and communication efficiency which were also considered
\cellrefs{D209}. Particularly, they have been shown to represent a bottleneck \cellrefs{D204,D210},
and thus are also crucial.

To facilitate all these factors, effective hardware utilization (using both the CPU and GPU) plays
an important role \cellrefs{D201}, as well as software engineering principles like separation of
concerns \cellrefs{D205}, further contribute to robust library design.

\textbf{Evaluation Scenarios.}
Generally, evaluation is done by assessing task performance metrics, which includes accuracy
across various tasks like image classification, vision or NLP \cellrefs{D303,D305,D306,D308,D311}.
Although the measurements regarding the speed, efficiency or resource utilization are important
\cellrefs{D306,D307,D308,D311}, scalability is a key metric that is largely used in the literature
\cellrefs{D306,D308,D311}. In particular, studies such as \cellrefs{D301} assess deployment
in real-world scenarios.

Lately, some algorithms have also focused on cross-framework evaluation to ensure broader
applicability \cellrefs{D304}.

\textbf{Limitations and Challenges.}
The main challenges faced by DNNs include communication overhead and resource under-utilization
\cellrefs{D401,D403,D404,D405,D407,D410}. The lack of standardized tools and frameworks lead
to difficulties regarding programming complexity and ease of use \cellrefs{D402,D403,D408}.

There exist high-level optimization challenges that prevent achieving peak performance, due to
different architectures and hardware configurations \cellrefs{D406,D411}. Moreover, there exist
also algorithmic limitations, tight coupling, and issues related to debugging code
\cellrefs{D405,D406,D408,D411}, which shows that there is still large room for improvement.

Finally, the need for manual tuning and best-configuration selection points towards the need to
create more user-friendly machine learning frameworks \cellrefs{D411}.

\subsubsection{GPU Programming.}
\paragraph{Motivating Factors.}
Below are summarized the motivating factors for the development of GPU programming libraries,
making use of the characteristics displayed in Table \ref{tab:gpu_passages}.

\textbf{Scalability and performance optimizations.}
The development of GPU programming libraries is primarily driven by the need to increase
performance \cellrefs{G1013,G1031,G1051}. This is most often done through focused % Also G1011,G1071
optimizations involving the learning kernels, which can be thought of as the ability to perform
large matrix multiplications quicker or alternatively reducing the need for auxiliary memory. These
improvements are intrinsically linked to scalability and the ability to develop larger and more
powerful architectures \cellrefs{G1011,G1012,G1071}. Optimizing speed implies faster training
on larger datasets, while reducing memory allows to increase the complexity of the network on
the same hardware.

\textbf{Compatibility.}
Another important concern is related to the integration and compatibility of GPU libraries with
existing frameworks \cellrefs{G1013,G1014,G1015,G1062}. This is particularly important as seamless
compatibility streamlines development and promotes wider adoption. As an example, Caffe \cite{Jia.EtAl_2014a}
particularly emphasizes how time-consuming developing optimized code for individual architectures is.
To combat this, frameworks such as CuDNN~\cite{chetlur_cudnn_2014} aim to provide optimized network
implementations for NVidia GPUs across multiple GPU architectures.

\textbf{Usability and programming experience.}
Another important factor involves the usability and programming experience \cellrefs{G1031,G1071},
as they emphasize the creation of tools accessible to a broader audience. This ensures that most
ML libraries can build fast neural network primitives, which in the end can satisfy an increasing
need for more complex applications \cellrefs{G1012,G1016,G1017,G1061}.

\paragraph{Critical factors.}
Performance remains as the key concern encompassing not only raw speed but also portability across
different hardware architectures \cellrefs{G2011,G2012,G2021}.

\textbf{Scalability.}
Scalability is equally important for handling increasingly complex models and datasets \cellrefs{G2011,G2041}.
One factor that greatly contributes to scalability is the ability to fully utilize the available resources
across heterogeneous hardware. This poses significant design constraints as it requires to integrate
both CPUs and GPUs simultaneously during training \cellrefs{G2021,G2041}.

Another important factor that influences scalability is to create modular code through the
separation of concerns as some libraries have highlighted \cellrefs{G2012,G2041}.

One aspect that plays a role in scalability is how efficient inter-GPU communication is in
multi-GPU setups \cellrefs{G2051}. As we shall see this is tightly linked to the choice of the
communication library and is a key concern for both GPU programming and DNN frameworks
\cite{noauthor_uxlfoundationoneccl_2025,noauthor_nvidianccl_2025,noauthor_rocmrccl_2025}.

\textbf{Usability.}
Moreover, broader adoption requires libraries that are easy to use, as this ensures developer productivity
\cellrefs{G2041}.

Also, usability is facilitated by the adoption of declarative programming styles and a focus on
high-level design which excluded boiler-plate code as much as possible \cellrefs{G2012,G2061}.

\paragraph{Evaluation.}
Both quantitative and qualitative studies are performed to assess the performance.

\textbf{Quantitative evaluation.}
Specifically, quantitative metrics assess solutions based on measured performance like convolution speed
and general throughput, frequently benchmarked against established methods \cellrefs{G3011,G3013}.
One approach to assess raw performance involves varying mini-batch sizes against popular neural network
architectures \cellrefs{G3011}. This is related also to portability as it allows to reach top
performance without imposing constraints on architecture choices or the hyperparameters used
\cellrefs{G3012,G3013,G3061}. This is a metric to assess generalization capabilities.

\textbf{Generalization capabilities.}
Given that generalization is important, the documents also touch on the effectiveness of the neural
networks in different domains (i.e. reinforcement learning, computer vision, NLP, etc.), which
ensures broad applicability~\cellrefs{G3012,G3061}.

To ensure fair assessment, the libraries are often evaluated in deployed environments and serve
practical real world applications~\cellrefs{G3041}.

\textbf{Qualitative evaluation.}
These methods are used to assess subjective aspects like the visual quality of the results, as it was
seen in AlexNet~\cellrefs{G3051}. Lastly, it is crucial to keep in mind the context of the evaluation,
as they are usually tailored to specific model architectures and application areas. As a result,
there is always a certain degree of subjectivity involved in the assessment~\cellrefs{G3031}.

\paragraph{Limitations and Challenges.}
Despite progress, GPU programming faces multiple challenges, which arguably can be attributed to 
the lack of a large open-source ecosystem and the use of closed-source proprietary hardware.

\textbf{Developer effort.}
One significant concern is that the kernel optimizations are time-consuming and require specialized
expertise about the dedicated GPU architecture \cellrefs{G4012,G4041}. Also, unless using standardized
libraries (i.e. cuDNN~\cite{chetlur_cudnn_2014}), the replication of results can be challenging as 
execution times can vary significantly \cellrefs{G4041}.

\textbf{Performance bottlenecks.}
Performance bottlenecks are due to small matrix operations, which are generally inefficient \cellrefs{G4051,G4061}, 
but also due to communication overhead in cross-GPU communication \cellrefs{G4051}.

Algorithmic and memory constraints are also present, with some algorithms being limited by high memory usage \cellrefs{G4013},
and others requiring specialized implementations for various corner cases \cellrefs{G4013}.

\textbf{Evolving architectures.}
The field also faces ongoing challenges with evolving architectures, which involves both hardware advancements
but also new neural network innovations, requiring continuous adaptation~\cellrefs{G4011}.


\subsubsection{Distributed Neural Networks}
\subsubsection{Distributed Neural Networks}
In short, studies about distributed neural networks are quite recent and offer interesting insights
into the key factors that motivate the development of DNNs. The key factors for developing such
frameworks range from the drive to improve performance and create general purpose intelligent
systems \cite{chen_mxnet_2015,lepikhin_gshard_2020,shoeybi_megatron-lm_2020} -- by leveraging
increasingly abundant computational resources and data availability -- to \TODO{...}.

\include{tables/translation_motivating_factors}
\include{tables/translation_critical_factors}
\include{tables/translation_evaluation_metrics}
\paragraph{C1. Key Motivating Factors.}
The motivating factors for training DNNs include the pursuit of better performance through more
efficient training and increased usability in practical scenarios. It has been widely regarded that
by scaling the architectures to a large number of parameters and leveraging larger datasets, the
evaluation accuracy on many benchmarks would be improved \cite{hestness_deep_2017}. However, it was
recently demonstrated by Deepseek R1
\cite{deepseekai2025deepseekr1incentivizingreasoningcapability} that scaling up the computational
power is not the only possible method of progress. As a result, future research is likely to focus
not only on distributed training, but also on innovating existing architectures. Nonetheless, below
is a review of the key properties

\textbf{Performance.}
For example, this was demonstrated in GShard \cite{kaplan_scaling_2020} and GPipe
\cite{huang_gpipe_2019} also shows that "by increasing the model capacity from 400M params to 1.3B,
and further to 6B, leads to significant quality improvements across all languages". In particular,
the NLP field is particularly sensitive to the model capacity, as shown in Megatron-LM
\cite{shoeybi_megatron-lm_2020}, noting that "empirical evidence indicates that larger language
models are dramatically more useful for NLP tasks such as article completion, question answering
and natural language inference". The need for larger models is not limited to text generation
tasks, as GPipe was also motivated by the desire to improve translation quality through increased
model size in low-resource languages \cite{huang_gpipe_2019}.

\textbf{Resource Utilization.}
However, the pursuit of larger models introduces challenges related to efficient training. The
BytePS paper \cite{jiang_unified_nodate} shows that existing frameworks that utilize solely the
CPU or the GPU are suboptimal. It demonstrates that utilizing all available resources leads to
significant performance improvements, where the CPU is used to perform summation of gradients,
while the parameter updates are performed on the GPU. BytePS outperforms all-reduce (traditional)
methods significantly both when CPUs are or are not available.

\textbf{Efficiency.}
The need for efficient training is also reflected in the development of systems like Tensorflow
\cite{abadi_tensorflow_2016} and Ray \cite{moritz_ray_2018}, both of which aimed to provide
general-purpose deep learning frameworks to address the needs of large communities. DeepSpeed
\cite{rasley_deepspeed_2020} was also created to facilitate training of large models with over 100
billion parameters. Deepspeed utilizes PyTorch \cite{noauthor_pytorchpytorch_nodate} as the
underlying framework in order to "make distributed training and inference easy, efficient, and
effective".

\textbf{Model and Pipeline Parallelism.}
In frameworks such as Pytorch DDP \cite{li_pytorch_2020}, the technique they used for distributed
training involved data parallelism (see \ref{sec:related_work}), which required the entire model to
fit on a single GPU device. This had the inconvenience that if a model did not fit in memory, the
approach would not be feasible. To address this issue, techniques such as pipeline parallelism
\cite{huang_gpipe_2019} and model parallelism \cite{shoeybi_megatron-lm_2020} were developed to
allow the effective training of models that are too large to fit on a single device.

\textbf{Usability.}
Another key aspect of motivation is practical usability. Frameworks like PyTorch \cite{li_pytorch_2020}
and Horovod \cite{sergeev_horovod_2018} ensured that distributed training could easily be integrated
in existing code with minimal changes. Huggingface Transformers \cite{wolf_huggingfaces_2020} was
explicitly designed to make NLP models easy to download, fine-tune and share. Colossal-AI
\cite{li_colossal-ai_2023} sought to create a unified system to simplify the complexities of
large scale distributed training. It was designed to improve on existing work such as the Alpa library
\cite{noauthor_alpa-projectsalpa_nodate}, which was discontinued in 2024.

Finally, MxNet \cite{chen_mxnet_2015} aimed to provide a uniform programming interface by bridging
the gap between imperative and declarative programming, allowing users to express computations in a
variety of styles.

\paragraph{C2. Critical Factors and Guidelines.}
This section aims to identify which factors were critical in the development of each library, and
to some degree what each library excels at.

\textbf{Review of key concepts.}
Scalability can be achieved through two inter-related ideas. First, it is possible to increase the number of parameters in the
model, yielding more powerful architectures, that tend to be more general-purpose. Second, it is also possible to increase the
amount of data being used for training, which yields better generalization capabilities and a lesser chance of
overfitting\footnote{Overfitting relates to the idea that the model learns the training data too well, and as a result,
	performs poorly on unseen examples.}.

The former approach is implemented through a technique called {\em{Model parallelism}} (and its
extension {\em{Pipeline parallelism}}) by distributing layers of the model across the GPU cluster,
where each individual GPU is responsible for computing the gradients of a portion of the model.
Pipeline parallelism is a technique that extends model parallelism by also dividing the processing
of a batch of data across the GPUs. This means that while one GPU is working on a segment of the
data batch through its assigned model layers, the other GPUs may work on different segments through
the layers that were assigned to them.

On the other hand, the later approach is implemented through {\em{Data parallelism}}. In this case,
the model is replicated (exact copy) across the GPU cluster, and each GPU is responsible for
computing the gradients of portions of the dataset. However, in this case, the model must be small
enough to fit in the memory of a single GPU.

For more information on the differences the reader is invited to refer to
\cite{dehghani_distributed_2023}.

\textbf{Scalability.}
GShard \cite{lepikhin_gshard_2020} uses model parallelism to scale up to large
datasets, innovating existing frameworks with techniques such as conditional computation and
automatic sharding. The former allows to activate only a sub-network of the larger model on
per-input basis, while the later allows to automatically partition a neural network model across
multiple GPUs without the need for human intervention.

Similarly, Colossal-AI \cite{li_colossal-ai_2023} and Megatron-LM \cite{shoeybi_megatron-lm_2020}
show methods to scale up large architecture through model parallelism, leading to models of up to
8.3 billion parameters.

Conversely, GPipe \cite{huang_gpipe_2019} introduces pipeline parallelism allowing to "split the
model into several chunks of consecutive layers and each chunk is allocated to a device" and "as a
result, this method reduces cross-node communication", leading to higher throughput. This leads to
training models of up to 83 billion parameters.

\textbf{Performance.}
The immediate thought that comes to mind when pondering over performance optimizations of a GPU cluster involves
efficient communication protocols that reduce the communication overhead or optimizations involving the
both the model or the data. Pytorch DDP \cite{li_pytorch_2020} attempts to optimize data parallelism by
using techniques like bucketing gradients and overlapping communication with computation.

Megatron-LM \cite{shoeybi_megatron-lm_2020}, aims to reduce communication and keep the GPUs busy by
duplicating computations across GPUs, technique called fused operations. GShard
\cite{lepikhin_gshard_2020} aims to give the programmer enough flexibility to easily change the
parallelisation strategy (through automatic sharding), without having to heavily modify existing
code.

\textbf{Automatic Configuration.}
This allows the library to adapt to diverse hardware setups and automatically configure the best
backend library when executing code. This is a connection between GPU programming and DNN frameworks.

For instance, Tensorflow \cite{abadi_tensorflow_2016} and MxNet \cite{chen_mxnet_2015} can
autonomously choose the most efficient algorithms considering different hardware configurations,
allowing seamless integration for mobile devices or large GPU clusters.

However, BytePS \cite{jiang_unified_nodate} attempts to improve the communication overhead by
leveraging heterogeneous GPU/CPU clusters. This is done by analyzing the network traffic for
finding the optimal way to allocate resources. Similarly, GPipe \cite{huang_gpipe_2019} can also
leverage automatic communication strategies achieving task independent parallelism in heterogeneous
environments (clusters featuring different hardware configurations).

\textbf{Ease of use.}
This category is primarily focused towards creating tools that enable practitioners and researchers to easily
use state-of-the-art models. The Huggingface Transformers \cite{wolf_huggingfaces_2020} is a prime example
that provides access to open-source models through the Huggingface hub \cite{noauthor_hugging_2025}.
Finally, Tensorflow \cite{abadi_tensorflow_2016} provides Tensorboard, which is a tool for recording
experimental data in a distributed cluster.

\paragraph{C3. Practical Evaluation Scenarios.}
The evaluation factors can be classified into the following categories:

\textbf{Scalability and Performance.}
For scaling neural networks across GPU clusters, the ideal scenario involves achieving linear speedup
with respect to the setting involving a single machines (with possibly multiple GPUs). The difficulties
that arise as a result of network communication are due to limitations involving the bandwidth of the
network making linear speedup challenging to achieve. Some libraries have shown demonstrated
effective techniques for reducing the communication overhead.

% Concepts definition checkpoint
%\textbf{Near Linear Speedup.}
GPipe \cite{huang_gpipe_2019}, GShard \cite{lepikhin_gshard_2020} and Pytorch DDP
\cite{li_pytorch_2020} achieve "near linear" speedup by leveraging pipeline parallelism and model
parallelism. For example, Pytorch DDP achieved this by aggregating gradients into buckets for
communication and overlapping communication with computation, over a cluster with 256 GPUs. GShard
took a practical approach by assessing the trade-offs between model size, training time and
resulting accuracy.

%\textbf{Heterogeneous Clusters.}
Other frameworks such as BytePS \cite{jiang_unified_nodate} excel at optimizing communication
overhead in heterogenous GPU/CPU clusters. They outperformed Parameter Server and All-Reduce
methods by scaling GPU clusters to 2048 devices and training for a time period of 4 days.

%\textbf{Efficiency Techniques.}
Effective techniques such as automatic parallelization, sharding and offloading have been proposed
by Colossal-AI \cite{li_colossal-ai_2023}, effectively training models of up to 13B parameters.

%\textbf{Reinforcement Learning.}
Other libraries such as Ray \cite{moritz_ray_2018} focused on specialized techniques for training
reinforcement learning models.

\textbf{}

\begin{table*}[ht]

	\centering
	\caption{Concepts recurring in the papers.}
	\label{tab:concepts}
	\resizebox{\textwidth}{!}{%
		\footnotesize
		\begin{tabular}{lll} % NOTE: concepts definition checkpoint
			\hline
			\textbf{Area}    & \textbf{Concept}                           & \textbf{Definition} \\
			\hline
			\multirow{8}{*}{DNNs}
			                 & Bucket Gradient Aggregation                & \makecell[l]{...}   \\ % Pytorch DDP \cite{li_pytorch_2020}
			                 & Overlapping Communication with Computation & ...                 \\ % Pytorch DDP \cite{li_pytorch_2020}
			                 & Heterogeneous Clusters                     & ...                 \\ % BytePS \cite{jiang_unified_nodate}
			                 & Parameter Server                           & ...                 \\ % BytePS \cite{jiang_unified_nodate}
			                 & All-Reduce                                 & ...                 \\ % BytePS \cite{jiang_unified_nodate}
			                 & Automatic Parallelization                  & ...                 \\ % Colossal-AI \cite{li_colossal-ai_2023}
			                 & Offloading                                 & ...                 \\ % Colossal-AI \cite{li_colossal-ai_2023}
			                 & Sharding                                   & ...                 \\ % Colossal-AI \cite{li_colossal-ai_2023}
			\hline
			\multirow{2}{*}{GPU Programming}
			                 & Model Parallelism                          &                     \\
			                 & Model Parallelism                          &                     \\
			\hline
			Data Parallelism & Data Parallelism                           &                     \\
			\hline
		\end{tabular}
	}
\end{table*}

\textbf{Achieved accuracy.}

\subsubsection{Distributed Learning Techniques Review}
The review will focus on distributed learning approaches, aligning with "Study 1" in Figure
\ref{fig:workflow}, with the following considerations:
\begin{itemize}
	\item Types of algorithms including \textbf{data parallelism, model parallelism, and asynchronous
		      Stochastic Gradient Descent (SGD)} \cite{ben-nun_demystifying_2020,langer_distributed_2020}.
	\item Different distributed architectures including parameter servers and peer-to-peer systems
	      \cite{verbraeken_survey_2021,ben-nun_demystifying_2020,langer_distributed_2020}.
	\item Specific machine learning models such as neural networks and support vector machines.
\end{itemize}

\subsubsection{CUDA-based Parallel Implementation Review}
For CUDA implementations, the review will consider aspects relevant to "Study 2" in Figure
\ref{fig:workflow}:
\begin{itemize}
	\item Implementation of distributed methods on NVIDIA GPUs using the CUDA framework
	\item Different CUDA libraries and architectures
	\item Specific hardware considerations including GPUs and Tensor Processing Units (TPUs)
\end{itemize}

\subsubsection{Justification for Inclusion}
Both distributed learning techniques and CUDA implementation studies will be included to provide a
complete picture of the current state-of-the-art research in the area. By including both study
types, a deeper understanding of both theoretical approaches and implementation techniques for
practical applications can be reached.

\subsection{Preliminary Protocol Development}
This systematic review follows the guidelines proposed by Kitchenham and Charters for software
engineering research. The preliminary review protocol was developed to establish the foundation for
the steps visualized in Figure \ref{fig:workflow}, particularly in the initial stages. An overview
of the papers included after the initial selection phase (corresponding to the output of the
"Studies Selection" phase in Figure \ref{fig:workflow}) will be presented in Table 2.1.

\subsubsection{Background and Rationale}
This section provides the necessary context for the review, outlining the research gaps that will
be addressed \cite{ben-nun_demystifying_2020}. It explicitly states the need for a systematic
review of the current literature to address this gap and provide a focused analysis.

\subsubsection{Initial Search Strategy}
The initial search strategy involves combining keywords using Boolean and proximity operators to
generate search strings, based on the "Goals, expected outputs, constraints, search terms and
keywords" documented as an input to Step 1 in Figure \ref{fig:workflow}. Databases like Scopus,
Google Scholar, and ACM Digital Library are selected for their coverage of computer science,
engineering, and applied mathematics literature. Studies published between 2015-2022 will be
considered to ensure recent advancements are included while maintaining a consistent period for
analysis.
\begin{itemize}
	\item \textbf{Search Terms:} Details of the search terms will be provided in Section \ref{sec:search_process_documentation}.
	\item \textbf{Database Justification:} Rationale for selecting specific databases is detailed in Section \ref{sec:search_process_documentation}.
	\item \textbf{Timeline:} The timeframe for including studies is 2015-2022.
\end{itemize}

\subsubsection{Preliminary Selection Criteria}
Preliminary criteria for inclusion will use specific examples such as ``studies that evaluate the
performance of synchronous distributed SGD in deep learning models'' rather than general terms like
``distributed computing'' \cite{ben-nun_demystifying_2020}. Preliminary quality thresholds will
ensure only high-quality studies are included in the final analysis. Specific inclusion and
exclusion criteria are detailed in Section \ref{sec:study-selection-criteria}.

\subsubsection{Initial Data Extraction Plan}
The following information will be extracted from each study:
\begin{itemize}
	\item Details of distributed systems \cite{ben-nun_demystifying_2020,langer_distributed_2020}:
	      \begin{enumerate}
		      \item Number of nodes
		      \item Communication network
		      \item Communication method
		      \item Topology
	      \end{enumerate}
	\item Machine learning algorithms and models used \cite{xing_strategies_2015}.
	\item Datasets and benchmarks \cite{ben-nun_demystifying_2020}.
	\item Performance metrics (training time, accuracy, speedup)
	      \cite{ben-nun_demystifying_2020,langer_distributed_2020,xing_strategies_2015}.
	\item CUDA implementation details (libraries, optimizations)
	      \cite{verbraeken_survey_2021,ben-nun_demystifying_2020,xing_strategies_2015}.
\end{itemize}
Further details on the data extraction strategy can be found in Section \ref{sec:data-extraction-strategy}.

\subsubsection{Quality Assessment Framework}
Preliminary quality assessment will use specific criteria to evaluate the validity and reliability
of methods, using established checklists from the literature \cite{ben-nun_demystifying_2020}. A
Likert scale will be used for a standardized approach. Detailed guidelines for reviewers will be
established to ensure consistency and prevent bias, as described further in Section
\ref{sec:quality-assessment-process}.
\begin{itemize}
	\item \textbf{Criteria:} Specific criteria are detailed in Section \ref{sec:quality-assessment-process}.
	\item \textbf{Scoring System:} A Likert scale will be used.
	\item \textbf{Guidelines:} Guidelines for reviewers are detailed in Section \ref{sec:quality-assessment-process}.
\end{itemize}

\subsubsection{Synthesis Approach}
The synthesis approach will involve meta-analysis where appropriate, using statistical analysis to
combine results from included studies with clearly defined methods
\cite{ben-nun_demystifying_2020}. Thematic synthesis will be used for narrative synthesis, allowing
an in-depth understanding of themes present in selected studies.
\begin{itemize}
	\item \textbf{Meta-analysis:} Details of the methods will be defined later.
	\item \textbf{Narrative synthesis:} Thematic synthesis will be employed.
\end{itemize}

\subsection{Search Process Documentation}
\label{sec:search_process_documentation}
This section provides an overview of our search strategy. Detailed search documentation, including exact search strings and results for each database, can be found in the supplementary materials (Section \ref{sec:search_strategy}).

\subsubsection{Search Strategy Overview}
Our search strategy combines terms from two main categories as shown in Table
\ref{tab:search_terms}. The number of articles retrieved from each database is presented in Table
\ref{tab:search_results}.

\begin{table*}[ht]
	\centering
	\caption{Number of Retrieved Articles by Database}
	\label{tab:search_results}
	\begin{tabular}{|l|c|c|c|}
		\hline
		\textbf{Database}   & \textbf{Initial Results} & \textbf{After Filtering} & \textbf{Final Selection} \\
		\hline
		Scopus              & XXX                      & XXX                      & XXX                      \\
		\hline
		Google Scholar      & XXX                      & XXX                      & XXX                      \\
		\hline
		ACM Digital Library & XXX                      & XXX                      & XXX                      \\
		\hline
		IEEE Xplore         & XXX                      & XXX                      & XXX                      \\
		\hline
		Science Direct      & XXX                      & XXX                      & XXX                      \\
		\hline
		arXiv               & XXX                      & XXX                      & XXX                      \\
		\hline
		\textbf{Total}      & XXX                      & XXX                      & XXX                      \\
		\hline
	\end{tabular}
\end{table*}

The complete search strings for each database, including any database-specific adaptations, are
documented in Section \ref{sec:search_strategy} of the supplementary materials.

\subsection{Study Selection Criteria}
\label{sec:study-selection-criteria}
This section specifies the detailed inclusion and exclusion criteria for the studies to be included
in the review. These will be specific, measurable, and objective to ensure that all studies are
assessed consistently and fairly \cite{ben-nun_demystifying_2020}.

% TODO: Razvan. Is this fine?
\subsubsection{Inclusion Criteria}
The following criteria will be used for including studies:
\begin{itemize}
	\item Studies published between 2013 and 2023
	\item Peer-reviewed articles and high-quality preprints
	\item Studies focusing on distributed training techniques
	\item Articles written in English
	\item Implementation details available
\end{itemize}
\cite{verbraeken_survey_2021,ben-nun_demystifying_2020}.

\subsubsection{Exclusion Criteria}
Studies will be excluded based on the following criteria:
\begin{itemize}
	\item Studies not focused on neural network training
	\item Pure theoretical papers without implementation
	\item Secondary studies (surveys, reviews)
	\item Insufficient technical details or results
\end{itemize}

\subsection{Quality Assessment Process}
\label{sec:quality-assessment-process}
This part of the methodology details the process used to assess the quality of the selected studies, aligning with the "Validation" phase shown in the bottom section of Figure \ref{fig:workflow}.

\subsubsection{Quality Criteria}
The quality of the studies will be evaluated based on the methodological rigour, clarity of
reporting, limitations of the studies, and potential for bias. Established checklists, such as
those provided by the CASP, will be used to address bias and validity in a rigorous and systematic
way.

\subsection{Data Extraction Strategy}
\label{sec:data-extraction-strategy}
This section details how data will be extracted from the included studies. The data extraction form will be designed to capture all necessary information, including study details, methodology, implementation specifics, dataset details, and results. The form will be piloted to ensure it captures the information effectively \cite{ben-nun_demystifying_2020}.

% \subsubsection{Extraction Form}
% The data extraction form was iteratively refined through:

% TODO Razvan
% \begin{itemize}
%     \item Publication metadata
%     \begin{itemize}
%         \item Authors, venue, year
%         \item Citation count and impact
%     \end{itemize}
%     \item Technical details
%     \begin{itemize}
%         \item Distributed training approach
%         \item Implementation specifications
%         \item Hardware configurations
%     \end{itemize}
%     \item Performance metrics
%     \begin{itemize}
%         \item Training time and convergence
%         \item Resource utilization
%         \item Scalability measures
%     \end{itemize}
%     \item Experimental setup
%     \begin{itemize}
%         \item Dataset characteristics
%         \item Hardware specifications
%         \item Software frameworks used
%     \end{itemize}
% \end{itemize}

\begin{itemize}
	\item \textbf{Details:} The data extraction form will capture study design, participants, interventions, and outcomes, as well as details of the implementation in distributed systems and parallel CUDA frameworks \cite{ben-nun_demystifying_2020}.
	\item \textbf{Piloting:} The extraction form will be piloted to ensure effectiveness \cite{ben-nun_demystifying_2020}.
	\item \textbf{Study Details:} The form will capture relevant details including implementation specifics.
\end{itemize}

% Final paragraph about methodology
By using this methodology, the systematic review will aim to provide a comprehensive and reliable
analysis of the current state of research in distributed deep learning and CUDA implementations.
The approach used here will enable the review to identify any trends and gaps in current research
and make recommendations for future study.

\subsection{Study Selection Results}
\label{sec:study_selection_results}
The complete lists of included and excluded studies, along with detailed information about each study, can be found in the supplementary materials (Section \ref{sec:study_selection}). A total of XXX studies were initially identified, with XXX studies meeting our inclusion criteria after screening. Table \ref{tab:study_types} provides an overview of the types of studies included in our review.

\begin{table*}[ht]
	\centering
	\caption{Overview of Included Study Types}
	\label{tab:study_types}
	\begin{tabular}{|l|c|p{8cm}|}
		\hline
		\textbf{Study Type}    & \textbf{Count} & \textbf{Description}                                                     \\
		\hline
		Empirical Studies      & XXX            & Studies with experimental evaluations of distributed learning techniques \\
		\hline
		Implementation Studies & XXX            & Studies focusing on CUDA implementations and optimizations               \\
		\hline
		Hybrid Studies         & XXX            & Studies covering both theoretical and implementation aspects             \\
		\hline
		\textbf{Total}         & XXX            &                                                                          \\
		\hline
	\end{tabular}
\end{table*}

\begin{itemize}
	\item \textbf{Goals:}
	\item To analyze parallelization frameworks in DDL.
	\item To evaluate libraries that support CUDA programming.
	\item To find out how these concepts are intertwined.
	\item To get practical experience with popular frameworks. \\
	\item \textbf{Expected Outputs:}
	\item Experience for conducting a systematic review.
	\item Systematic mapping of programming frameworks.
	\item Comparative analysis with strengths and weaknesses.
	\item Identification of research gaps. \\
	\item \textbf{Constraints:}
	\item Time period limited to 2012-2024\footnote{CUDA: 2012 is the year when AlexNet was published.} and
	      2015-2024\footnote{DDL: 2015 is the year when the shift towards scalability was noticed.}.
	\item Peer-reviewed articles and conference papers only.
	\item English language publications only.
	\item Technical implementation details must be present. \\
	\item \textbf{Search Terms and Keywords:}
	\item Listed in Table \ref{tab:search_terms}.
\end{itemize}

% \begin{table*}[htbp]
% 	\centering
% 	\caption{The DNN papers included in the review}
% 	\label{tab2:dnn_papers}
% 	\begin{tabular}{llp{8.01cm}p{2cm}lll}
% 		\hline
% 		\textbf{\#} & \textbf{Ref.}           & \textbf{Title}                                                                & \textbf{Type} & \textbf{Year} & \textbf{Citations} & \textbf{Stars}                             \\
% 		\hline
% 		%1           & \cite{huang_gpipe_2019} & GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism & Pipeline      & 2018          & 1446               & 2.8k \cite{noauthor_tensorflowlingvo_2025} \\
% 		2           & \cite{chen_mxnet_2015}  & MXNet: A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems & Hybrid (Data, Model) & 2015 & 2212 & 20.8k \cite{noauthor_apachemxnet_2025} \\
% 		% 3           & \cite{li_pytorch_2020}  & PyTorch Distributed: Experiences on Accelerating Data Parallel Training & Data & 2020 & 175 & 86.1k \cite{noauthor_pytorchpytorch_nodate} \\
% 		% 4           & \cite{abadi_tensorflow_2016} & TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems & Data & 2016 & 9998 & 187k \cite{abadi_tensorflow_2015} \\
% 		% 5           & \cite{rasley_deepspeed_2020} & DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters & Hybrid (Data, Model) & 2020 & 1059 & 36.3k \cite{noauthor_microsoftdeepspeed_2025} \\
% 		% 6           & \cite{sergeev_horovod_2018} & Horovod: fast and easy distributed deep learning in TensorFlow & Data & 2018 & 1152 & 14.3k \cite{noauthor_horovodhorovod_2025} \\
% 		% 7           & \cite{jiang_unified_nodate} & A Unified Architecture for Accelerating Distributed DNN Training in Heterogeneous GPU/CPU Clusters & Data & 2020 & 10 & 3.7k \cite{noauthor_bytedancebyteps_2025} \\
% 		% 8           & \cite{frostig_compiling_nodate} & Compiling machine learning programs via high-level tracing & Data & 2018 & 363 & 31k \cite{noauthor_jax-mljax_2025} \\
% 		\hline
% 		% Add more papers as needed
% 	\end{tabular}
% \end{table*}


\begin{abstract}
	This paper presents a systematic literature review of distributed and parallel techniques for running
	deep neural networks on multiple machines and GPUs. It is composed of three parts: 1) a review of the
	available libraries that enable distributed training across GPU clusters, 2) a review of the
	most popular frameworks that facilitate parallelizing the training process on GPUs, and 3) a
	practical section that demonstrates a proof-of-concept implementation of the training process
	using common frameworks, namely PyTorch DDP and cuDNN.

	The review synthesizes research from the past decade, examining various approaches to distributed
	training, their effectiveness, and implementation challenges. The work is aimed at students and
	practitioners, with the goal to provide an introduction to the topic and help create a general idea
	of the most common libraries in each domain.

	The distributed experiments use data parallelism to accelerate the training process, while the GPU
	experiments use cuDNN, cuBLAS and manual kernel implementations to train a small network. The
	effectiveness of each approach is demonstrated and to aid reproduction and experimentation Docker
	environments are provided. This allows to simulate a multi-GPU setup on a single NVidia GPU, promoting
	ease of use by not relying on cloud services. The repository is available at 
	\url{https://github.com/atomwalk12/deep-bridge-survey}.
\end{abstract}

% Due to its many applications across various fields of research, engineering, and daily life, deep learning has seen a surge in popularity. Therefore, larger and more expressive models have been proposed, with examples like Turing-NLG using as many as 17 billion parameters. Training these very large models becomes increasingly difficult due to the high computational costs and large memory footprint. Therefore, several approaches for distributed training based on data parallelism (e.g., Horovod) and model/pipeline parallelism (e.g., GPipe, PipeDream) have emerged.  In this work, we focus on an in-depth comparison of three different parallelism models that address these needs: data, model and pipeline parallelism.

% Because training a deep neural network (DNN) takes arduous amounts of time and computation, often researchers expedite the training process via distributed parallel training on GPUs.
% On one hand, this lower computing-to-communication ratio makes traditional data parallelism difficult to scale, and traditional model parallelism leads to low GPU utilization. Both make it difficult to obtain a higher speedup. On the other hand, multi-GPU systems exhibit complex connectivity among GPUs.

% Experiments with four different DNN models show that Pipetorch averages 1.4x speedup compared to data parallelism. 

% TODO Razvan: explain model, data, pipeline and hybrid parallelism.

%  We demonstrate FCUBE executing five different learners contributed by three different machine learning groups on a 100 node deployment on Amazon EC2. They collectively solve a publicly available classification problem trained with 11 million exemplars from the Higgs dataset. 
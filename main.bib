
@misc{Jia.EtAl_2014a,
	title = {Caffe: {Convolutional} {Architecture} for {Fast} {Feature} {Embedding}},
	shorttitle = {Caffe},
	howpublished = {\url{http://arxiv.org/abs/1408.5093}},
	doi = {10.48550/arXiv.1408.5093},
	abstract = {Caffe provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models. The framework is a BSD-licensed C++ library with Python and MATLAB bindings for training and deploying general-purpose convolutional neural networks and other deep models efficiently on commodity architectures. Caffe fits industry and internet-scale media needs by CUDA GPU computation, processing over 40 million images a day on a single K40 or Titan GPU (\${\textbackslash}approx\$ 2.5 ms per image). By separating model representation from actual implementation, Caffe allows experimentation and seamless switching among platforms for ease of development and deployment from prototyping machines to cloud environments. Caffe is maintained and developed by the Berkeley Vision and Learning Center (BVLC) with the help of an active community of contributors on GitHub. It powers ongoing research projects, large-scale industrial applications, and startup prototypes in vision, speech, and multimedia.},
	urldate = {2025-02-12},
	publisher = {arXiv},
	author = {Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff and Karayev, Sergey and Long, Jonathan and Girshick, Ross and Guadarrama, Sergio and Darrell, Trevor},
	month = jun,
	year = {2014},
	note = {arXiv:1408.5093 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: Tech report for the Caffe software at http://github.com/BVLC/Caffe/},
	file = {Preprint PDF:/home/atomwalk12/Zotero/storage/SN2YM3B4/Jia et al. - 2014 - Caffe Convolutional Architecture for Fast Feature Embedding.pdf:application/pdf;Snapshot:/home/atomwalk12/Zotero/storage/9JPIA7FR/1408.html:text/html},
}


@misc{_ag,
	title = {Google {Code} {Archive} - {Long}-term storage for {Google} {Code} {Project} {Hosting}.},
	howpublished = {\url{https://code.google.com/archive/p/cuda-convnet/}},
	author = {Alex Krizhevsky},
	year = {2011},
	urldate = {2025-02-12},
	file = {Google Code Archive - Long-term storage for Google Code Project Hosting.:/home/atomwalk12/Zotero/storage/STY6QLEZ/cuda-convnet.html:text/html},
}

@misc{Goodfellow.EtAl_2013,
	title = {Pylearn2: a machine learning research library},
	shorttitle = {Pylearn2},
	howpublished = {\url{http://arxiv.org/abs/1308.4214}},
	doi = {10.48550/arXiv.1308.4214},
	abstract = {Pylearn2 is a machine learning research library. This does not just mean that it is a collection of machine learning algorithms that share a common API; it means that it has been designed for flexibility and extensibility in order to facilitate research projects that involve new or unusual use cases. In this paper we give a brief history of the library, an overview of its basic philosophy, a summary of the library's architecture, and a description of how the Pylearn2 community functions socially.},
	urldate = {2025-02-12},
	publisher = {arXiv},
	author = {Goodfellow, Ian J. and Warde-Farley, David and Lamblin, Pascal and Dumoulin, Vincent and Mirza, Mehdi and Pascanu, Razvan and Bergstra, James and Bastien, Frédéric and Bengio, Yoshua},
	month = aug,
	year = {2013},
	note = {arXiv:1308.4214 [stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Mathematical Software, Statistics - Machine Learning},
	annote = {Comment: 9 pages},
	file = {Preprint PDF:/home/atomwalk12/Zotero/storage/STWGBBZY/Goodfellow et al. - 2013 - Pylearn2 a machine learning research library.pdf:application/pdf;Snapshot:/home/atomwalk12/Zotero/storage/4PQE3ASF/1308.html:text/html},
}

@misc{Sermanet.EtAl_2014,
	title = {{OverFeat}: {Integrated} {Recognition}, {Localization} and {Detection} using {Convolutional} {Networks}},
	shorttitle = {{OverFeat}},
	howpublished = {\url{http://arxiv.org/abs/1312.6229}},
	doi = {10.48550/arXiv.1312.6229},
	abstract = {We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat.},
	urldate = {2025-02-12},
	publisher = {arXiv},
	author = {Sermanet, Pierre and Eigen, David and Zhang, Xiang and Mathieu, Michael and Fergus, Rob and LeCun, Yann},
	month = feb,
	year = {2014},
	note = {arXiv:1312.6229 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:/home/atomwalk12/Zotero/storage/HJ58E6SA/Sermanet et al. - 2014 - OverFeat Integrated Recognition, Localization and Detection using Convolutional Networks.pdf:application/pdf;Snapshot:/home/atomwalk12/Zotero/storage/88ZZZHJK/1312.html:text/html},
}

@article{Collobert.EtAl_,
	title = {Torch7: {A} {Matlab}-like {Environment} for {Machine} {Learning}},
	abstract = {Torch7 is a versatile numeric computing framework and machine learning library that extends Lua. Its goal is to provide a ﬂexible environment to design and train learning machines. Flexibility is obtained via Lua, an extremely lightweight scripting language. High performance is obtained via efﬁcient OpenMP/SSE and CUDA implementations of low-level numeric routines. Torch7 can easily be interfaced to third-party software thanks to Lua’s light interface.},
	language = {en},
	howpublished = {\url{https://ronan.collobert.com/pub/matos/2011_torch7_nipsw.pdf}},
	author = {Collobert, Ronan and Kavukcuoglu, Koray and Farabet, Clement},
	file = {PDF:/home/atomwalk12/Zotero/storage/P28YHQ8G/Collobert et al. - Torch7 A Matlab-like Environment for Machine Learning.pdf:application/pdf},
}

@misc{chetlur_cudnn_2014,
	title = {{cuDNN}: {Efficient} {Primitives} for {Deep} {Learning}},
	shorttitle = {{cuDNN}},
	howpublished = {\url{http://arxiv.org/abs/1410.0759}},
	doi = {10.48550/arXiv.1410.0759},
	abstract = {We present a library of efficient implementations of deep learning primitives. Deep learning workloads are computationally intensive, and optimizing their kernels is difficult and time-consuming. As parallel architectures evolve, kernels must be reoptimized, which makes maintaining codebases difficult over time. Similar issues have long been addressed in the HPC community by libraries such as the Basic Linear Algebra Subroutines (BLAS). However, there is no analogous library for deep learning. Without such a library, researchers implementing deep learning workloads on parallel processors must create and optimize their own implementations of the main computational kernels, and this work must be repeated as new parallel processors emerge. To address this problem, we have created a library similar in intent to BLAS, with optimized routines for deep learning workloads. Our implementation contains routines for GPUs, although similarly to the BLAS library, these routines could be implemented for other platforms. The library is easy to integrate into existing frameworks, and provides optimized performance and memory usage. For example, integrating cuDNN into Caffe, a popular framework for convolutional networks, improves performance by 36\% on a standard model while also reducing memory consumption.},
	urldate = {2024-12-19},
	publisher = {arXiv},
	author = {Chetlur, Sharan and Woolley, Cliff and Vandermersch, Philippe and Cohen, Jonathan and Tran, John and Catanzaro, Bryan and Shelhamer, Evan},
	month = dec,
	year = {2014},
	note = {arXiv:1410.0759 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Mathematical Software, Computer Science - Neural and Evolutionary Computing},
	file = {Preprint PDF:/home/atomwalk12/Zotero/storage/RL3D3MXP/Chetlur et al. - 2014 - cuDNN Efficient Primitives for Deep Learning.pdf:application/pdf;Snapshot:/home/atomwalk12/Zotero/storage/EBSM39I9/1410.html:text/html},
}

@misc{krizhevsky_one_2014,
	title = {One weird trick for parallelizing convolutional neural networks},
	howpublished = {\url{http://arxiv.org/abs/1404.5997}},
	doi = {10.48550/arXiv.1404.5997},
	abstract = {I present a new way to parallelize the training of convolutional neural networks across multiple GPUs. The method scales significantly better than all alternatives when applied to modern convolutional neural networks.},
	urldate = {2024-12-19},
	publisher = {arXiv},
	author = {Krizhevsky, Alex},
	month = apr,
	year = {2014},
	note = {arXiv:1404.5997 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {Preprint PDF:/home/atomwalk12/Zotero/storage/ZQGF4UVN/Krizhevsky - 2014 - One weird trick for parallelizing convolutional neural networks.pdf:application/pdf;Snapshot:/home/atomwalk12/Zotero/storage/QUFUCVLP/1404.html:text/html},
}

@misc{jia_caffe_2014,
	title = {Caffe: {Convolutional} {Architecture} for {Fast} {Feature} {Embedding}},
	shorttitle = {Caffe},
	howpublished = {\url{http://arxiv.org/abs/1408.5093}},
	doi = {10.48550/arXiv.1408.5093},
	abstract = {Caﬀe provides multimedia scientists and practitioners with a clean and modiﬁable framework for state-of-the-art deep learning algorithms and a collection of reference models. The framework is a BSD-licensed C++ library with Python and MATLAB bindings for training and deploying generalpurpose convolutional neural networks and other deep models eﬃciently on commodity architectures. Caﬀe ﬁts industry and internet-scale media needs by CUDA GPU computation, processing over 40 million images a day on a single K40 or Titan GPU (≈ 2.5 ms per image). By separating model representation from actual implementation, Caﬀe allows experimentation and seamless switching among platforms for ease of development and deployment from prototyping machines to cloud environments.},
	language = {en},
	urldate = {2024-12-27},
	publisher = {arXiv},
	author = {Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff and Karayev, Sergey and Long, Jonathan and Girshick, Ross and Guadarrama, Sergio and Darrell, Trevor},
	month = jun,
	year = {2014},
	note = {arXiv:1408.5093 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Computer Vision and Pattern Recognition},
	file = {PDF:/home/atomwalk12/Zotero/storage/TFCGA4KC/Jia et al. - 2014 - Caffe Convolutional Architecture for Fast Feature Embedding.pdf:application/pdf},
}

@misc{paszke_automatic_nodate,
	title = {Automatic differentiation in {PyTorch}},
	abstract = {In this article, we describe an automatic differentiation module of PyTorch — a library designed to enable rapid research on machine learning models. It builds upon a few projects, most notably Lua Torch, Chainer, and HIPS Autograd [4], and provides a high performance environment with easy access to automatic differentiation of models executed on different devices (CPU and GPU). To make prototyping easier, PyTorch does not follow the symbolic approach used in many other deep learning frameworks, but focuses on differentiation of purely imperative programs, with a focus on extensibility and low overhead. Note that this preprint is a draft of certain sections from an upcoming paper covering all PyTorch features.},
	language = {en},
	author = {Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban},
	file = {PDF:/home/atomwalk12/Zotero/storage/NIPIP9DT/Paszke et al. - Automatic differentiation in PyTorch.pdf:application/pdf},
}

@misc{verbraeken_survey_2021,
	title = {A {Survey} on {Distributed} {Machine} {Learning}},
	volume = {53},
	issn = {0360-0300, 1557-7341},
	howpublished = {\url{https://dl.acm.org/doi/10.1145/3377454}},
	doi = {10.1145/3377454},
	abstract = {The demand for artificial intelligence has grown significantly over the past decade, and this growth has been fueled by advances in machine learning techniques and the ability to leverage hardware acceleration. However, to increase the quality of predictions and render machine learning solutions feasible for more complex applications, a substantial amount of training data is required. Although small machine learning models can be trained with modest amounts of data, the input for training larger models such as neural networks grows exponentially with the number of parameters. Since the demand for processing training data has outpaced the increase in computation power of computing machinery, there is a need for distributing the machine learning workload across multiple machines, and turning the centralized into a distributed system. These distributed systems present new challenges: first and foremost, the efficient parallelization of the training process and the creation of a coherent model. This article provides an extensive overview of the current state-of-the-art in the field by outlining the challenges and opportunities of distributed machine learning over conventional (centralized) machine learning, discussing the techniques used for distributed machine learning, and providing an overview of the systems that are available.},
	language = {en},
	number = {2},
	urldate = {2024-12-27},
	journal = {ACM Computing Surveys},
	author = {Verbraeken, Joost and Wolting, Matthijs and Katzy, Jonathan and Kloppenburg, Jeroen and Verbelen, Tim and Rellermeyer, Jan S.},
	month = mar,
	year = {2021},
	pages = {1--33},
	file = {PDF:/home/atomwalk12/Zotero/storage/6MFPCFG2/Verbraeken et al. - 2021 - A Survey on Distributed Machine Learning.pdf:application/pdf},
}

@misc{ben-nun_demystifying_2020,
	title = {Demystifying {Parallel} and {Distributed} {Deep} {Learning}: {An} {In}-depth {Concurrency} {Analysis}},
	volume = {52},
	issn = {0360-0300, 1557-7341},
	shorttitle = {Demystifying {Parallel} and {Distributed} {Deep} {Learning}},
	howpublished = {\url{https://dl.acm.org/doi/10.1145/3320060}},
	doi = {10.1145/3320060},
	abstract = {Deep Neural Networks (DNNs) are becoming an important tool in modern computing applications. Accelerating their training is a major challenge and techniques range from distributed algorithms to low-level circuit design. In this survey, we describe the problem from a theoretical perspective, followed by approaches for its parallelization. We present trends in DNN architectures and the resulting implications on parallelization strategies. We then review and model the different types of concurrency in DNNs: from the single operator, through parallelism in network inference and training, to distributed deep learning. We discuss asynchronous stochastic optimization, distributed system architectures, communication schemes, and neural architecture search. Based on those approaches, we extrapolate potential directions for parallelism in deep learning.},
	language = {en},
	number = {4},
	urldate = {2024-12-27},
	journal = {ACM Computing Surveys},
	author = {Ben-Nun, Tal and Hoefler, Torsten},
	month = jul,
	year = {2020},
	pages = {1--43},
	file = {PDF:/home/atomwalk12/Zotero/storage/GBBDSCCF/Ben-Nun and Hoefler - 2020 - Demystifying Parallel and Distributed Deep Learning An In-depth Concurrency Analysis.pdf:application/pdf},
}

@misc{chahal_hitchhikers_2018,
	title = {A {Hitchhiker}'s {Guide} {On} {Distributed} {Training} of {Deep} {Neural} {Networks}},
	howpublished = {\url{http://arxiv.org/abs/1810.11787}},
	doi = {10.48550/arXiv.1810.11787},
	abstract = {Deep learning has led to tremendous advancements in the ﬁeld of Artiﬁcial Intelligence. One caveat however is the substantial amount of compute needed to train these deep learning models. Training a benchmark dataset like ImageNet on a single machine with a modern GPU can take upto a week, distributing training on multiple machines has been observed to drastically bring this time down. Recent work has brought down ImageNet training time to a time as low as 4 minutes by using a cluster of 2048 GPUs. This paper surveys the various algorithms and techniques used to distribute training and presents the current state of the art for a modern distributed training framework. More speciﬁcally, we explore the synchronous and asynchronous variants of distributed Stochastic Gradient Descent, various All Reduce gradient aggregation strategies and best practices for obtaining higher throughout and lower latency over a cluster such as mixed precision training, large batch training and gradient compression.},
	language = {en},
	urldate = {2024-12-27},
	publisher = {arXiv},
	author = {Chahal, Karanbir and Grover, Manraj Singh and Dey, Kuntal},
	month = oct,
	year = {2018},
	note = {arXiv:1810.11787 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Distributed, Parallel, and Cluster Computing, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {PDF:/home/atomwalk12/Zotero/storage/7HZX2TRH/Chahal et al. - 2018 - A Hitchhiker's Guide On Distributed Training of Deep Neural Networks.pdf:application/pdf},
}

@misc{nichols_survey_2022,
	title = {A {Survey} and {Empirical} {Evaluation} of {Parallel} {Deep} {Learning} {Frameworks}},
	howpublished = {\url{http://arxiv.org/abs/2111.04949}},
	doi = {10.48550/arXiv.2111.04949},
	abstract = {The field of deep learning has witnessed a remarkable shift towards extremely compute- and memory-intensive neural networks. These newer larger models have enabled researchers to advance stateof-the-art tools across a variety of fields. This phenomenon has spurred the development of algorithms for distributed training of neural networks over a larger number of hardware accelerators. In this paper, we discuss and compare current state-of-the-art frameworks for large scale distributed deep learning. First, we survey current practices in distributed learning and identify the different types of parallelism used. Then, we present empirical results comparing their performance on large image and language training tasks. Additionally, we address their statistical efficiency and memory consumption behavior. Based on our results, we discuss algorithmic and implementation portions of each framework which hinder performance.},
	language = {en},
	urldate = {2024-12-27},
	publisher = {arXiv},
	author = {Nichols, Daniel and Singh, Siddharth and Lin, Shu-Huai and Bhatele, Abhinav},
	month = jul,
	year = {2022},
	note = {arXiv:2111.04949 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Artificial Intelligence},
	file = {PDF:/home/atomwalk12/Zotero/storage/JWXILBYW/Nichols et al. - 2022 - A Survey and Empirical Evaluation of Parallel Deep Learning Frameworks.pdf:application/pdf},
}

@incollection{li_deep_2016,
	title = {Deep {Learning} and {Its} {Parallelization}},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	isbn = {978-0-12-805394-2},
	howpublished = {\url{https://linkinghub.elsevier.com/retrieve/pii/B9780128053942000040}},
	language = {en},
	urldate = {2024-12-27},
	booktitle = {Big {Data}},
	publisher = {Elsevier},
	author = {Li, X. and Zhang, G. and Li, K. and Zheng, W.},
	year = {2016},
	doi = {10.1016/B978-0-12-805394-2.00004-0},
	pages = {95--118},
	file = {PDF:/home/atomwalk12/Zotero/storage/GX9SFI95/Li et al. - 2016 - Deep Learning and Its Parallelization.pdf:application/pdf},
}

@misc{narayanan_link_2011,
	title = {Link {Prediction} by {De}-anonymization: {How} {We} {Won} the {Kaggle} {Social} {Network} {Challenge}},
	shorttitle = {Link {Prediction} by {De}-anonymization},
	howpublished = {\url{http://arxiv.org/abs/1102.4374}},
	doi = {10.48550/arXiv.1102.4374},
	abstract = {This paper describes the winning entry to the IJCNN 2011 Social Network Challenge run by Kaggle.com. The goal of the contest was to promote research on real-world link prediction, and the dataset was a graph obtained by crawling the popular Flickr social photo sharing website, with user identities scrubbed. By de-anonymizing much of the competition test set using our own Flickr crawl, we were able to effectively game the competition. Our attack represents a new application of de-anonymization to gaming machine learning contests, suggesting changes in how future competitions should be run. We introduce a new simulated annealing-based weighted graph matching algorithm for the seeding step of de-anonymization. We also show how to combine de-anonymization with link prediction---the latter is required to achieve good performance on the portion of the test set not de-anonymized---for example by training the predictor on the de-anonymized portion of the test set, and combining probabilistic predictions from de-anonymization and link prediction.},
	urldate = {2024-12-27},
	publisher = {arXiv},
	author = {Narayanan, Arvind and Shi, Elaine and Rubinstein, Benjamin I. P.},
	month = feb,
	year = {2011},
	note = {arXiv:1102.4374 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Cryptography and Security, kaggle},
	file = {Preprint PDF:/home/atomwalk12/Zotero/storage/MSE526D8/Narayanan et al. - 2011 - Link Prediction by De-anonymization How We Won the Kaggle Social Network Challenge.pdf:application/pdf;Snapshot:/home/atomwalk12/Zotero/storage/2SI3XQ5J/1102.html:text/html},
}

@misc{barros_e_sa_deep_2024,
	title = {Deep reinforcement learning in real-time strategy games: a systematic literature review},
	volume = {55},
	issn = {1573-7497},
	shorttitle = {Deep reinforcement learning in real-time strategy games},
	howpublished = {\url{https://doi.org/10.1007/s10489-024-06220-4}},
	doi = {10.1007/s10489-024-06220-4},
	abstract = {Reinforcement learning is a field of Machine Learning in which agents learn from interacting with the environment. These agents can deal with more complex problems when their decision-making process is combined with deep learning. While deep reinforcement learning can be used in many real-world applications, games often provide a good source of simulation environments for testing such algorithms. Among all game categories, real-time strategy games usually pose a difficult challenge since they have large state and action spaces, partial observation maps, sparse reward, and Multi-Agent problems, where the events occur continuously simultaneously. Thus, this paper provides a systematic literature review of deep reinforcement learning related to real-time strategy games. The main goals of this review are presented as follows: (a) identify the games used in recent works; (b) summarize the architectures and techniques used; (c) identify the simulation environments adopted and (d) understand whether the works focus on micromanagement or macromanagement tasks when dealing with real-time strategy games. The results show that some architectures have achieved better performance overall when handling both micro and macromanagement tasks, and that techniques for reducing the training time and the state space may improve the agents learning. This paper may help to guide future research on developing strategies to build agents for complex scenarios such as those faced in real-time strategy games.},
	language = {en},
	number = {3},
	urldate = {2024-12-30},
	journal = {Applied Intelligence},
	author = {Barros e Sá, Gabriel Caldas and Madeira, Charles Andrye Galvão},
	month = dec,
	year = {2024},
	keywords = {Agent architectures, Artificial Intelligence, Deep reinforcement learning, Real-time strategy games, Systematic literature review},
	pages = {243},
	file = {Full Text PDF:/home/atomwalk12/Zotero/storage/PD2XJWPE/Barros e Sá and Madeira - 2024 - Deep reinforcement learning in real-time strategy games a systematic literature review.pdf:application/pdf},
}

@misc{dos_santos_sustainable_2024,
	title = {Sustainable systematic literature reviews},
	volume = {176},
	issn = {0950-5849},
	howpublished = {\url{https://www.sciencedirect.com/science/article/pii/S0950584924001563}},
	doi = {10.1016/j.infsof.2024.107551},
	abstract = {Context:
Systematic Literature Reviews (SLR) have been recognized as an important research method for summarizing evidence in Software Engineering (SE). At the same, SLR still presents several problems, such as the high resource consumption (mainly human resources) and lack of effective impact on SE practitioners, although much research has already been done.
Objective:
The main goal of this paper is to explore the concept of sustainability in the SLR area, intending to contribute to understanding better and solving such problems in an integrated way. More specifically, this paper characterizes what sustainable SLR are, their core characteristics, critical factors (i.e., sensitive points in the SLR process), and guidelines for conducting such SLR.
Methods:
We performed a meta-ethnographic study to find key concepts of sustainable software systems and transpose them to sustainable SLR. For this, we systematically selected 16 studies about sustainable software systems and 14 distinguished studies about SLR. Following, we extracted the main keywords and metaphors, determined how both areas are correlated, and transposed them to obtain a set of core characteristics of sustainable SLR as well as critical factors and guidelines. Additionally, we validated them with specialists using the Delphi method.
Results:
We found 15 core characteristics that offer a broad view of sustainable SLR, 15 critical factors in the SLR process that should be carefully addressed when conducting and updating SLR, and also 16 guidelines to manage SLR from the sustainability perspective.
Conclusion:
The concept of sustainability in SLR can contribute to solving SLR problems in a more integrated way, while this work could change the mindset of the SLR community about the need to conduct sustainable SLR.},
	urldate = {2024-12-31},
	journal = {Information and Software Technology},
	author = {dos Santos, Vinicius and Iwazaki, Anderson Y. and Felizardo, Katia R. and de Souza, Érica F. and Nakagawa, Elisa Y.},
	month = dec,
	year = {2024},
	keywords = {Systematic literature review, Secondary study, SLR, Sustainability},
	pages = {107551},
	file = {ScienceDirect Snapshot:/home/atomwalk12/Zotero/storage/MCEA5L9Z/S0950584924001563.html:text/html},
}

@misc{bolanos_artificial_2024,
	title = {Artificial intelligence for literature reviews: opportunities and challenges},
	volume = {57},
	issn = {1573-7462},
	shorttitle = {Artificial intelligence for literature reviews},
	howpublished = {\url{https://doi.org/10.1007/s10462-024-10902-3}},
	doi = {10.1007/s10462-024-10902-3},
	abstract = {This paper presents a comprehensive review of the use of Artificial Intelligence (AI) in Systematic Literature Reviews (SLRs). A SLR is a rigorous and organised methodology that assesses and integrates prior research on a given topic. Numerous tools have been developed to assist and partially automate the SLR process. The increasing role of AI in this field shows great potential in providing more effective support for researchers, moving towards the semi-automatic creation of literature reviews. Our study focuses on how AI techniques are applied in the semi-automation of SLRs, specifically in the screening and extraction phases. We examine 21 leading SLR tools using a framework that combines 23 traditional features with 11 AI features. We also analyse 11 recent tools that leverage large language models for searching the literature and assisting academic writing. Finally, the paper discusses current trends in the field, outlines key research challenges, and suggests directions for future research. We highlight three primary research challenges: integrating advanced AI solutions, such as large language models and knowledge graphs, improving usability, and developing a standardised evaluation framework. We also propose best practices to ensure more robust evaluations in terms of performance, usability, and transparency. Overall, this review offers a detailed overview of AI-enhanced SLR tools for researchers and practitioners, providing a foundation for the development of next-generation AI solutions in this field.},
	language = {en},
	number = {10},
	urldate = {2025-01-05},
	journal = {Artificial Intelligence Review},
	author = {Bolaños, Francisco and Salatino, Angelo and Osborne, Francesco and Motta, Enrico},
	month = aug,
	year = {2024},
	keywords = {Artificial Intelligence, Artificial intelligence, Evaluation framework, Large language models, Literature review, Natural anguage processing, Systematic literature reviews, Usability},
	pages = {259},
	file = {Full Text PDF:/home/atomwalk12/Zotero/storage/9XU7TFA7/Bolaños et al. - 2024 - Artificial intelligence for literature reviews opportunities and challenges.pdf:application/pdf},
}

@misc{bolanos_artificial_2024-1,
	title = {Artificial intelligence for literature reviews: opportunities and challenges},
	volume = {57},
	issn = {1573-7462},
	shorttitle = {Artificial intelligence for literature reviews},
	howpublished = {\url{https://doi.org/10.1007/s10462-024-10902-3}},
	doi = {10.1007/s10462-024-10902-3},
	abstract = {This paper presents a comprehensive review of the use of Artificial Intelligence (AI) in Systematic Literature Reviews (SLRs). A SLR is a rigorous and organised methodology that assesses and integrates prior research on a given topic. Numerous tools have been developed to assist and partially automate the SLR process. The increasing role of AI in this field shows great potential in providing more effective support for researchers, moving towards the semi-automatic creation of literature reviews. Our study focuses on how AI techniques are applied in the semi-automation of SLRs, specifically in the screening and extraction phases. We examine 21 leading SLR tools using a framework that combines 23 traditional features with 11 AI features. We also analyse 11 recent tools that leverage large language models for searching the literature and assisting academic writing. Finally, the paper discusses current trends in the field, outlines key research challenges, and suggests directions for future research. We highlight three primary research challenges: integrating advanced AI solutions, such as large language models and knowledge graphs, improving usability, and developing a standardised evaluation framework. We also propose best practices to ensure more robust evaluations in terms of performance, usability, and transparency. Overall, this review offers a detailed overview of AI-enhanced SLR tools for researchers and practitioners, providing a foundation for the development of next-generation AI solutions in this field.},
	language = {en},
	number = {10},
	urldate = {2025-01-05},
	journal = {Artificial Intelligence Review},
	author = {Bolaños, Francisco and Salatino, Angelo and Osborne, Francesco and Motta, Enrico},
	month = aug,
	year = {2024},
	keywords = {Artificial Intelligence, Artificial intelligence, Evaluation framework, Large language models, Literature review, Natural anguage processing, Systematic literature reviews, Usability},
	pages = {259},
	file = {Full Text PDF:/home/atomwalk12/Zotero/storage/HN75PDD9/Bolaños et al. - 2024 - Artificial intelligence for literature reviews opportunities and challenges.pdf:application/pdf},
}

@misc{kitchenham_procedures_nodate,
	title = {Procedures for {Performing} {Systematic} {Regviews}},
	language = {en},
	journal = {NICTA},
	year = {2004},
	author = {Kitchenham, Barbara},
	keywords = {literature-review},
	file = {PDF:/home/atomwalk12/Zotero/storage/8SUS6QW9/Kitchenham - Procedures for Performing Systematic Reviews.pdf:application/pdf},
}

@misc{budgen_reporting_2018,
	title = {Reporting systematic reviews: {Some} lessons from a tertiary study},
	volume = {95},
	issn = {0950-5849},
	shorttitle = {Reporting systematic reviews},
	howpublished = {\url{https://www.sciencedirect.com/science/article/pii/S0950584916303548}},
	doi = {10.1016/j.infsof.2017.10.017},
	abstract = {Context
Many of the systematic reviews published in software engineering are related to research or methodological issues and hence are unlikely to be of direct benefit to practitioners or teachers. Those that are relevant to practice and teaching need to be presented in a form that makes their findings usable with minimum interpretation.
Objective
We have examined a sample of the many systematic reviews that have been published over a period of six years, in order to assess how well these are reported and identify useful lessons about how this might be done.
Method
We undertook a tertiary study, performing a systematic review of systematic reviews. Our study found 178 systematic reviews published in a set of major software engineering journals over the period 2010–2015. Of these, 37 provided recommendations or conclusions of relevance to education and/or practice and we used the DARE criteria as well as other attributes related to the systematic review process to analyse how well they were reported.
Results
We have derived a set of 12 ‘lessons’ that could help authors with reporting the outcomes of a systematic review in software engineering. We also provide an associated checklist for use by journal and conference referees.
Conclusion
There are several areas where better reporting is needed, including quality assessment, synthesis, and the procedures followed by the reviewers. Researchers, practitioners, teachers and journal referees would all benefit from better reporting of systematic reviews, both for clarity and also for establishing the provenance of any findings.},
	urldate = {2025-01-05},
	journal = {Information and Software Technology},
	author = {Budgen, David and Brereton, Pearl and Drummond, Sarah and Williams, Nikki},
	month = mar,
	year = {2018},
	keywords = {Provenance of findings, Reporting quality, Systematic review},
	pages = {62--74},
	file = {Accepted Version:/home/atomwalk12/Zotero/storage/SET4XTE6/Budgen et al. - 2018 - Reporting systematic reviews Some lessons from a tertiary study.pdf:application/pdf;ScienceDirect Snapshot:/home/atomwalk12/Zotero/storage/LAL2AWVY/S0950584916303548.html:text/html},
}

@misc{brereton_lessons_2007,
	series = {Software {Performance}},
	title = {Lessons from applying the systematic literature review process within the software engineering domain},
	volume = {80},
	issn = {0164-1212},
	howpublished = {\url{https://www.sciencedirect.com/science/article/pii/S016412120600197X}},
	doi = {10.1016/j.jss.2006.07.009},
	abstract = {A consequence of the growing number of empirical studies in software engineering is the need to adopt systematic approaches to assessing and aggregating research outcomes in order to provide a balanced and objective summary of research evidence for a particular topic. The paper reports experiences with applying one such approach, the practice of systematic literature review, to the published studies relevant to topics within the software engineering domain. The systematic literature review process is summarised, a number of reviews being undertaken by the authors and others are described and some lessons about the applicability of this practice to software engineering are extracted. The basic systematic literature review process seems appropriate to software engineering and the preparation and validation of a review protocol in advance of a review activity is especially valuable. The paper highlights areas where some adaptation of the process to accommodate the domain-specific characteristics of software engineering is needed as well as areas where improvements to current software engineering infrastructure and practices would enhance its applicability. In particular, infrastructure support provided by software engineering indexing databases is inadequate. Also, the quality of abstracts is poor; it is usually not possible to judge the relevance of a study from a review of the abstract alone.},
	number = {4},
	urldate = {2025-01-05},
	journal = {Journal of Systems and Software},
	author = {Brereton, Pearl and Kitchenham, Barbara A. and Budgen, David and Turner, Mark and Khalil, Mohamed},
	month = apr,
	year = {2007},
	keywords = {Systematic literature review, Empirical software engineering},
	pages = {571--583},
	file = {ScienceDirect Snapshot:/home/atomwalk12/Zotero/storage/V2L67PHV/S016412120600197X.html:text/html},
}

@misc{brereton_lessons_2007-1,
	title = {Lessons from applying the systematic literature review process within the software engineering domain},
	volume = {80},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {01641212},
	howpublished = {\url{https://linkinghub.elsevier.com/retrieve/pii/S016412120600197X}},
	doi = {10.1016/j.jss.2006.07.009},
	abstract = {A consequence of the growing number of empirical studies in software engineering is the need to adopt systematic approaches to assessing and aggregating research outcomes in order to provide a balanced and objective summary of research evidence for a particular topic. The paper reports experiences with applying one such approach, the practice of systematic literature review, to the published studies relevant to topics within the software engineering domain. The systematic literature review process is summarised, a number of reviews being undertaken by the authors and others are described and some lessons about the applicability of this practice to software engineering are extracted.},
	language = {en},
	number = {4},
	urldate = {2025-01-05},
	journal = {Journal of Systems and Software},
	author = {Brereton, Pearl and Kitchenham, Barbara A. and Budgen, David and Turner, Mark and Khalil, Mohamed},
	month = apr,
	year = {2007},
	pages = {571--583},
	file = {PDF:/home/atomwalk12/Zotero/storage/9U8H4DYF/Brereton et al. - 2007 - Lessons from applying the systematic literature review process within the software engineering domai.pdf:application/pdf},
}

@misc{Howard2016SWIFTReviewAT,
  title={SWIFT-Review: a text-mining workbench for systematic review},
  author={Brian E. Howard and Jason R. Phillips and Kyle Miller and Arpit Tandon and Deepak Mav and Mihir R. Shah and Stephanie D Holmgren and Katherine E Pelch and Vickie R. Walker and Andrew A. Rooney and Malcolm Robert Macleod and Ruchir R. Shah and Kristina Thayer},
  journal={Systematic Reviews},
  year={2016},
  volume={5},
  howpublished={\url{https://api.semanticscholar.org/CorpusID:5970071}}
}


@misc{noauthor_gemini_nodate,
	title = { Chat to supercharge your ideas},
	howpublished = {\url{https://gemini.google.com}},
	author = {Google},
	year = {2023},
	abstract = {Bard is now Gemini. Get help with writing, planning, learning and more from Google AI.},
	language = {en-GB},
	urldate = {2025-01-23},
	journal = {Gemini},
	file = {Snapshot:/home/atomwalk12/Zotero/storage/TMJ32DPN/app.html:text/html},
}

@misc{notebooklm_google_2024,
	title = {Google {NotebookLM} {\textbar} {Note} {Taking} \& {Research} {Assistant} {Powered} by {AI}},
	howpublished = {\url{https://notebooklm.google/}},
	abstract = {Use the power of AI for quick summarization and note taking, NotebookLM is your powerful virtual research assistant rooted in information you can trust.},
	language = {en-GB},
	urldate = {2025-01-23},
	author = {Google},
	year = {2023},
	file = {Snapshot:/home/atomwalk12/Zotero/storage/II36H7PV/notebooklm.google.html:text/html},
}


@misc{team_gemini_2024,
	title = {Gemini: {A} {Family} of {Highly} {Capable} {Multimodal} {Models}},
	shorttitle = {Gemini},
	howpublished = {\url{http://arxiv.org/abs/2312.11805}},
	doi = {10.48550/arXiv.2312.11805},
	abstract = {This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of the Gemini family in cross-modal reasoning and language understanding will enable a wide variety of use cases. We discuss our approach toward post-training and deploying Gemini models responsibly to users through services including Gemini, Gemini Advanced, Google AI Studio, and Cloud Vertex AI.},
	urldate = {2025-01-23},
	publisher = {arXiv},
	author = {{Gemini Authors}},
	month = jun,
	year = {2024},
	note = {arXiv:2312.11805 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:/home/atomwalk12/Zotero/storage/G8FL5XFH/Team et al. - 2024 - Gemini A Family of Highly Capable Multimodal Models.pdf:application/pdf;Snapshot:/home/atomwalk12/Zotero/storage/4RHAQN3Z/2312.html:text/html},
}

@misc{dos_santos_sustainable_2024-1,
	title = {Sustainable systematic literature reviews},
	volume = {176},
	issn = {09505849},
	howpublished = {\url{https://linkinghub.elsevier.com/retrieve/pii/S0950584924001563}},
	doi = {10.1016/j.infsof.2024.107551},
	abstract = {Objective: The main goal of this paper is to explore the concept of sustainability in the SLR area, intending to contribute to understanding better and solving such problems in an integrated way. More specifically, this paper characterizes what sustainable SLR are, their core characteristics, critical factors (i.e., sensitive points in the SLR process), and guidelines for conducting such SLR.
Methods: We performed a meta-ethnographic study to find key concepts of sustainable software systems and transpose them to sustainable SLR. For this, we systematically selected 16 studies about sustainable software systems and 14 distinguished studies about SLR. Following, we extracted the main keywords and metaphors, determined how both areas are correlated, and transposed them to obtain a set of core characteristics of sustainable SLR as well as critical factors and guidelines. Additionally, we validated them with specialists using the Delphi method.
Results: We found 15 core characteristics that offer a broad view of sustainable SLR, 15 critical factors in the SLR process that should be carefully addressed when conducting and updating SLR, and also 16 guidelines to manage SLR from the sustainability perspective.
Conclusion: The concept of sustainability in SLR can contribute to solving SLR problems in a more integrated way, while this work could change the mindset of the SLR community about the need to conduct sustainable SLR.},
	language = {en},
	urldate = {2025-01-05},
	journal = {Information and Software Technology},
	author = {Dos Santos, Vinicius and Iwazaki, Anderson Y. and Felizardo, Katia R. and De Souza, Érica F. and Nakagawa, Elisa Y.},
	month = dec,
	year = {2024},
	pages = {107551},
	file = {PDF:/home/atomwalk12/Zotero/storage/H3SLL832/Dos Santos et al. - 2024 - Sustainable systematic literature reviews.pdf:application/pdf},
}

@misc{dehghani_distributed_2023,
	title = {From distributed machine to distributed deep learning: a comprehensive survey},
	volume = {10},
	issn = {2196-1115},
	shorttitle = {From distributed machine to distributed deep learning},
	howpublished = {\url{https://doi.org/10.1186/s40537-023-00829-x}},
	doi = {10.1186/s40537-023-00829-x},
	abstract = {Artificial intelligence has made remarkable progress in handling complex tasks, thanks to advances in hardware acceleration and machine learning algorithms. However, to acquire more accurate outcomes and solve more complex issues, algorithms should be trained with more data. Processing this huge amount of data could be time-consuming and require a great deal of computation. To address these issues, distributed machine learning has been proposed, which involves distributing the data and algorithm across several machines. There has been considerable effort put into developing distributed machine learning algorithms, and different methods have been proposed so far. We divide these algorithms in classification and clustering (traditional machine learning), deep learning and deep reinforcement learning groups. Distributed deep learning has gained more attention in recent years and most of the studies have focused on this approach. Therefore, we mostly concentrate on this category. Based on the investigation of the mentioned algorithms, we highlighted the limitations that should be addressed in future research.},
	language = {en},
	number = {1},
	urldate = {2025-01-05},
	journal = {Journal of Big Data},
	author = {Dehghani, Mohammad and Yazdanparast, Zahra},
	month = oct,
	year = {2023},
	keywords = {Artificial Intelligence, Artificial intelligence, Data-parallelism, Distributed deep learning, Distributed machine learning, Ditributed reinforcement learning, Machine learning, Model-parallelism},
	pages = {158},
	file = {Full Text PDF:/home/atomwalk12/Zotero/storage/FQPFJL85/Dehghani and Yazdanparast - 2023 - From distributed machine to distributed deep learning a comprehensive survey.pdf:application/pdf},
}

@misc{berloco_systematic_2022,
	address = {Cham},
	title = {A {Systematic} {Review} of {Distributed} {Deep} {Learning} {Frameworks} for {Big} {Data}},
	isbn = {978-3-031-13832-4},
	doi = {10.1007/978-3-031-13832-4_21},
	abstract = {Traditional Machine Learning and Deep Learning techniques (data acquisition, preparation, model training and evaluation) take a lot of computational resources and time to produce even a simple prediction model, especially when implemented on a single machine. Intuitively, the demand for computational requirements is higher in case of management of Big Data and training of complex models. Thus, a paradigm shift from a single machine to a BD-oriented approach is required for making traditional Machine Learning and Deep Learning techniques fit to Big Data. In particular, it emerges the need for developing and deploying Big Data Analytics Infrastructures on cluster of machines. In this context, main features and principles of Distributed Deep Learning frameworks are here discussed. The main contribution of this paper is a systematic review of proposed solutions, aimed at investigating under a unifying lens their foundational elements, functional features and capabilities, despite the inherent literature fragmentation. To this, we conducted a literature search in Scopus and Google Scholar. This review also compares Distributed Deep Learning approaches according to more technical facets: implemented of parallelism techniques, supported hardware, model parameters sharing modalities, computation modalities for stochastic gradient descent and compatibility with other frameworks.},
	language = {en},
	booktitle = {Intelligent {Computing} {Methodologies}},
	publisher = {Springer International Publishing},
	author = {Berloco, Francesco and Bevilacqua, Vitoantonio and Colucci, Simona},
	editor = {Huang, De-Shuang and Jo, Kang-Hyun and Jing, Junfeng and Premaratne, Prashan and Bevilacqua, Vitoantonio and Hussain, Abir},
	year = {2022},
	keywords = {Big Data, Distributed Deep Learning, Distributed Deep Learning Frameworks, Parallel computing},
	pages = {242--256},
	file = {Full Text PDF:/home/atomwalk12/Zotero/storage/PJJTIG7E/Berloco et al. - 2022 - A Systematic Review of Distributed Deep Learning Frameworks for Big Data.pdf:application/pdf},
}

@misc{langer_distributed_2020,
	title = {Distributed {Training} of {Deep} {Learning} {Models}: {A} {Taxonomic} {Perspective}},
	volume = {31},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {1045-9219, 1558-2183, 2161-9883},
	shorttitle = {Distributed {Training} of {Deep} {Learning} {Models}},
	howpublished = {\url{https://ieeexplore.ieee.org/document/9120226/}},
	doi = {10.1109/TPDS.2020.3003307},
	abstract = {Distributed deep learning systems (DDLS) train deep neural network models by utilizing the distributed resources of a cluster. Developers of DDLS are required to make many decisions to process their particular workloads in their chosen environment efficiently. The advent of GPU-based deep learning, the ever-increasing size of datasets, and deep neural network models, in combination with the bandwidth constraints that exist in cluster environments require developers of DDLS to be innovative in order to train high-quality models quickly. Comparing DDLS side-by-side is difficult due to their extensive feature lists and architectural deviations. We aim to shine some light on the fundamental principles that are at work when training deep neural networks in a cluster of independent machines by analyzing the general properties associated with training deep learning models and how such workloads can be distributed in a cluster to achieve collaborative model training. Thereby we provide an overview of the different techniques that are used by contemporary DDLS and discuss their influence and implications on the training process. To conceptualize and compare DDLS, we group different techniques into categories, thus establishing a taxonomy of distributed deep learning systems.},
	number = {12},
	urldate = {2025-01-05},
	journal = {IEEE Transactions on Parallel and Distributed Systems},
	author = {Langer, Matthias and He, Zhen and Rahayu, Wenny and Xue, Yanbo},
	month = dec,
	year = {2020},
	pages = {2802--2818},
	file = {Submitted Version:/home/atomwalk12/Zotero/storage/4649MWFD/Langer et al. - 2020 - Distributed Training of Deep Learning Models A Taxonomic Perspective.pdf:application/pdf},
}

@misc{nichols_survey_2022-1,
	title = {A {Survey} and {Empirical} {Evaluation} of {Parallel} {Deep} {Learning} {Frameworks}},
	howpublished = {\url{http://arxiv.org/abs/2111.04949}},
	doi = {10.48550/arXiv.2111.04949},
	abstract = {The field of deep learning has witnessed a remarkable shift towards extremely compute- and memory-intensive neural networks. These newer larger models have enabled researchers to advance state-of-the-art tools across a variety of fields. This phenomenon has spurred the development of algorithms for distributed training of neural networks over a larger number of hardware accelerators. In this paper, we discuss and compare current state-of-the-art frameworks for large scale distributed deep learning. First, we survey current practices in distributed learning and identify the different types of parallelism used. Then, we present empirical results comparing their performance on large image and language training tasks. Additionally, we address their statistical efficiency and memory consumption behavior. Based on our results, we discuss algorithmic and implementation portions of each framework which hinder performance.},
	urldate = {2025-01-05},
	publisher = {arXiv},
	author = {Nichols, Daniel and Singh, Siddharth and Lin, Shu-Huai and Bhatele, Abhinav},
	month = jul,
	year = {2022},
	note = {arXiv:2111.04949 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:/home/atomwalk12/Zotero/storage/U3PUQJSE/Nichols et al. - 2022 - A Survey and Empirical Evaluation of Parallel Deep Learning Frameworks.pdf:application/pdf;Snapshot:/home/atomwalk12/Zotero/storage/XYGZ2AQZ/2111.html:text/html},
}

@misc{xing_strategies_2015,
	title = {Strategies and {Principles} of {Distributed} {Machine} {Learning} on {Big} {Data}},
	howpublished = {\url{http://arxiv.org/abs/1512.09295}},
	doi = {10.48550/arXiv.1512.09295},
	abstract = {The rise of Big Data has led to new demands for Machine Learning (ML) systems to learn complex models with millions to billions of parameters, that promise adequate capacity to digest massive datasets and offer powerful predictive analytics thereupon. In order to run ML algorithms at such scales, on a distributed cluster with 10s to 1000s of machines, it is often the case that significant engineering efforts are required --- and one might fairly ask if such engineering truly falls within the domain of ML research or not. Taking the view that Big ML systems can benefit greatly from ML-rooted statistical and algorithmic insights --- and that ML researchers should therefore not shy away from such systems design --- we discuss a series of principles and strategies distilled from our recent efforts on industrial-scale ML solutions. These principles and strategies span a continuum from application, to engineering, and to theoretical research and development of Big ML systems and architectures, with the goal of understanding how to make them efficient, generally-applicable, and supported with convergence and scaling guarantees. They concern four key questions which traditionally receive little attention in ML research: How to distribute an ML program over a cluster? How to bridge ML computation with inter-machine communication? How to perform such communication? What should be communicated between machines? By exposing underlying statistical and algorithmic characteristics unique to ML programs but not typically seen in traditional computer programs, and by dissecting successful cases to reveal how we have harnessed these principles to design and develop both high-performance distributed ML software as well as general-purpose ML frameworks, we present opportunities for ML researchers and practitioners to further shape and grow the area that lies between ML and systems.},
	urldate = {2025-01-05},
	publisher = {arXiv},
	author = {Xing, Eric P. and Ho, Qirong and Xie, Pengtao and Dai, Wei},
	month = dec,
	year = {2015},
	note = {arXiv:1512.09295 [stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Distributed, Parallel, and Cluster Computing, Statistics - Machine Learning},
	file = {Preprint PDF:/home/atomwalk12/Zotero/storage/RHYCBQMX/Xing et al. - 2015 - Strategies and Principles of Distributed Machine Learning on Big Data.pdf:application/pdf;Snapshot:/home/atomwalk12/Zotero/storage/GWFZVF5U/1512.html:text/html},
}

@misc{vasile_survey_nodate,
	title = {Survey on {Distributed} {Techniques} applied to {Neural} {Networks}},
	language = {en},
	author = {Vasile, Razvan Florian},
	file = {PDF:/home/atomwalk12/Zotero/storage/HE8WRHGK/Vasile - Survey on Distributed Techniques applied to Neural Networks.pdf:application/pdf},
}

}

@misc{brereton_lessons_2007,
	series = {Software {Performance}},
	title = {Lessons from applying the systematic literature review process within the software engineering domain},
	volume = {80},
	issn = {0164-1212},
	howpublished = {\url{https://www.sciencedirect.com/science/article/pii/S016412120600197X}},
	doi = {10.1016/j.jss.2006.07.009},
	abstract = {A consequence of the growing number of empirical studies in software engineering is the need to adopt systematic approaches to assessing and aggregating research outcomes in order to provide a balanced and objective summary of research evidence for a particular topic. The paper reports experiences with applying one such approach, the practice of systematic literature review, to the published studies relevant to topics within the software engineering domain. The systematic literature review process is summarised, a number of reviews being undertaken by the authors and others are described and some lessons about the applicability of this practice to software engineering are extracted. The basic systematic literature review process seems appropriate to software engineering and the preparation and validation of a review protocol in advance of a review activity is especially valuable. The paper highlights areas where some adaptation of the process to accommodate the domain-specific characteristics of software engineering is needed as well as areas where improvements to current software engineering infrastructure and practices would enhance its applicability. In particular, infrastructure support provided by software engineering indexing databases is inadequate. Also, the quality of abstracts is poor; it is usually not possible to judge the relevance of a study from a review of the abstract alone.},
	number = {4},
	urldate = {2025-01-05},
	journal = {Journal of Systems and Software},
	author = {Brereton, Pearl and Kitchenham, Barbara A. and Budgen, David and Turner, Mark and Khalil, Mohamed},
	month = apr,
	year = {2007},
	keywords = {Systematic literature review, Empirical software engineering},
	pages = {571--583},
	file = {ScienceDirect Snapshot:/home/atomwalk12/Zotero/storage/V2L67PHV/S016412120600197X.html:text/html},
}

% Two papers from the project proposal below
@misc{SierraCanto2010ParallelTO,
  title={Parallel Training of a Back-Propagation Neural Network Using CUDA},
  author={Xavier Sierra-Canto and Francisco Madera-Ramirez and V{\'i}ctor Uc Cetina},
  journal={2010 Ninth International Conference on Machine Learning and Applications},
  year={2010},
  pages={307-312},
  howpublished={\url{https://api.semanticscholar.org/CorpusID:53717101}}
}

@misc{li_pytorch_2020,
      title={PyTorch Distributed: Experiences on Accelerating Data Parallel Training}, 
      author={Shen Li and Yanli Zhao and Rohan Varma and Omkar Salpekar and Pieter Noordhuis and Teng Li and Adam Paszke and Jeff Smith and Brian Vaughan and Pritam Damania and Soumith Chintala},
      year={2020},
      eprint={2006.15704},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      howpublished={\url{https://arxiv.org/abs/2006.15704}}
}

@misc{keele_systematic_2007,
	author = {Keele University},
	title = {Guidelines for performing Systematic Literature Reviews in Software Engineering},
	howpublished = {\url{https://legacyfileshare.elsevier.com/promis_misc/525444systematicreviewsguide.pdf}},
	year = {2007},
	note = {[Accessed 13-01-2025]},
}


@misc{krizhevsky_imagenet_2012,
	title = {{ImageNet} {Classification} with {Deep} {Convolutional} {Neural} {Networks}},
	volume = {25},
	howpublished = {\url{https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html}},
	abstract = {We trained a large, deep convolutional neural network to classify the 1.3 million high-resolution images in the LSVRC-2010 ImageNet training set into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 39.7{\textbackslash}\% and 18.9{\textbackslash}\% which is considerably better than the previous state-of-the-art results. The neural network, which has 60 million parameters and 500,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and two globally connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of convolutional nets. To reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective.},
	urldate = {2025-01-16},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	year = {2012},
	file = {Full Text PDF:/home/atomwalk12/Zotero/storage/M6RC4IZF/Krizhevsky et al. - 2012 - ImageNet Classification with Deep Convolutional Neural Networks.pdf:application/pdf},
}


@misc{kaplan_scaling_2020,
	title = {Scaling {Laws} for {Neural} {Language} {Models}},
	howpublished = {\url{http://arxiv.org/abs/2001.08361}},
	doi = {10.48550/arXiv.2001.08361},
	abstract = {We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.},
	urldate = {2025-01-17},
	publisher = {arXiv},
	author = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
	month = jan,
	year = {2020},
	note = {arXiv:2001.08361 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:/home/atomwalk12/Zotero/storage/QS7HU2T9/Kaplan et al. - 2020 - Scaling Laws for Neural Language Models.pdf:application/pdf;Snapshot:/home/atomwalk12/Zotero/storage/PF85BBSG/2001.html:text/html},
}


@book{kitchenham_evidence-based_2015,
	title = {Evidence-{Based} {Software} {Engineering} and {Systematic} {Reviews}},
	isbn = {978-1-4822-2866-3},
	abstract = {In the decade since the idea of adapting the evidence-based paradigm for software engineering was first proposed, it has become a major tool of empirical software engineering. Evidence-Based Software Engineering and Systematic Reviews provides a clear introduction to the use of an evidence-based model for software engineering research and practice.},
	language = {en},
	publisher = {CRC Press},
	author = {Kitchenham, Barbara Ann and Budgen, David and Brereton, Pearl},
	month = nov,
	year = {2015},
	note = {Google-Books-ID: bGfdCgAAQBAJ},
	keywords = {Computers / Computer Science, Computers / General, Computers / Programming / Games, Computers / Software Development \& Engineering / General, Mathematics / General},
}


@misc{huang_gpipe_2019,
	title = {{GPipe}: {Efficient} {Training} of {Giant} {Neural} {Networks} using {Pipeline} {Parallelism}},
	shorttitle = {{GPipe}},
	howpublished = {\url{http://arxiv.org/abs/1811.06965}},
	doi = {10.48550/arXiv.1811.06965},
	abstract = {Scaling up deep neural network capacity has been known as an effective approach to improving model quality for several different machine learning tasks. In many cases, increasing model capacity beyond the memory limit of a single accelerator has required developing special algorithms or infrastructure. These solutions are often architecture-specific and do not transfer to other tasks. To address the need for efficient and task-independent model parallelism, we introduce GPipe, a pipeline parallelism library that allows scaling any network that can be expressed as a sequence of layers. By pipelining different sub-sequences of layers on separate accelerators, GPipe provides the flexibility of scaling a variety of different networks to gigantic sizes efficiently. Moreover, GPipe utilizes a novel batch-splitting pipelining algorithm, resulting in almost linear speedup when a model is partitioned across multiple accelerators. We demonstrate the advantages of GPipe by training large-scale neural networks on two different tasks with distinct network architectures: (i) Image Classification: We train a 557-million-parameter AmoebaNet model and attain a top-1 accuracy of 84.4\% on ImageNet-2012, (ii) Multilingual Neural Machine Translation: We train a single 6-billion-parameter, 128-layer Transformer model on a corpus spanning over 100 languages and achieve better quality than all bilingual models.},
	urldate = {2025-01-21},
	publisher = {arXiv},
	author = {Huang, Yanping and Cheng, Youlong and Bapna, Ankur and Firat, Orhan and Chen, Mia Xu and Chen, Dehao and Lee, HyoukJoong and Ngiam, Jiquan and Le, Quoc V. and Wu, Yonghui and Chen, Zhifeng},
	month = jul,
	year = {2019},
	note = {arXiv:1811.06965 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 11 pages. Work in progress. Copyright 2018 by the authors},
	annote = {Reference Semantic Scholar: [TLDR] GPipe is introduced, a pipeline parallelism library that allows scaling any network that can be expressed as a sequence of layers by pipelining different sub-sequences of layers on separate accelerators, resulting in almost linear speedup when a model is partitioned across multiple accelerators.
},
	file = {Preprint PDF:/home/atomwalk12/Zotero/storage/VMSKDDS4/Huang et al. - 2019 - GPipe Efficient Training of Giant Neural Networks using Pipeline Parallelism.pdf:application/pdf;Snapshot:/home/atomwalk12/Zotero/storage/BABZQA3T/1811.html:text/html},
}

@misc{noauthor_tensorflowlingvo_2025,
	title = {tensorflow/lingvo},
	copyright = {Apache-2.0},
	howpublished = {\url{https://github.com/tensorflow/lingvo}},
	abstract = {Lingvo},
	urldate = {2025-01-21},
	publisher = {tensorflow},
	month = jan,
	year = {2025},
	note = {original-date: 2018-07-24T22:30:28Z},
	keywords = {asr, distributed, distributed-systems-ddl, distributed-systems-ddl-selected, distributed-systems-ddl-selected-selected, gpu-computing, language-model, lm, machine-translation, mnist, nlp, research, seq2seq, speech, speech-recognition, speech-synthesis, speech-to-text, tensorflow, translation, tts},
	annote = {GPipe},
	key = {GPipe},
}

@misc{noauthor_hugging_2025,
	title = {Hugging {Face} re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2025-02-06},
	month = feb,
	howpublished = {\url{https://huggingface.co/}},
	author = {Hugging Face},
	year = {2025},
	file = {Snapshot:/home/atomwalk12/Zotero/storage/5Y2HBF2Y/huggingface.co.html:text/html},
}

@misc{noauthor_alpa-projectsalpa_nodate,
	title = {alpa-projects/alpa: {Training} and serving large-scale neural networks with auto parallelization.},
	howpublished = {\url{https://github.com/alpa-projects/alpa?tab=readme-ov-file}},
	urldate = {2025-02-06},
	publisher = {alpa-projects},
	author = {alpa-projects},
	year = {2021},
	file = {alpa-projects/alpa\: Training and serving large-scale neural networks with auto parallelization.:/home/atomwalk12/Zotero/storage/PT2RMGTY/alpa.html:text/html},
}
@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,
      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, 
      author={DeepSeek-AI and Daya Guo and Dejian Yang and Haowei Zhang and Junxiao Song and Ruoyu Zhang and Runxin Xu and Qihao Zhu and Shirong Ma and Peiyi Wang and Xiao Bi and Xiaokang Zhang and Xingkai Yu and Yu Wu and Z. F. Wu and Zhibin Gou and Zhihong Shao and Zhuoshu Li and Ziyi Gao and Aixin Liu and Bing Xue},
      year={2025},
      eprint={2501.12948},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
	  howpublished = {\url{https://arxiv.org/abs/2501.12948}}
}
@misc{hestness_deep_2017,
	title = {Deep {Learning} {Scaling} is {Predictable}, {Empirically}},
	howpublished = {\url{https://www.semanticscholar.org/paper/Deep-Learning-Scaling-is-Predictable\%2C-Empirically-Hestness-Narang/a1c922be467d1c0c64b963e65dae41778b81b2a0}},
	abstract = {Deep learning (DL) creates impactful advances following a virtuous recipe: model architecture search, creating large training data sets, and scaling computation. It is widely believed that growing training sets and models should improve accuracy and result in better products. As DL application domains grow, we would like a deeper understanding of the relationships between training set size, computational scale, and model accuracy improvements to advance the state-of-the-art. 
This paper presents a large scale empirical characterization of generalization error and model size growth as training sets grow. We introduce a methodology for this measurement and test four machine learning domains: machine translation, language modeling, image processing, and speech recognition. Our empirical results show power-law generalization error scaling across a breadth of factors, resulting in power-law exponents---the "steepness" of the learning curve---yet to be explained by theoretical work. Further, model improvements only shift the error but do not appear to affect the power-law exponent. We also show that model size scales sublinearly with data size. These scaling relationships have significant implications on deep learning research, practice, and systems. They can assist model debugging, setting accuracy targets, and decisions about data set growth. They can also guide computing system design and underscore the importance of continued computational scaling.},
	urldate = {2025-02-06},
	publisher = {ArXiv},
	author = {Hestness, Joel and Narang, Sharan and Ardalani, Newsha and Diamos, G. and Jun, Heewoo and Kianinejad, Hassan and Patwary, Md Mostofa Ali and Yang, Yang and Zhou, Yanqi},
	month = dec,
	year = {2017},
}

@misc{chen_mxnet_2015,
	title = {{MXNet}: {A} {Flexible} and {Efficient} {Machine} {Learning} {Library} for {Heterogeneous} {Distributed} {Systems}},
	shorttitle = {{MXNet}},
	howpublished = {\url{https://www.semanticscholar.org/paper/MXNet\%3A-A-Flexible-and-Efficient-Machine-Learning-Chen-Li/62df84d6a4d26f95e4714796c2337c9848cc13b5}},
	abstract = {MXNet is a multi-language machine learning (ML) library to ease the development of ML algorithms, especially for deep neural networks. Embedded in the host language, it blends declarative symbolic expression with imperative tensor computation. It offers auto differentiation to derive gradients. MXNet is computation and memory efficient and runs on various heterogeneous systems, ranging from mobile devices to distributed GPU clusters. 
This paper describes both the API design and the system implementation of MXNet, and explains how embedding of both symbolic expression and tensor operation is handled in a unified fashion. Our preliminary experiments reveal promising results on large scale deep neural network applications using multiple GPU machines.},
	urldate = {2025-01-21},
	journal = {ArXiv},
	author = {Chen, Tianqi and Li, Mu and Li, Yutian and Lin, Min and Wang, Naiyan and Wang, Minjie and Xiao, Tianjun and Xu, Bing and Zhang, Chiyuan and Zhang, Zheng},
	month = dec,
	year = {2015},
	annote = {[TLDR] The API design and the system implementation of MXNet are described, and it is explained how embedding of both symbolic expression and tensor operation is handled in a unified fashion.},
	file = {Full Text PDF:/home/atomwalk12/Zotero/storage/3DEFVTEC/Chen et al. - 2015 - MXNet A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems.pdf:application/pdf},
}

@misc{noauthor_apachemxnet_2025,
	title = {apache/mxnet},
	copyright = {Apache-2.0},
	howpublished = {\url{https://github.com/apache/mxnet}},
	abstract = {Lightweight, Portable, Flexible Distributed/Mobile Deep Learning with Dynamic, Mutation-aware Dataflow Dep Scheduler; for Python, R, Julia, Scala, Go, Javascript and more},
	urldate = {2025-01-21},
	publisher = {The Apache Software Foundation},
	month = jan,
	year = {2025},
	note = {original-date: 2015-04-30T16:21:15Z},
	keywords = {mxnet},
	key = {MXNet},
}




@misc{noauthor_pytorchpytorch_nodate,
	year = {2025},
	title = {pytorch/pytorch: {Tensors} and {Dynamic} neural networks in {Python} with strong {GPU} acceleration},
	howpublished = {\url{https://github.com/pytorch/pytorch}},
	urldate = {2025-01-21},
	keywords = {distributed-systems-ddl-selected, distributed-systems-ddl-selected-selected},
	key = {PyTorch},
}


@misc{xie_optimal_2022,
	title = {Optimal distributed parallel algorithms for deep learning framework {Tensorflow}},
	volume = {52},
	howpublished = {\url{https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111778509&doi=10.1007%2fs10489-021-02588-9&partnerID=40&md5=4b83017d376799bb5bb630f933db0ba3}},
	doi = {10.1007/s10489-021-02588-9},
	abstract = {Since its release, the Tensorflow framework has been widely used in various fields due to its advantages in deep learning. However, it is still at its early state. Its native distributed implementation has difficulty in expanding for large models because it has issues of low utilization of multiple GPUs and slow distribution compared with running on single machine. It is of great significance to reduce the training time through parallel models. In view of this, we firstly provided an in-depth analysis of the implementation principle of Tensorflow and identify the bottlenecks of its native distributed parallel models to improve. Then, two optimal algorithms are designed and implemented based on data parallelism and model parallelism modes of Tensorflow. For data parallelism, the proposed algorithm is implemented to replace the native linear execution mode with pipeline execution mode. As for model parallelism, the native random partitioning mode is replaced by our proposed novel greedy algorithm. Finally, we built a homogeneous distributed cluster and a heterogeneous distributed cluster respectively to verify the effectiveness of the proposed algorithms. Through a number of comparative experiments, we showed that the proposed optimal parallel algorithms can effectively reduce model training time by an average of 26.5\%(or average 1.5x speedup than native distributed algorithms) and improve the utilization of the cluster while keeping the same accuracy level of native Tensorflow. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.},
	number = {4},
	journal = {Applied Intelligence},
	author = {Xie, Y. and He, M. and Ma, T. and Tian, W.},
	year = {2022},
	keywords = {Deep learning, Data parallelism, Clustering algorithms, Learning algorithms, Model parallelism, Training time, Program processors, Learning frameworks, Distributed clusters, Distributed parallel algorithm, Optimal distributed parallel algorithm, Optimal distributed parallel algorithms, Parallel algorithms, Parallel models, Tensorflow, distributed-systems-ddl, distributed-systems-ddl-tensorflow, distributed-systems-ddl-selected},
	pages = {3880--3900},
	annote = {Export Date: 20 January 2025; Cited By: 10},
	annote = {Reference 2: Its native distributed implementation has difficulty in expanding for large models because it has issues of low utilization of multiple GPUs and slow distribution compared with running on single machine. We firstly provided an in-depth analysis of the implementation principle of Tensorflow and identify the bottlenecks of its native distributed parallel models to improve. Two optimal algorithms are designed and implemented based on data parallelism and model parallelism modes of Tensorflow. For data parallelism, the proposed algorithm is implemented to replace the native linear execution mode with pipeline execution mode. As for model parallelism, the native random partitioning mode is replaced by our proposed novel greedy algorithm. Finally, we built a homogeneous distributed cluster and a heterogeneous distributed cluster respectively to verify the effectiveness of the proposed algorithms. Through a number of comparative experiments, we showed that the proposed optimal parallel algorithms can effectively reduce model training time by an average of 26.5\%(or average 1.5x speedup than native distributed algorithms) and improve the utilization of the cluster while keeping the same accuracy level of native Tensorflow.
},
	file = {PDF:/home/atomwalk12/Zotero/storage/RPUTUDCB/Xie et al. - 2022 - Optimal distributed parallel algorithms for deep learning framework Tensorflow.pdf:application/pdf},
}

@misc{abadi_tensorflow_2015,
	title = {{TensorFlow}, {Large}-scale machine learning on heterogeneous systems},
	copyright = {Apache-2.0},
	howpublished = {\url{https://github.com/tensorflow/tensorflow}},
	abstract = {An Open Source Machine Learning Framework for Everyone},
	urldate = {2025-01-21},
	author = {Abadi, Martín and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S. and Davis, Andy and Dean},
	month = nov,
	year = {2015},
	doi = {10.5281/zenodo.4724125},
	keywords = {distributed-systems-ddl, distributed-systems-ddl-selected, distributed-systems-ddl-selected-selected},
}

@misc{rasley_deepspeed_2020,
	title = {{DeepSpeed}: {System} {Optimizations} {Enable} {Training} {Deep} {Learning} {Models} with {Over} 100 {Billion} {Parameters}},
	shorttitle = {{DeepSpeed}},
	howpublished = {\url{https://dl.acm.org/doi/10.1145/3394486.3406703}},
	doi = {10.1145/3394486.3406703},
	abstract = {Explore new techniques in Microsoft's open source library called DeepSpeed, which advances large model training by improving scale, speed, cost, and usability, unlocking the ability to train 100-billion-parameter models. DeepSpeed is compatible with PyTorch. One piece of our library, called ZeRO, is a new parallelized optimizer that greatly reduces the resources needed for model and data parallelism while massively increasing the number of parameters that can be trained. Researchers have used these breakthroughs to create Turing Natural Language Generation (Turing-NLG), which at the time of its release was the largest publicly known language model at 17 billion parameters. In addition we will also go over our latest transformer kernel advancements that led the DeepSpeed team to achieve the world fastest BERT pretraining record. The Zero Redundancy Optimizer (ZeRO) is a novel memory optimization technology for large-scale distributed deep learning. ZeRO can train deep learning models with over 100 billion parameters on the current generation of GPU clusters at three to five times the throughput of the current best system. It also presents a clear path to training models with trillions of parameters, demonstrating an unprecedented leap in deep learning system technology. DeepSpeed brings state-of-the-art training techniques, such as ZeRO, optimized kernels, distributed training, mixed precision, and checkpointing, through lightweight APIs compatible with PyTorch. With just a few lines of code changes to your PyTorch model, you can leverage DeepSpeed to address underlying performance challenges and boost the speed and scale of your training.},
	language = {en},
	urldate = {2025-01-21},
	journal = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
	author = {Rasley, Jeff and Rajbhandari, Samyam and Ruwase, Olatunji and He, Yuxiong},
	month = aug,
	year = {2020},
	note = {Conference Name: KDD '20: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining
ISBN: 9781450379984
Place: Virtual Event CA USA
Publisher: ACM},
	keywords = {distributed-systems-ddl, distributed-systems-ddl-deepspeed, distributed-systems-ddl-selected, distributed-systems-ddl-selected-selected},
	pages = {3505--3506},
	annote = {[TLDR] New techniques in Microsoft's open source library called DeepSpeed, which advances large model training by improving scale, speed, cost, and usability, unlocking the ability to train 100-billion-parameter models, are explored.},
	file = {PDF:/home/atomwalk12/Zotero/storage/9DRCQC3S/Rasley et al. - 2020 - DeepSpeed System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameter.pdf:application/pdf},
}

@misc{noauthor_microsoftdeepspeed_2025,
	author = {Microsoft},
	title = {microsoft/{DeepSpeed}},
	copyright = {Apache-2.0},
	howpublished = {\url{https://github.com/microsoft/DeepSpeed}},
	abstract = {DeepSpeed is a deep learning optimization library that makes distributed training and inference easy, efficient, and effective.},
	urldate = {2025-01-21},
	publisher = {Microsoft},
	month = jan,
	year = {2025},
	note = {original-date: 2020-01-23T18:35:18Z},
	keywords = {billion-parameters, compression, data-parallelism, deep-learning, distributed-systems-ddl, distributed-systems-ddl-deepspeed, distributed-systems-ddl-selected, distributed-systems-ddl-selected-selected, gpu, inference, machine-learning, mixture-of-experts, model-parallelism, pipeline-parallelism, pytorch, trillion-parameters, zero},
}


@misc{sergeev_horovod_2018,
	title = {Horovod: fast and easy distributed deep learning in {TensorFlow}},
	shorttitle = {Horovod},
	howpublished = {\url{http://arxiv.org/abs/1802.05799}},
	doi = {10.48550/arXiv.1802.05799},
	abstract = {Training modern deep learning models requires large amounts of computation, often provided by GPUs. Scaling computation from one GPU to many can enable much faster training and research progress but entails two complications. First, the training library must support inter-GPU communication. Depending on the particular methods employed, this communication may entail anywhere from negligible to significant overhead. Second, the user must modify his or her training code to take advantage of inter-GPU communication. Depending on the training library's API, the modification required may be either significant or minimal. Existing methods for enabling multi-GPU training under the TensorFlow library entail non-negligible communication overhead and require users to heavily modify their model-building code, leading many researchers to avoid the whole mess and stick with slower single-GPU training. In this paper we introduce Horovod, an open source library that improves on both obstructions to scaling: it employs efficient inter-GPU communication via ring reduction and requires only a few lines of modification to user code, enabling faster, easier distributed training in TensorFlow. Horovod is available under the Apache 2.0 license at https://github.com/uber/horovod},
	urldate = {2025-01-21},
	publisher = {arXiv},
	author = {Sergeev, Alexander and Balso, Mike Del},
	month = feb,
	year = {2018},
	note = {arXiv:1802.05799 [cs]},
	keywords = {Computer Science - Machine Learning, distributed-systems-ddl, distributed-systems-ddl-selected-selected, distributed-systems-ddl-snowballed, Statistics - Machine Learning},
	file = {Preprint PDF:/home/atomwalk12/Zotero/storage/UP4SSA4G/Sergeev and Balso - 2018 - Horovod fast and easy distributed deep learning in TensorFlow.pdf:application/pdf;Snapshot:/home/atomwalk12/Zotero/storage/G96PZGXD/1802.html:text/html},
}

@misc{noauthor_horovodhorovod_2025,
	author = {Uber},
	title = {horovod/horovod},
	howpublished = {\url{https://github.com/horovod/horovod}},
	abstract = {Distributed training framework for TensorFlow, Keras, PyTorch, and Apache MXNet.},
	urldate = {2025-01-21},
	publisher = {Horovod},
	month = jan,
	year = {2025},
	note = {original-date: 2017-08-09T19:39:59Z},
	keywords = {baidu, deep-learning, deeplearning, distributed-systems-ddl, distributed-systems-ddl-selected-selected, distributed-systems-ddl-snowballed, keras, machine-learning, machinelearning, mpi, mxnet, pytorch, ray, spark, tensorflow, uber},
}


@misc{noauthor_papers_nodate,
	author = {Papers with Code},
	year = {2025},
	title = {Papers with {Code} - {The} {Methods} {Corpus}},
	howpublished = {\url{https://paperswithcode.com/methods}},
	abstract = {2322 methods • 154506 papers with code.},
	language = {en},
	urldate = {2025-02-02},
	file = {Snapshot:/home/atomwalk12/Zotero/storage/Y5A33CYP/methods.html:text/html},
}


@misc{noauthor_examplesdistributedddpreadmemd_nodate,
	author = {PyTorch},
	year = {2025},
	title = {examples/distributed at main · pytorch/examples},
	howpublished = {\url{https://github.com/pytorch/examples/blob/main/distributed}},
	abstract = {A set of examples around pytorch in Vision, Text, Reinforcement Learning, etc. - pytorch/examples},
	language = {en},
	urldate = {2025-02-03},
	journal = {GitHub},
	file = {Snapshot:/home/atomwalk12/Zotero/storage/V59UK77B/ddp.html:text/html},
}

@misc{noauthor_nvidiacuda-samples_2025,
	author = {NVIDIA Corporation},
	year = {2025},
	title = {{NVIDIA}/cuda-samples},
	howpublished = {\url{https://github.com/NVIDIA/cuda-samples}},
	abstract = {Samples for CUDA Developers which demonstrates features in CUDA Toolkit},
	urldate = {2025-02-03},
	publisher = {NVIDIA Corporation},
	month = feb,
	note = {original-date: 2018-03-27T17:36:24Z},
	keywords = {cuda, cuda-driver-api, cuda-kernels, cuda-opengl},
}
@misc{jiang_unified_nodate,
	journal = {Proceedings of the 14th USENIX Symposium on Operating Systems Design and Implementation},
	year = {2020},
	title = {A {Unified} {Architecture} for {Accelerating} {Distributed} {DNN} {Training} in {Heterogeneous} {GPU}/{CPU} {Clusters}},
	abstract = {Data center clusters that run DNN training jobs are inherently heterogeneous. They have GPUs and CPUs for computation and network bandwidth for distributed training. However, existing distributed DNN training architectures, all-reduce and Parameter Server (PS), cannot fully utilize such heterogeneous resources. In this paper, we present a new distributed DNN training architecture called BytePS. BytePS can leverage spare CPU and bandwidth resources in the cluster to accelerate distributed DNN training tasks running on GPUs. It provides a communication framework that is both proved optimal and uniﬁed – existing all-reduce and PS become two special cases of BytePS. To achieve the proved optimality in practice, BytePS further splits the functionalities of a parameter optimizer. It introduces a Summation Service abstraction for aggregating gradients, which is common for all the optimizers. Summation Service can be accelerated by AVX instructions and can be efﬁciently run on CPUs, while DNN model-related optimizer algorithms are run on GPUs for computation acceleration. BytePS can accelerate DNN training for major frameworks including TensorFlow, PyTorch and MXNet. For representative DNN training jobs with up to 256 GPUs, BytePS outperforms the state-of-the-art open source all-reduce and PS by up to 84\% and 245\%, respectively.},
	language = {en},
	author = {Jiang, Yimin and Lan, Chang and Yi, Bairen and Cui, Yong and Guo, Chuanxiong},
	keywords = {distributed-systems-ddl, distributed-systems-ddl-selected-selected},
	file = {PDF:/home/atomwalk12/Zotero/storage/QE7UACZ4/Jiang et al. - A Unified Architecture for Accelerating Distributed DNN Training in Heterogeneous GPUCPU Clusters.pdf:application/pdf},
}

@misc{noauthor_bytedancebyteps_2025,
	author = {{ByteDance}},
	title = {bytedance/byteps},
	howpublished = {\url{https://github.com/bytedance/byteps}},
	abstract = {A high performance and generic framework for distributed DNN training},
	urldate = {2025-01-22},
	publisher = {Bytedance Inc.},
	month = jan,
	year = {2025},
	note = {original-date: 2019-06-25T07:00:13Z},
	keywords = {deep-learning, distributed-systems-ddl, distributed-systems-ddl-byteps, distributed-systems-ddl-selected-selected, distributed-training, keras, machine-learning, mxnet, pytorch, tensorflow},
}


@misc{abadi_tensorflow_2016,
	title = {{TensorFlow}: {Large}-{Scale} {Machine} {Learning} on {Heterogeneous} {Distributed} {Systems}},
	shorttitle = {{TensorFlow}},
	howpublished = {\url{http://arxiv.org/abs/1603.04467}},
	doi = {10.48550/arXiv.1603.04467},
	abstract = {TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.},
	urldate = {2025-01-22},
	publisher = {arXiv},
	author = {Abadi, Martín and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S. and Davis, Andy and Dean},
	month = mar,
	year = {2016},
	note = {arXiv:1603.04467 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning, distributed-systems-ddl, distributed-systems-ddl-selected-selected, distributed-systems-ddl-snowballed},
	annote = {Comment: Version 2 updates only the metadata, to correct the formatting of Mart{\textbackslash}'in Abadi's name},
	file = {Preprint PDF:/home/atomwalk12/Zotero/storage/5BA54RAE/Abadi et al. - 2016 - TensorFlow Large-Scale Machine Learning on Heterogeneous Distributed Systems.pdf:application/pdf;Snapshot:/home/atomwalk12/Zotero/storage/W9VMLV5B/1603.html:text/html},
}


@misc{noauthor_jax-mljax_2025,
	author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James Johnson and Chris Leary and Dougal Maclaurin and George Necula and Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao Zhang},
	title = {jax-ml/jax},
	copyright = {Apache-2.0},
	howpublished = {\url{https://github.com/jax-ml/jax}},
	abstract = {Composable transformations of Python+NumPy programs: differentiate, vectorize, JIT to GPU/TPU, and more},
	urldate = {2025-01-22},
	publisher = {jax-ml},
	month = jan,
	year = {2025},
	note = {original-date: 2018-10-25T21:25:02Z},
	keywords = {distributed-systems-ddl, distributed-systems-ddl-selected-selected, distributed-systems-ddl-snowballed, jax},
}

@misc{frostig_compiling_nodate,
	journal = {arXiv},
	year = {2018},
	title = {Compiling machine learning programs via high-level tracing},
	abstract = {We describe JAX, a domain-specific tracing JIT compiler for generating high-performance accelerator code from pure Python and Numpy machine learning programs. JAX uses the XLA compiler infrastructure to generate optimized code for the program subroutines that are most favorable for acceleration, and these optimized subroutines can be called and orchestrated by arbitrary Python. Because the system is fully compatible with Autograd, it allows forward- and reverse-mode automatic differentiation of Python functions to arbitrary order. Because JAX supports structured control flow, it can generate code for sophisticated machine learning algorithms while maintaining high performance. We show that by combining JAX with Autograd and Numpy we get an easily programmable and highly performant ML system that targets CPUs, GPUs, and TPUs, capable of scaling to multi-core Cloud TPUs.},
	language = {en},
	author = {Frostig, Roy and Johnson, Matthew James and Leary, Chris},
	keywords = {distributed-systems-ddl, distributed-systems-ddl-selected-selected, distributed-systems-ddl-snowballed},
	file = {PDF:/home/atomwalk12/Zotero/storage/3TJUWJHJ/Frostig et al. - Compiling machine learning programs via high-level tracing.pdf:application/pdf},
}


@misc{li_colossal-ai_2023,
	title = {Colossal-{AI}: {A} {Unified} {Deep} {Learning} {System} {For} {Large}-{Scale} {Parallel} {Training}},
	shorttitle = {Colossal-{AI}},
	howpublished = {\url{http://arxiv.org/abs/2110.14883}},
	doi = {10.48550/arXiv.2110.14883},
	abstract = {The success of Transformer models has pushed the deep learning model scale to billions of parameters. Due to the limited memory resource of a single GPU, However, the best practice for choosing the optimal parallel strategy is still lacking, since it requires domain expertise in both deep learning and parallel computing. The Colossal-AI system addressed the above challenge by introducing a unified interface to scale your sequential code of model training to distributed environments. It supports parallel training methods such as data, pipeline, tensor, and sequence parallelism, as well as heterogeneous training methods integrated with zero redundancy optimizer. Compared to the baseline system, Colossal-AI can achieve up to 2.76 times training speedup on large-scale models.},
	urldate = {2025-01-22},
	publisher = {arXiv},
	author = {Li, Shenggui and Liu, Hongxin and Bian, Zhengda and Fang, Jiarui and Huang, Haichen and Liu, Yuliang and Wang, Boxiang and You, Yang},
	month = oct,
	year = {2023},
	note = {arXiv:2110.14883 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning},
	file = {Preprint PDF:/home/atomwalk12/Zotero/storage/JVITUQ25/Li et al. - 2023 - Colossal-AI A Unified Deep Learning System For Large-Scale Parallel Training.pdf:application/pdf;Snapshot:/home/atomwalk12/Zotero/storage/5URDZC59/2110.html:text/html},
}

@misc{noauthor_hpcaitechcolossalai_2025,
	title = {hpcaitech/{ColossalAI}},
	copyright = {Apache-2.0},
	howpublished = {\url{https://github.com/hpcaitech/ColossalAI}},
	abstract = {Making large AI models cheaper, faster and more accessible},
	urldate = {2025-01-22},
	author = {HPC-AI Tech},
	publisher = {HPC-AI Tech},
	month = jan,
	year = {2025},
	note = {original-date: 2021-10-28T16:19:44Z},
	keywords = {ai, big-model, data-parallelism, deep-learning, distributed-computing, foundation-models, heterogeneous-training, hpc, inference, large-scale, model-parallelism, pipeline-parallelism},
}

@misc{wolf_huggingfaces_2020,
	title = {{HuggingFace}'s {Transformers}: {State}-of-the-art {Natural} {Language} {Processing}},
	shorttitle = {{HuggingFace}'s {Transformers}},
	howpublished = {\url{http://arxiv.org/abs/1910.03771}},
	doi = {10.48550/arXiv.1910.03771},
	abstract = {Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. {\textbackslash}textit\{Transformers\} is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. {\textbackslash}textit\{Transformers\} is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at {\textbackslash}url\{https://github.com/huggingface/transformers\}.},
	urldate = {2025-01-22},
	publisher = {arXiv},
	author = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, Rémi and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and Platen, Patrick von and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Scao, Teven Le and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander M.},
	month = jul,
	year = {2020},
	note = {arXiv:1910.03771 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 8 pages, 4 figures, more details at https://github.com/huggingface/transformers},
	file = {Preprint PDF:/home/atomwalk12/Zotero/storage/LLD3DIDW/Wolf et al. - 2020 - HuggingFace's Transformers State-of-the-art Natural Language Processing.pdf:application/pdf;Snapshot:/home/atomwalk12/Zotero/storage/WN99XIQ3/1910.html:text/html},
}

@misc{noauthor_huggingfaceaccelerate_2025,
	title = {huggingface/accelerate},
	copyright = {Apache-2.0},
	howpublished = {\url{https://github.com/huggingface/accelerate}},
	abstract = {🚀 A simple way to launch, train, and use PyTorch models on almost any device and distributed configuration, automatic mixed precision (including fp8), and easy-to-configure FSDP and DeepSpeed support},
	urldate = {2025-01-22},
	author = {Hugging Face},
	publisher = {Hugging Face},
	month = jan,
	year = {2025},
	note = {original-date: 2020-10-30T13:27:12Z},
}

@misc{falcon_pytorch_2019,
	title = {{PyTorch} {Lightning}},
	copyright = {Apache-2.0},
	howpublished = {\url{https://github.com/Lightning-AI/lightning}},
	abstract = {Pretrain, finetune ANY AI model of ANY size on multiple GPUs, TPUs with zero code changes.},
	urldate = {2025-01-22},
	author = {Falcon, William and {The PyTorch Lightning team}},
	month = mar,
	year = {2019},
	doi = {10.5281/zenodo.3828935},
}

@misc{shoeybi_megatron-lm_2020,
	title = {Megatron-{LM}: {Training} {Multi}-{Billion} {Parameter} {Language} {Models} {Using} {Model} {Parallelism}},
	shorttitle = {Megatron-{LM}},
	howpublished = {\url{http://arxiv.org/abs/1909.08053}},
	doi = {10.48550/arXiv.1909.08053},
	abstract = {Recent work in language modeling demonstrates that training large transformer models advances the state of the art in Natural Language Processing applications. However, very large models can be quite difficult to train due to memory constraints. In this work, we present our techniques for training very large transformer models and implement a simple, efficient intra-layer model parallel approach that enables training transformer models with billions of parameters. Our approach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model parallelism, and can be fully implemented with the insertion of a few communication operations in native PyTorch. We illustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 GPUs. We sustain 15.1 PetaFLOPs across the entire application with 76\% scaling efficiency when compared to a strong single GPU baseline that sustains 39 TeraFLOPs, which is 30\% of peak FLOPs. To demonstrate that large language models can further advance the state of the art (SOTA), we train an 8.3 billion parameter transformer language model similar to GPT-2 and a 3.9 billion parameter model similar to BERT. We show that careful attention to the placement of layer normalization in BERT-like models is critical to achieving increased performance as the model size grows. Using the GPT-2 model we achieve SOTA results on the WikiText103 (10.8 compared to SOTA perplexity of 15.8) and LAMBADA (66.5\% compared to SOTA accuracy of 63.2\%) datasets. Our BERT model achieves SOTA results on the RACE dataset (90.9\% compared to SOTA accuracy of 89.4\%).},
	urldate = {2025-01-22},
	publisher = {arXiv},
	author = {Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
	month = mar,
	year = {2020},
	note = {arXiv:1909.08053 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/home/atomwalk12/Zotero/storage/T3BLVW33/Shoeybi et al. - 2020 - Megatron-LM Training Multi-Billion Parameter Language Models Using Model Parallelism.pdf:application/pdf;Snapshot:/home/atomwalk12/Zotero/storage/WNHHGISL/1909.html:text/html},
}

@misc{noauthor_nvidiamegatron-lm_2025,
	title = {{NVIDIA}/{Megatron}-{LM}},
	howpublished = {\url{https://github.com/NVIDIA/Megatron-LM}},
	abstract = {Ongoing research training transformer models at scale},
	urldate = {2025-01-22},
	publisher = {NVIDIA Corporation},
	month = jan,
	year = {2025},
	note = {original-date: 2019-03-21T16:15:52Z},
	keywords = {large-language-models, model-para, transformers},
	author = {NVIDIA Corporation},
}


@Misc{FairScale2021,
  author =       {{FairScale authors}},
  title =        {FairScale:  A general purpose modular PyTorch library for high performance and large scale training},
  howpublished = {\url{https://github.com/facebookresearch/fairscale}},
  year =         {2021}
}

@misc{karakus_amazon_2021,
	title = {Amazon {SageMaker} {Model} {Parallelism}: {A} {General} and {Flexible} {Framework} for {Large} {Model} {Training}},
	shorttitle = {Amazon {SageMaker} {Model} {Parallelism}},
	howpublished = {\url{http://arxiv.org/abs/2111.05972}},
	doi = {10.48550/arXiv.2111.05972},
	abstract = {With deep learning models rapidly growing in size, systems-level solutions for large-model training are required. We present Amazon SageMaker model parallelism, a software library that integrates with PyTorch, and enables easy training of large models using model parallelism and other memory-saving features. In contrast to existing solutions, the implementation of the SageMaker library is much more generic and flexible, in that it can automatically partition and run pipeline parallelism over arbitrary model architectures with minimal code change, and also offers a general and extensible framework for tensor parallelism, which supports a wider range of use cases, and is modular enough to be easily applied to new training scripts. The library also preserves the native PyTorch user experience to a much larger degree, supporting module re-use and dynamic graphs, while giving the user full control over the details of the training step. We evaluate performance over GPT-3, RoBERTa, BERT, and neural collaborative filtering, and demonstrate competitive performance over existing solutions.},
	urldate = {2025-01-22},
	publisher = {arXiv},
	author = {Karakus, Can and Huilgol, Rahul and Wu, Fei and Subramanian, Anirudh and Daniel, Cade and Cavdar, Derya and Xu, Teng and Chen, Haohan and Rahnama, Arash and Quintela, Luis},
	month = nov,
	year = {2021},
	note = {arXiv:2111.05972 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning},
	annote = {Comment: 24 pages. Submitted for review},
	file = {Preprint PDF:/home/atomwalk12/Zotero/storage/JXINKX8T/Karakus et al. - 2021 - Amazon SageMaker Model Parallelism A General and Flexible Framework for Large Model Training.pdf:application/pdf;Snapshot:/home/atomwalk12/Zotero/storage/6G8FQV5L/2111.html:text/html},
}

@misc{noauthor_awsamazon-sagemaker-examples_2025,
	title = {aws/amazon-sagemaker-examples},
	copyright = {Apache-2.0},
	howpublished = {\url{https://github.com/aws/amazon-sagemaker-examples}},
	abstract = {Example 📓 Jupyter notebooks that demonstrate how to build, train, and deploy machine learning models using 🧠 Amazon SageMaker.},
	urldate = {2025-01-22},
	publisher = {Amazon Web Services},
	month = jan,
	year = {2025},
	author = {Amazon Web Services},
	note = {original-date: 2017-10-23T05:55:22Z},
	keywords = {aws, data-science, deep-learning, examples, inference, jupyter-notebook, machine-learning, mlops, reinforcement-learning, sagemaker, training},
}

@misc{noauthor_azureazureml-examples_2025,
	title = {Azure/azureml-examples},
	copyright = {MIT},
	howpublished = {\url{https://github.com/Azure/azureml-examples}},
	abstract = {Official community-driven Azure Machine Learning examples, tested with GitHub Actions.},
	urldate = {2025-01-22},
	publisher = {Microsoft Azure},
	author = {Microsoft Azure},
	month = jan,
	year = {2025},
	note = {original-date: 2020-08-21T18:04:26Z},
	keywords = {azure, azure-machine-learning, azureml, data-science, ml},
}


@misc{noauthor_googlecloudplatformgenerative-ai_2025,
	title = {{GoogleCloudPlatform}/generative-ai},
	copyright = {Apache-2.0},
	howpublished = {\url{https://github.com/GoogleCloudPlatform/generative-ai}},
	abstract = {Sample code and notebooks for Generative AI on Google Cloud, with Gemini on Vertex AI},
	urldate = {2025-01-22},
	publisher = {Google Cloud Platform},
	month = jan,
	year = {2025},
	note = {original-date: 2023-05-05T12:31:07Z},
	keywords = {gemini, gemini-api, generative-ai, google, google-cloud, google-gemini, langchain, llm, palm-api, vertex-ai, vertex-ai-gemini-api, vertexai},
}

@misc{lepikhin_gshard_2020,
	title = {{GShard}: {Scaling} {Giant} {Models} with {Conditional} {Computation} and {Automatic} {Sharding}},
	shorttitle = {{GShard}},
	howpublished = {\url{http://arxiv.org/abs/2006.16668}},
	doi = {10.48550/arXiv.2006.16668},
	abstract = {Neural network scaling has been critical for improving the model quality in many real-world machine learning applications with vast amounts of training data and compute. Although this trend of scaling is affirmed to be a sure-fire approach for better model quality, there are challenges on the path such as the computation cost, ease of programming, and efficient implementation on parallel devices. GShard is a module composed of a set of lightweight annotation APIs and an extension to the XLA compiler. It provides an elegant way to express a wide range of parallel computation patterns with minimal changes to the existing model code. GShard enabled us to scale up multilingual neural machine translation Transformer model with Sparsely-Gated Mixture-of-Experts beyond 600 billion parameters using automatic sharding. We demonstrate that such a giant model can efficiently be trained on 2048 TPU v3 accelerators in 4 days to achieve far superior quality for translation from 100 languages to English compared to the prior art.},
	urldate = {2025-01-22},
	publisher = {arXiv},
	author = {Lepikhin, Dmitry and Lee, HyoukJoong and Xu, Yuanzhong and Chen, Dehao and Firat, Orhan and Huang, Yanping and Krikun, Maxim and Shazeer, Noam and Chen, Zhifeng},
	month = jun,
	year = {2020},
	note = {arXiv:2006.16668 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:/home/atomwalk12/Zotero/storage/NUIVKAZX/Lepikhin et al. - 2020 - GShard Scaling Giant Models with Conditional Computation and Automatic Sharding.pdf:application/pdf;Snapshot:/home/atomwalk12/Zotero/storage/RJKL5CGM/2006.html:text/html},
}

@misc{moritz_ray_2018,
	title = {Ray: {A} {Distributed} {Framework} for {Emerging} {AI} {Applications}},
	shorttitle = {Ray},
	howpublished = {\url{http://arxiv.org/abs/1712.05889}},
	doi = {10.48550/arXiv.1712.05889},
	abstract = {The next generation of AI applications will continuously interact with the environment and learn from these interactions. These applications impose new and demanding systems requirements, both in terms of performance and flexibility. In this paper, we consider these requirements and present Ray---a distributed system to address them. Ray implements a unified interface that can express both task-parallel and actor-based computations, supported by a single dynamic execution engine. To meet the performance requirements, Ray employs a distributed scheduler and a distributed and fault-tolerant store to manage the system's control state. In our experiments, we demonstrate scaling beyond 1.8 million tasks per second and better performance than existing specialized systems for several challenging reinforcement learning applications.},
	urldate = {2025-01-22},
	publisher = {arXiv},
	author = {Moritz, Philipp and Nishihara, Robert and Wang, Stephanie and Tumanov, Alexey and Liaw, Richard and Liang, Eric and Elibol, Melih and Yang, Zongheng and Paul, William and Jordan, Michael I. and Stoica, Ion},
	month = sep,
	year = {2018},
	note = {arXiv:1712.05889 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: 17 pages, 14 figures, 13th USENIX Symposium on Operating Systems Design and Implementation, 2018},
	file = {Preprint PDF:/home/atomwalk12/Zotero/storage/789QX42R/Moritz et al. - 2018 - Ray A Distributed Framework for Emerging AI Applications.pdf:application/pdf;Snapshot:/home/atomwalk12/Zotero/storage/Z6L76EKF/1712.html:text/html},
}

@misc{noauthor_ray-projectray_2025,
	title = {ray-project/ray},
	copyright = {Apache-2.0},
	howpublished = {\url{https://github.com/ray-project/ray}},
	abstract = {Ray is an AI compute engine. Ray consists of a core distributed runtime and a set of AI Libraries for accelerating ML workloads.},
	urldate = {2025-01-22},
	publisher = {ray-project},
	author = {ray-project},
	month = jan,
	year = {2025},
	note = {original-date: 2016-10-25T19:38:30Z},
	keywords = {automl, data-science, deep-learning, deployment, distributed, hyperparameter-optimization, hyperparameter-search, java, llm-serving, machine-learning, model-selection, optimization, parallel, python, pytorch, ray, reinforcement-learning, rllib, serving, tensorflow},
}

@misc{sdgilley_azure_nodate,
	title = {Azure {Machine} {Learning} documentation},
	howpublished = {\url{https://learn.microsoft.com/en-us/azure/machine-learning/?view=azureml-api-2}},
	abstract = {Learn how to train and deploy models and manage the ML lifecycle (MLOps) with Azure Machine Learning. Tutorials, code examples, API references, and more.},
	language = {en-us},
	year = {2021},
	urldate = {2025-01-22},
	author = {sdgilley},
	file = {Snapshot:/home/atomwalk12/Zotero/storage/KIQW3TQ2/machine-learning.html:text/html},
}


@misc{noauthor_amazon_nodate,
	title = {Amazon {SageMaker} {Documentation}},
	howpublished = {\url{https://docs.aws.amazon.com/sagemaker/}},
	author = {Amazon Web Services},
	year = {2021},
	file = {Amazon SageMaker Documentation:/home/atomwalk12/Zotero/storage/VWW6G38L/sagemaker.html:text/html},
}

@misc{noauthor_fairscale_nodate,
	title = {{FairScale} {Documentation} {\textbar} {FairScale} documentation},
	howpublished = {\url{https://fairscale.readthedocs.ioindex.html}},
	abstract = {API docs for FairScale. FairScale is a PyTorch extension library for high performance and large scale training.},
	language = {en},
	urldate = {2025-01-22},
	author = {FairScale},
	year = {2021},
	file = {Snapshot:/home/atomwalk12/Zotero/storage/5MLSVDSS/latest.html:text/html},
}
@misc{noauthor_overview_nodate,
	title = {Overview {Lightning} {AI}},
	howpublished = {\url{https://lightning.ai/docs/overview/getting-started}},
	abstract = {The all-in-one platform for AI development. Code together. Prototype. Train. Scale. Serve. From your browser - with zero setup. From the creators of PyTorch Lightning.},
	language = {en},
	year = {2010},
	author = {Lightning AI},
	urldate = {2025-01-22},
	file = {Snapshot:/home/atomwalk12/Zotero/storage/R3ZKYXXR/getting-started.html:text/html},
}


@misc{noauthor_vertex_nodate,
	title = {Vertex {AI} documentation},
	howpublished = {\url{https://cloud.google.com/vertex-ai/docs}},
	abstract = {Documentation for Vertex AI, a suite of machine learning tools that enables developers to train high-quality models specific to their business needs.},
	language = {en},
	author = {Google Cloud},
	year = {2021},
	urldate = {2025-01-22},
	file = {Snapshot:/home/atomwalk12/Zotero/storage/59SL92FV/docs.html:text/html},
}

@misc{noauthor_googlecloudplatformvertex-ai-samples_2025,
	title = {{GoogleCloudPlatform}/vertex-ai-samples},
	copyright = {Apache-2.0},
	howpublished = {\url{https://github.com/GoogleCloudPlatform/vertex-ai-samples}},
	abstract = {Notebooks, code samples, sample apps, and other resources that demonstrate how to use, develop and manage machine learning and generative AI workflows using Google Cloud Vertex AI.},
	urldate = {2025-01-22},
	publisher = {Google Cloud Platform},
	author = {Google Cloud Platform},
	month = jan,
	year = {2025},
	note = {original-date: 2021-05-27T00:06:43Z},
	keywords = {automl, colab, colab-enterprise, gemini, gemini-api, genai, generative-ai, google-cloud-platform, ml, mlops, model, model-garden, notebook, pipeline, predictions, samples, vertex-ai, vertexai, workbench},
}



@misc{okuta_cupy_2017,
	key = {cupy},
	title = {{CuPy} : {A} {NumPy}-{Compatible} {Library} for {NVIDIA} {GPU} {Calculations}},
	shorttitle = {{CuPy}},
	howpublished = {\url{https://www.semanticscholar.org/paper/CuPy-\%3A-A-NumPy-Compatible-Library-for-NVIDIA-GPU-Okuta-Unno/a59da4639436f582e483347a4833e7659fd3e598}},
	abstract = {CuPy 1 is an open-source library with NumPy syntax that increases speed by doing matrix operations on NVIDIA GPUs. It is accelerated with the CUDA platform from NVIDIA and also uses CUDA-related libraries, including cuBLAS, cuDNN, cuRAND, cuSOLVER, cuSPARSE, and NCCL, to make full use of the GPU architecture. CuPy {cuBLAS} 12.8 documentation},
	urldate = {2025-01-24},
	author = {NVIDIA},
	booktitle = {CuPy},
	year = {2017},
}


@misc{noauthor_cublas_nodate,
	title = {{cuBLAS}},
	howpublished = {\url{https://developer.nvidia.com/cublas}},
	language = {en-US},
	urldate = {2025-01-25},
	journal = {NVIDIA Developer},
	author = {NVIDIA},
	year = {2007},
	file = {Snapshot:/home/atomwalk12/Zotero/storage/K2ZLUQ6T/cublas.html:text/html},
}

@misc{noauthor_nvidianccl_2025,
	title = {{NVIDIA}/nccl},
	howpublished = {\url{https://github.com/NVIDIA/nccl}},
	abstract = {Optimized primitives for collective multi-GPU communication},
	urldate = {2025-01-24},
	publisher = {NVIDIA Corporation},
	keywords = {nccl},
	month = jan,
	key = {nccl},
	year = {2025},
	note = {original-date: 2015-11-14T00:12:04Z},
}
@misc{noauthor_cupycupy_2025,
	title = {cupy/cupy},
	copyright = {MIT},
	howpublished = {\url{https://github.com/cupy/cupy}},
	key = {cupy},
	abstract = {NumPy \& SciPy for GPU},
	urldate = {2025-01-25},
	publisher = {CuPy},
	month = jan,
	year = {2025},
	note = {original-date: 2016-11-01T09:54:45Z},
	keywords = {cublas, cuda, cudnn, cupy, curand, cusolver, cusparse, cusparselt, cutensor, gpu, nccl, numpy, nvrtc, nvtx, python, rocm, scipy, tensor},
}

@misc{yin_comparative_2021,
	title = {Comparative evaluation of deep learning workloads for leadership-class systems},
	volume = {1},
	issn = {2772-4859},
	howpublished = {\url{https://www.sciencedirect.com/science/article/pii/S2772485921000053}},
	doi = {10.1016/j.tbench.2021.100005},
	abstract = {Deep learning (DL) workloads and their performance at scale are becoming important factors to consider as we design, develop and deploy next-generation high-performance computing systems. Since DL applications rely heavily on DL frameworks and underlying compute (CPU/GPU) stacks, it is essential to gain a holistic understanding from compute kernels, models, and frameworks of popular DL stacks, and to assess their impact on science-driven, mission-critical applications. At Oak Ridge Leadership Computing Facility (OLCF), we employ a set of micro and macro DL benchmarks established through the Collaboration of Oak Ridge, Argonne, and Livermore (CORAL) to evaluate the AI readiness of our next-generation supercomputers. In this paper, we present our early observations and performance benchmark comparisons between the Nvidia V100 based Summit system with its CUDA stack and an AMD MI100 based testbed system with its ROCm stack. We take a layered perspective on DL benchmarking and point to opportunities for future optimizations in the technologies that we consider.},
	number = {1},
	urldate = {2025-01-24},
	journal = {BenchCouncil Transactions on Benchmarks, Standards and Evaluations},
	author = {Yin, Junqi and Tsaris, Aristeidis and Dash, Sajal and Miller, Ross and Wang, Feiyi and Shankar, Mallikarjun (Arjun)},
	month = oct,
	year = {2021},
	keywords = {CORAL benchmark, Deep learning stack, ROCm},
	pages = {100005},
	file = {Full Text PDF:/home/atomwalk12/Zotero/storage/ZC54WFK2/Yin et al. - 2021 - Comparative evaluation of deep learning workloads for leadership-class systems.pdf:application/pdf},
}


@misc{nguyen_machine_2019,
	title = {Machine {Learning} and {Deep} {Learning} frameworks and libraries for large-scale data mining: a survey},
	volume = {52},
	issn = {1573-7462},
	shorttitle = {Machine {Learning} and {Deep} {Learning} frameworks and libraries for large-scale data mining},
	howpublished = {\url{https://doi.org/10.1007/s10462-018-09679-z}},
	doi = {10.1007/s10462-018-09679-z},
	abstract = {The combined impact of new computing resources and techniques with an increasing avalanche of large datasets, is transforming many research areas and may lead to technological breakthroughs that can be used by billions of people. In the recent years, Machine Learning and especially its subfield Deep Learning have seen impressive advances. Techniques developed within these two fields are now able to analyze and learn from huge amounts of real world examples in a disparate formats. While the number of Machine Learning algorithms is extensive and growing, their implementations through frameworks and libraries is also extensive and growing too. The software development in this field is fast paced with a large number of open-source software coming from the academy, industry, start-ups or wider open-source communities. This survey presents a recent time-slide comprehensive overview with comparisons as well as trends in development and usage of cutting-edge Artificial Intelligence software. It also provides an overview of massive parallelism support that is capable of scaling computation effectively and efficiently in the era of Big Data.},
	language = {en},
	number = {1},
	urldate = {2025-01-25},
	journal = {Artificial Intelligence Review},
	author = {Nguyen, Giang and Dlugolinsky, Stefan and Bobák, Martin and Tran, Viet and López García, Álvaro and Heredia, Ignacio and Malík, Peter and Hluchý, Ladislav},
	month = jun,
	year = {2019},
	keywords = {Artificial Intelligence, Graphics processing unit (GPU), Parallel processing, Deep Learning, Machine Learning, Artificial Intelligence software, Intensive computing, Large-scale data mining, to-check},
	pages = {77--124},
	file = {Full Text PDF:/home/atomwalk12/Zotero/storage/TRD4F4EK/Nguyen et al. - 2019 - Machine Learning and Deep Learning frameworks and libraries for large-scale data mining a survey.pdf:application/pdf},
}


@misc{noauthor_intelcompute-samples_2025,
	title = {intel/compute-samples},
	copyright = {MIT},
	key = {Intel},
	howpublished = {\url{https://github.com/intel/compute-samples}},
	abstract = {Intel® GPU Compute Samples},
	urldate = {2025-02-03},
	publisher = {Intel® Corporation},
	month = jan,
	year = {2025},
	note = {original-date: 2017-10-20T22:04:08Z},
	keywords = {gpu, intel, opencl},
}
@misc{noauthor_rocmrocblas_2025,
	title = {{ROCm}/{rocBLAS}},
	howpublished = {\url{https://github.com/ROCm/rocBLAS}},
	abstract = {Next generation BLAS implementation for ROCm platform},
	urldate = {2025-01-24},
	publisher = {AMD ROCm Software},
	keywords = {rocblas},
	month = jan,
	key = {rocblas},
	year = {2025},
	note = {original-date: 2017-06-27T17:51:22Z},
}

@misc{noauthor_amdamd-lab-notes_2025,
	title = {amd/amd-lab-notes},
	copyright = {MIT},
	key={AMD},
	howpublished = {\url{https://github.com/amd/amd-lab-notes}},
	abstract = {AMD lab notes with code examples to demonstrate use of AMD GPUs},
	urldate = {2025-02-03},
	publisher = {AMD},
	month = jan,
	year = {2025},
	note = {original-date: 2022-10-20T02:26:59Z},
}

@misc{noauthor_khronosgroupsycl-docs_2025,
	title = {{KhronosGroup}/{SYCL}-{Docs}},
	howpublished = {\url{https://github.com/KhronosGroup/SYCL-Docs}},
	abstract = {SYCL Open Source Specification},
	urldate = {2025-01-24},
	key = {KhronosGroup},
	publisher = {The Khronos Group},
	month = jan,
	year = {2025},
	note = {original-date: 2019-05-13T16:07:19Z},
}

@misc{noauthor_khronosgroupopencl-sdk_2025,
	title = {{KhronosGroup}/{OpenCL}-{SDK}},
	copyright = {Apache-2.0},
	howpublished = {\url{https://github.com/KhronosGroup/OpenCL-SDK}},
	abstract = {OpenCL SDK},
	urldate = {2025-01-24},
	publisher = {The Khronos Group},
	month = jan,
	key = {KhronosGroup},
	year = {2025},
	note = {original-date: 2020-02-10T17:18:01Z},
}

@misc{trott_kokkos_2022,
	title = {Kokkos 3: {Programming} {Model} {Extensions} for the {Exascale} {Era}},
	shorttitle = {Kokkos 3},
	howpublished = {\url{https://github.com/kokkos/kokkos}},
	abstract = {Kokkos C++ Performance Portability Programming Ecosystem: The Programming Model - Parallel Execution and Memory Abstraction},
	urldate = {2025-01-24},
	author = {Trott, Christian R. and Lebrun-Grandié, Damien and Arndt, Daniel and Ciesko, Jan and Dang, Vinh and Ellingwood, Nathan and Gayatri, Rahulkumar and Harvey, Evan and Hollman, Daisy S. and Ibanez, Dan and Liber, Nevin and Madsen, Jonathan and Miles, Jeff and Poliakoff, David and Powell, Amy and Rajamanickam, Sivasankaran and Simberg, Mikael and Sunderland, Dan and Turcksin, Bruno and Wilke, Jeremiah},
	year = {2022},
	doi = {10.1109/TPDS.2021.3097283},
	note = {Issue: 4
Pages: 805–817
Publication Title: IEEE Transactions on Parallel and Distributed Systems
Volume: 33
original-date: 2015-04-08T21:55:55Z},
	file = {Full Text:/home/atomwalk12/Zotero/storage/S656RN47/Trott et al. - 2022 - Kokkos 3 Programming Model Extensions for the Exascale Era.pdf:application/pdf},
}

@misc{noauthor_rocmhip_2025,
	title = {{ROCm}/{HIP}},
	copyright = {MIT},
	howpublished = {\url{https://github.com/ROCm/HIP}},
	abstract = {HIP: C++ Heterogeneous-Compute Interface for Portability},
	urldate = {2025-01-25},
	publisher = {AMD ROCm™ Software},
	month = jan,
	key = {AMD},
	year = {2025},
	note = {original-date: 2016-01-07T17:41:56Z},
	keywords = {cuda, hip, hip-kernel-language, hip-portability, hip-runtime, hipify},
}




@misc{noauthor_numbanumba_2025,
	title = {numba/numba},
	copyright = {BSD-2-Clause},
	howpublished = {\url{https://github.com/numba/numba}},
	abstract = {NumPy aware dynamic Python compiler using LLVM},
	urldate = {2025-01-24},
	key = {numba},
	publisher = {Numba},
	month = jan,
	year = {2025},
	note = {original-date: 2012-03-08T11:12:43Z},
	keywords = {parallel, python, cuda, compiler, llvm, numba, numpy},
}

@misc{noauthor_juliagpucudajl_2025,
	title = {{JuliaGPU}/{CUDA}.jl},
	howpublished = {\url{https://github.com/JuliaGPU/CUDA.jl}},
	abstract = {CUDA programming in Julia.},
	urldate = {2025-01-25},
	key = {JuliaGPU},
	publisher = {JuliaGPU},
	month = jan,
	year = {2025},
	note = {original-date: 2019-09-06T13:35:08Z},
	keywords = {cuda, gpu, hacktoberfest, julia},
}

@misc{noauthor_juliagpuamdgpujl_2025,
	title = {{JuliaGPU}/{AMDGPU}.jl},
	howpublished = {\url{https://github.com/JuliaGPU/AMDGPU.jl}},
	abstract = {AMD GPU  (ROCm) programming in Julia},
	urldate = {2025-01-25},
	key = {JuliaGPU},
	publisher = {JuliaGPU},
	month = jan,
	year = {2025},
	note = {original-date: 2020-07-02T16:16:24Z},
	keywords = {amdgpu, julia, rocm},
}

@misc{besard_oneapijl_2022,
	title = {{oneAPI}.jl},
	howpublished = {\url{https://github.com/JuliaGPU/oneAPI.jl}},
	abstract = {Julia support for the oneAPI programming toolkit.},
	urldate = {2025-01-25},
	author = {Besard, Tim},
	month = oct,
	year = {2022},
	doi = {10.5281/zenodo.7139359},
}

@misc{noauthor_uxlfoundationonemath_2025,
	title = {uxlfoundation/{oneMath}},
	copyright = {Apache-2.0},
	howpublished = {\url{https://github.com/uxlfoundation/oneMath}},
	abstract = {oneAPI Math Library (oneMath)},
	urldate = {2025-01-24},
	key = {UXL},
	publisher = {Unified Acceleration (UXL) Foundation},
	month = jan,
	year = {2025},
	note = {original-date: 2020-03-05T23:55:03Z},
	keywords = {gpu, hpc, blas, cuda, api, cpu, dpcpp, intel, math-libraries, oneapi, onemkl, parallel-computing, parallel-programming, performance, rng},
}

@misc{onednn_contributors_oneapi_2025,
	title = {{oneAPI} {Deep} {Neural} {Network} {Library} ({oneDNN})},
	copyright = {Apache-2.0},
	howpublished = {\url{https://github.com/oneapi-src/oneDNN}},
	abstract = {oneAPI Deep Neural Network Library (oneDNN)},
	urldate = {2025-01-24},
	author = {{oneDNN Contributors}},
	month = jan,
	year = {2025},
	note = {original-date: 2016-05-09T23:26:42Z},
}

@misc{noauthor_uxlfoundationoneccl_2025,
	title = {uxlfoundation/{oneCCL}},
	howpublished = {\url{https://github.com/uxlfoundation/oneCCL}},
	abstract = {oneAPI Collective Communications Library (oneCCL)},
	urldate = {2025-01-24},
	publisher = {Unified Acceleration (UXL) Foundation},
	month = jan,
	key = {UXL},
	year = {2025},
	note = {original-date: 2019-09-09T21:57:46Z},
	keywords = {deep-learning, oneapi, ai-machine-learning, ai-training, libraries, swrepo, sycl},
}

@misc{noauthor_rocmmiopen_2025,
	title = {{ROCm}/{MIOpen}},
	howpublished = {\url{https://github.com/ROCm/MIOpen}},
	abstract = {AMD's Machine Intelligence Library},
	key = {AMD},
	urldate = {2025-01-24},
	publisher = {AMD ROCm™ Software},
	month = jan,
	year = {2025},
	note = {original-date: 2017-06-27T17:51:22Z},
}

@misc{noauthor_enccsgpu-programming_nodate,
	title = {{ENCCS}/gpu-programming: {Meta}-{GPU} lesson covering general aspects of {GPU} programming as well as specific frameworks},
	author = {ENCCS},
	year = {2025},
	howpublished = {\url{https://github.com/ENCCS/gpu-programming/tree/main}},
	urldate = {2025-01-27},
	file = {ENCCS/gpu-programming\: Meta-GPU lesson covering general aspects of GPU programming as well as specific frameworks:/home/atomwalk12/Zotero/storage/3BNE5JAD/main.html:text/html},
}

@misc{noauthor_build_nodate,
	title = {Build software better, together},
	howpublished = {\url{https://github.com/search/advanced}},
	abstract = {GitHub is where people build software. More than 150 million people use GitHub to discover, fork, and contribute to over 420 million projects.},
	language = {en},
	year = {2025},
	author = {GitHub},
	urldate = {2025-01-26},
	file = {Snapshot:/home/atomwalk12/Zotero/storage/RRS9HJ6C/advanced.html:text/html},
}

@misc{noauthor_perplexity_nodate,
	title = {Perplexity},
	howpublished = {\url{https://www.perplexity.ai}},
	abstract = {Perplexity is a free AI-powered answer engine that provides accurate, trusted, and real-time answers to any question.},
	language = {en},
	urldate = {2025-01-26},
	journal = {Perplexity AI},
	year = {2025},
	author = {Perplexity},
	file = {Snapshot:/home/atomwalk12/Zotero/storage/935KGR7I/www.perplexity.ai.html:text/html},
}
@misc{noauthor_deepseek_nodate,
	title = {{DeepSeek}},
	howpublished = {\url{https://chat.deepseek.com}},
	key = {DeepSeek},
	year = {2025},
	abstract = {Chat with DeepSeek AI.},
	language = {en},
	urldate = {2025-01-26},
	file = {Snapshot:/home/atomwalk12/Zotero/storage/EMP4I3Z5/chat.deepseek.com.html:text/html},
}
@misc{zhou_map_2016,
	title = {A {Map} of {Threats} to {Validity} of {Systematic} {Literature} {Reviews} in {Software} {Engineering}},
	howpublished = {\url{https://ieeexplore.ieee.org/abstract/document/7890583}},
	doi = {10.1109/APSEC.2016.031},
	abstract = {Context: The assessment of Threats to Validity (TTVs) is critical to secure the quality of empirical studies in Software Engineering (SE). In the recent decade, Systematic Literature Review (SLR) was becoming an increasingly important empirical research method in SE. One of the mechanisms of insuring the level of scientific value in the findings of an SLR is to rigorously assess its validity. Hence, it is necessary to realize the status quo and issues of TTVs of SLRs in SE. Objective: This study aims to investigate the-state-of-the-practice of TTVs of the SLRs published in SE, and further support SE researchers to improve the assessment and strategies against TTVs in order to increase the quality of SLRs in SE. Method: We conducted a tertiary study by reviewing the SLRs in SE that report the assessment of TTVs. Results: We identified 316 SLRs published from 2004 to the first half of 2015, in which TTVs are discussed. The issues associated to TTVs were also summarized and categorized. Conclusion: The common TTVs related to SLR research, such as internal validity and reliability, were thoroughly discussed in most SLRs. The threats to construct validity and external validity drew less attention. Moreover, there are few strategies and tactics being reported to cope with the various TTVs.},
	urldate = {2025-01-26},
	booktitle = {2016 23rd {Asia}-{Pacific} {Software} {Engineering} {Conference} ({APSEC})},
	author = {Zhou, Xin and Jin, Yuqin and Zhang, He and Li, Shanshan and Huang, Xin},
	month = dec,
	year = {2016},
	note = {ISSN: 1530-1362},
	keywords = {Bibliographies, Data mining, Evidence-Based Software Engineering, Manuals, Search problems, Software, Software engineering, Systematic (Literature) Review, Systematics, Threats to Validity},
	pages = {153--160},
	file = {IEEE Xplore Abstract Record:/home/atomwalk12/Zotero/storage/6JY8C9HN/7890583.html:text/html},
}

@misc{thakkar_cutlass_2023,
	title = {{CUTLASS}},
	howpublished = {\url{https://github.com/NVIDIA/cutlass}},
	abstract = {CUDA Templates for Linear Algebra Subroutines},
	urldate = {2025-01-25},
	author = {Thakkar, Vijay and Ramani, Pradeep and Cecka, Cris and Shivam, Aniket and Lu, Honghao and Yan, Ethan and Kosaian, Jack and Hoemmen, Mark and Wu, Haicheng and Kerr, Andrew and Nicely, Matt and Merrill, Duane and Blasig, Dustyn and Qiao, Fengqi and Majcher, Piotr and Springer, Paul and Hohnerbach, Markus and Wang, Jin and Gupta, Manish},
	month = jan,
	year = {2023},
	note = {original-date: 2017-11-30T00:11:24Z},
}

@misc{jalali_systematic_2012,
	address = {New York, NY, USA},
	series = {{ESEM} '12},
	title = {Systematic literature studies: database searches vs. backward snowballing},
	isbn = {978-1-4503-1056-7},
	shorttitle = {Systematic literature studies},
	howpublished = {\url{https://doi.org/10.1145/2372251.2372257}},
	doi = {10.1145/2372251.2372257},
	abstract = {Systematic studies of the literature can be done in different ways. In particular, different guidelines propose different first steps in their recommendations, e.g. start with search strings in different databases or start with the reference lists of a starting set of papers.In software engineering, the main recommended first step is using search strings in a number of databases, while in information systems, snowballing has been recommended as the first step. This paper compares the two different search approaches for conducting literature review studies.The comparison is conducted by searching for articles addressing "Agile practices in global software engineering". The focus of the paper is on evaluating the two different search approaches.Despite the differences in the included papers, the conclusions and the patterns found in both studies are quite similar. The strengths and weaknesses of each first step are discussed separately and in comparison with each other.It is concluded that none of the first steps is outperforming the other, and the choice of guideline to follow, and hence the first step, may be context-specific, i.e. depending on the area of study.},
	urldate = {2025-01-25},
	booktitle = {Proceedings of the {ACM}-{IEEE} international symposium on {Empirical} software engineering and measurement},
	publisher = {Association for Computing Machinery},
	author = {Jalali, Samireh and Wohlin, Claes},
	month = sep,
	year = {2012},
	pages = {29--38},
	file = {Full Text:/home/atomwalk12/Zotero/storage/ZTCPMF6X/Jalali and Wohlin - 2012 - Systematic literature studies database searches vs. backward snowballing.pdf:application/pdf},
}
@misc{noauthor_rocmrccl_2025,
	title = {{ROCm}/rccl},
	howpublished = {\url{https://github.com/ROCm/rccl}},
	abstract = {ROCm Communication Collectives Library (RCCL)},
	urldate = {2025-01-24},
	publisher = {AMD ROCm {HIP} 6.3.42133 {Documentation}},
	urldate = {2025-01-24},
	keywords = {rccl},
	key = {rccl},
	file = {What is HIP?  Software},
	month = jan,
	year = {2025},
	note = {original-date: 2016-01-07T17:41:56Z},
	keywords = {hip, cuda, hip-kernel-language, hip-portability, hip-runtime, hipify},
}

@misc{noauthor_cufft_nodate,
	title = {{cuFFT}},
	howpublished = {\url{https://developer.nvidia.com/cufft}},
	abstract = {Benefit from automatic regular performance improvements and new GPU architectures.},
	language = {en-US},
	urldate = {2025-01-24},
	journal = {NVIDIA Developer},
	year = {2007},
	author = {NVIDIA},
	file = {Snapshot:/home/atomwalk12/Zotero/storage/CFHJF2CJ/cufft.html:text/html},
}

@misc{noauthor_rocmrocfft_2025,
	title = {{ROCm}/{rocFFT}},
	key = {rocfft},
	year = {2016},
	keywords = {rocfft},
	howpublished = {\url{https://github.com/ROCm/rocFFT}},
	abstract = {Next generation FFT implementation for ROCm},
	urldate = {2025-01-24},
	publisher = {AMD ROCm Software - GitHub Home},
	urldate = {2025-01-24},
	keywords = {rocfft},
	file = {Full Text:/home/atomwalk12/Zotero/storage/S656RN47/Trott et al. - 2022 - Kokkos 3 Programming Model Extensions for the Exascale Era.pdf:application/pdf},
}


@misc{noauthor_nvidia_nodate,
	title = {{NVIDIA} {TensorRT}},
	howpublished = {\url{https://developer.nvidia.com/tensorrt}},
	abstract = {An SDK with an optimizer for high-performance deep learning inference.},
	language = {en-US},
	urldate = {2025-01-25},
	journal = {NVIDIA Developer},
	author = {NVIDIA},
	year = {2007},
	file = {Snapshot:/home/atomwalk12/Zotero/storage/28SVYN52/tensorrt.html:text/html},
}
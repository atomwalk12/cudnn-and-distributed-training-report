
@misc{chetlur_cudnn_2014,
	title = {{cuDNN}: {Efficient} {Primitives} for {Deep} {Learning}},
	shorttitle = {{cuDNN}},
	howpublished = {http://arxiv.org/abs/1410.0759},
	doi = {10.48550/arXiv.1410.0759},
	abstract = {We present a library of efficient implementations of deep learning primitives. Deep learning workloads are computationally intensive, and optimizing their kernels is difficult and time-consuming. As parallel architectures evolve, kernels must be reoptimized, which makes maintaining codebases difficult over time. Similar issues have long been addressed in the HPC community by libraries such as the Basic Linear Algebra Subroutines (BLAS). However, there is no analogous library for deep learning. Without such a library, researchers implementing deep learning workloads on parallel processors must create and optimize their own implementations of the main computational kernels, and this work must be repeated as new parallel processors emerge. To address this problem, we have created a library similar in intent to BLAS, with optimized routines for deep learning workloads. Our implementation contains routines for GPUs, although similarly to the BLAS library, these routines could be implemented for other platforms. The library is easy to integrate into existing frameworks, and provides optimized performance and memory usage. For example, integrating cuDNN into Caffe, a popular framework for convolutional networks, improves performance by 36\% on a standard model while also reducing memory consumption.},
	urldate = {2024-12-19},
	publisher = {arXiv},
	author = {Chetlur, Sharan and Woolley, Cliff and Vandermersch, Philippe and Cohen, Jonathan and Tran, John and Catanzaro, Bryan and Shelhamer, Evan},
	month = dec,
	year = {2014},
	note = {arXiv:1410.0759 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Mathematical Software, Computer Science - Neural and Evolutionary Computing},
	file = {Preprint PDF:/home/atomwalk12/Zotero/storage/RL3D3MXP/Chetlur et al. - 2014 - cuDNN Efficient Primitives for Deep Learning.pdf:application/pdf;Snapshot:/home/atomwalk12/Zotero/storage/EBSM39I9/1410.html:text/html},
}

@misc{krizhevsky_one_2014,
	title = {One weird trick for parallelizing convolutional neural networks},
	howpublished = {http://arxiv.org/abs/1404.5997},
	doi = {10.48550/arXiv.1404.5997},
	abstract = {I present a new way to parallelize the training of convolutional neural networks across multiple GPUs. The method scales significantly better than all alternatives when applied to modern convolutional neural networks.},
	urldate = {2024-12-19},
	publisher = {arXiv},
	author = {Krizhevsky, Alex},
	month = apr,
	year = {2014},
	note = {arXiv:1404.5997 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {Preprint PDF:/home/atomwalk12/Zotero/storage/ZQGF4UVN/Krizhevsky - 2014 - One weird trick for parallelizing convolutional neural networks.pdf:application/pdf;Snapshot:/home/atomwalk12/Zotero/storage/QUFUCVLP/1404.html:text/html},
}

@misc{jia_caffe_2014,
	title = {Caffe: {Convolutional} {Architecture} for {Fast} {Feature} {Embedding}},
	shorttitle = {Caffe},
	howpublished = {http://arxiv.org/abs/1408.5093},
	doi = {10.48550/arXiv.1408.5093},
	abstract = {Caﬀe provides multimedia scientists and practitioners with a clean and modiﬁable framework for state-of-the-art deep learning algorithms and a collection of reference models. The framework is a BSD-licensed C++ library with Python and MATLAB bindings for training and deploying generalpurpose convolutional neural networks and other deep models eﬃciently on commodity architectures. Caﬀe ﬁts industry and internet-scale media needs by CUDA GPU computation, processing over 40 million images a day on a single K40 or Titan GPU (≈ 2.5 ms per image). By separating model representation from actual implementation, Caﬀe allows experimentation and seamless switching among platforms for ease of development and deployment from prototyping machines to cloud environments.},
	language = {en},
	urldate = {2024-12-27},
	publisher = {arXiv},
	author = {Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff and Karayev, Sergey and Long, Jonathan and Girshick, Ross and Guadarrama, Sergio and Darrell, Trevor},
	month = jun,
	year = {2014},
	note = {arXiv:1408.5093 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Computer Vision and Pattern Recognition},
	file = {PDF:/home/atomwalk12/Zotero/storage/TFCGA4KC/Jia et al. - 2014 - Caffe Convolutional Architecture for Fast Feature Embedding.pdf:application/pdf},
}

@article{paszke_automatic_nodate,
	title = {Automatic differentiation in {PyTorch}},
	abstract = {In this article, we describe an automatic differentiation module of PyTorch — a library designed to enable rapid research on machine learning models. It builds upon a few projects, most notably Lua Torch, Chainer, and HIPS Autograd [4], and provides a high performance environment with easy access to automatic differentiation of models executed on different devices (CPU and GPU). To make prototyping easier, PyTorch does not follow the symbolic approach used in many other deep learning frameworks, but focuses on differentiation of purely imperative programs, with a focus on extensibility and low overhead. Note that this preprint is a draft of certain sections from an upcoming paper covering all PyTorch features.},
	language = {en},
	author = {Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban},
	file = {PDF:/home/atomwalk12/Zotero/storage/NIPIP9DT/Paszke et al. - Automatic differentiation in PyTorch.pdf:application/pdf},
}

@article{verbraeken_survey_2021,
	title = {A {Survey} on {Distributed} {Machine} {Learning}},
	volume = {53},
	issn = {0360-0300, 1557-7341},
	howpublished = {https://dl.acm.org/doi/10.1145/3377454},
	doi = {10.1145/3377454},
	abstract = {The demand for artificial intelligence has grown significantly over the past decade, and this growth has been fueled by advances in machine learning techniques and the ability to leverage hardware acceleration. However, to increase the quality of predictions and render machine learning solutions feasible for more complex applications, a substantial amount of training data is required. Although small machine learning models can be trained with modest amounts of data, the input for training larger models such as neural networks grows exponentially with the number of parameters. Since the demand for processing training data has outpaced the increase in computation power of computing machinery, there is a need for distributing the machine learning workload across multiple machines, and turning the centralized into a distributed system. These distributed systems present new challenges: first and foremost, the efficient parallelization of the training process and the creation of a coherent model. This article provides an extensive overview of the current state-of-the-art in the field by outlining the challenges and opportunities of distributed machine learning over conventional (centralized) machine learning, discussing the techniques used for distributed machine learning, and providing an overview of the systems that are available.},
	language = {en},
	number = {2},
	urldate = {2024-12-27},
	journal = {ACM Computing Surveys},
	author = {Verbraeken, Joost and Wolting, Matthijs and Katzy, Jonathan and Kloppenburg, Jeroen and Verbelen, Tim and Rellermeyer, Jan S.},
	month = mar,
	year = {2021},
	pages = {1--33},
	file = {PDF:/home/atomwalk12/Zotero/storage/6MFPCFG2/Verbraeken et al. - 2021 - A Survey on Distributed Machine Learning.pdf:application/pdf},
}

@article{ben-nun_demystifying_2020,
	title = {Demystifying {Parallel} and {Distributed} {Deep} {Learning}: {An} {In}-depth {Concurrency} {Analysis}},
	volume = {52},
	issn = {0360-0300, 1557-7341},
	shorttitle = {Demystifying {Parallel} and {Distributed} {Deep} {Learning}},
	howpublished = {https://dl.acm.org/doi/10.1145/3320060},
	doi = {10.1145/3320060},
	abstract = {Deep Neural Networks (DNNs) are becoming an important tool in modern computing applications. Accelerating their training is a major challenge and techniques range from distributed algorithms to low-level circuit design. In this survey, we describe the problem from a theoretical perspective, followed by approaches for its parallelization. We present trends in DNN architectures and the resulting implications on parallelization strategies. We then review and model the different types of concurrency in DNNs: from the single operator, through parallelism in network inference and training, to distributed deep learning. We discuss asynchronous stochastic optimization, distributed system architectures, communication schemes, and neural architecture search. Based on those approaches, we extrapolate potential directions for parallelism in deep learning.},
	language = {en},
	number = {4},
	urldate = {2024-12-27},
	journal = {ACM Computing Surveys},
	author = {Ben-Nun, Tal and Hoefler, Torsten},
	month = jul,
	year = {2020},
	pages = {1--43},
	file = {PDF:/home/atomwalk12/Zotero/storage/GBBDSCCF/Ben-Nun and Hoefler - 2020 - Demystifying Parallel and Distributed Deep Learning An In-depth Concurrency Analysis.pdf:application/pdf},
}

@misc{chahal_hitchhikers_2018,
	title = {A {Hitchhiker}'s {Guide} {On} {Distributed} {Training} of {Deep} {Neural} {Networks}},
	howpublished = {http://arxiv.org/abs/1810.11787},
	doi = {10.48550/arXiv.1810.11787},
	abstract = {Deep learning has led to tremendous advancements in the ﬁeld of Artiﬁcial Intelligence. One caveat however is the substantial amount of compute needed to train these deep learning models. Training a benchmark dataset like ImageNet on a single machine with a modern GPU can take upto a week, distributing training on multiple machines has been observed to drastically bring this time down. Recent work has brought down ImageNet training time to a time as low as 4 minutes by using a cluster of 2048 GPUs. This paper surveys the various algorithms and techniques used to distribute training and presents the current state of the art for a modern distributed training framework. More speciﬁcally, we explore the synchronous and asynchronous variants of distributed Stochastic Gradient Descent, various All Reduce gradient aggregation strategies and best practices for obtaining higher throughout and lower latency over a cluster such as mixed precision training, large batch training and gradient compression.},
	language = {en},
	urldate = {2024-12-27},
	publisher = {arXiv},
	author = {Chahal, Karanbir and Grover, Manraj Singh and Dey, Kuntal},
	month = oct,
	year = {2018},
	note = {arXiv:1810.11787 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Distributed, Parallel, and Cluster Computing, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {PDF:/home/atomwalk12/Zotero/storage/7HZX2TRH/Chahal et al. - 2018 - A Hitchhiker's Guide On Distributed Training of Deep Neural Networks.pdf:application/pdf},
}

@misc{nichols_survey_2022,
	title = {A {Survey} and {Empirical} {Evaluation} of {Parallel} {Deep} {Learning} {Frameworks}},
	howpublished = {http://arxiv.org/abs/2111.04949},
	doi = {10.48550/arXiv.2111.04949},
	abstract = {The field of deep learning has witnessed a remarkable shift towards extremely compute- and memory-intensive neural networks. These newer larger models have enabled researchers to advance stateof-the-art tools across a variety of fields. This phenomenon has spurred the development of algorithms for distributed training of neural networks over a larger number of hardware accelerators. In this paper, we discuss and compare current state-of-the-art frameworks for large scale distributed deep learning. First, we survey current practices in distributed learning and identify the different types of parallelism used. Then, we present empirical results comparing their performance on large image and language training tasks. Additionally, we address their statistical efficiency and memory consumption behavior. Based on our results, we discuss algorithmic and implementation portions of each framework which hinder performance.},
	language = {en},
	urldate = {2024-12-27},
	publisher = {arXiv},
	author = {Nichols, Daniel and Singh, Siddharth and Lin, Shu-Huai and Bhatele, Abhinav},
	month = jul,
	year = {2022},
	note = {arXiv:2111.04949 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Artificial Intelligence},
	file = {PDF:/home/atomwalk12/Zotero/storage/JWXILBYW/Nichols et al. - 2022 - A Survey and Empirical Evaluation of Parallel Deep Learning Frameworks.pdf:application/pdf},
}

@incollection{li_deep_2016,
	title = {Deep {Learning} and {Its} {Parallelization}},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	isbn = {978-0-12-805394-2},
	howpublished = {https://linkinghub.elsevier.com/retrieve/pii/B9780128053942000040},
	language = {en},
	urldate = {2024-12-27},
	booktitle = {Big {Data}},
	publisher = {Elsevier},
	author = {Li, X. and Zhang, G. and Li, K. and Zheng, W.},
	year = {2016},
	doi = {10.1016/B978-0-12-805394-2.00004-0},
	pages = {95--118},
	file = {PDF:/home/atomwalk12/Zotero/storage/GX9SFI95/Li et al. - 2016 - Deep Learning and Its Parallelization.pdf:application/pdf},
}

@misc{narayanan_link_2011,
	title = {Link {Prediction} by {De}-anonymization: {How} {We} {Won} the {Kaggle} {Social} {Network} {Challenge}},
	shorttitle = {Link {Prediction} by {De}-anonymization},
	howpublished = {http://arxiv.org/abs/1102.4374},
	doi = {10.48550/arXiv.1102.4374},
	abstract = {This paper describes the winning entry to the IJCNN 2011 Social Network Challenge run by Kaggle.com. The goal of the contest was to promote research on real-world link prediction, and the dataset was a graph obtained by crawling the popular Flickr social photo sharing website, with user identities scrubbed. By de-anonymizing much of the competition test set using our own Flickr crawl, we were able to effectively game the competition. Our attack represents a new application of de-anonymization to gaming machine learning contests, suggesting changes in how future competitions should be run. We introduce a new simulated annealing-based weighted graph matching algorithm for the seeding step of de-anonymization. We also show how to combine de-anonymization with link prediction---the latter is required to achieve good performance on the portion of the test set not de-anonymized---for example by training the predictor on the de-anonymized portion of the test set, and combining probabilistic predictions from de-anonymization and link prediction.},
	urldate = {2024-12-27},
	publisher = {arXiv},
	author = {Narayanan, Arvind and Shi, Elaine and Rubinstein, Benjamin I. P.},
	month = feb,
	year = {2011},
	note = {arXiv:1102.4374 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Cryptography and Security, kaggle},
	file = {Preprint PDF:/home/atomwalk12/Zotero/storage/MSE526D8/Narayanan et al. - 2011 - Link Prediction by De-anonymization How We Won the Kaggle Social Network Challenge.pdf:application/pdf;Snapshot:/home/atomwalk12/Zotero/storage/2SI3XQ5J/1102.html:text/html},
}

@article{barros_e_sa_deep_2024,
	title = {Deep reinforcement learning in real-time strategy games: a systematic literature review},
	volume = {55},
	issn = {1573-7497},
	shorttitle = {Deep reinforcement learning in real-time strategy games},
	howpublished = {https://doi.org/10.1007/s10489-024-06220-4},
	doi = {10.1007/s10489-024-06220-4},
	abstract = {Reinforcement learning is a field of Machine Learning in which agents learn from interacting with the environment. These agents can deal with more complex problems when their decision-making process is combined with deep learning. While deep reinforcement learning can be used in many real-world applications, games often provide a good source of simulation environments for testing such algorithms. Among all game categories, real-time strategy games usually pose a difficult challenge since they have large state and action spaces, partial observation maps, sparse reward, and Multi-Agent problems, where the events occur continuously simultaneously. Thus, this paper provides a systematic literature review of deep reinforcement learning related to real-time strategy games. The main goals of this review are presented as follows: (a) identify the games used in recent works; (b) summarize the architectures and techniques used; (c) identify the simulation environments adopted and (d) understand whether the works focus on micromanagement or macromanagement tasks when dealing with real-time strategy games. The results show that some architectures have achieved better performance overall when handling both micro and macromanagement tasks, and that techniques for reducing the training time and the state space may improve the agents learning. This paper may help to guide future research on developing strategies to build agents for complex scenarios such as those faced in real-time strategy games.},
	language = {en},
	number = {3},
	urldate = {2024-12-30},
	journal = {Applied Intelligence},
	author = {Barros e Sá, Gabriel Caldas and Madeira, Charles Andrye Galvão},
	month = dec,
	year = {2024},
	keywords = {Agent architectures, Artificial Intelligence, Deep reinforcement learning, Real-time strategy games, Systematic literature review},
	pages = {243},
	file = {Full Text PDF:/home/atomwalk12/Zotero/storage/PD2XJWPE/Barros e Sá and Madeira - 2024 - Deep reinforcement learning in real-time strategy games a systematic literature review.pdf:application/pdf},
}

@article{dos_santos_sustainable_2024,
	title = {Sustainable systematic literature reviews},
	volume = {176},
	issn = {0950-5849},
	howpublished = {https://www.sciencedirect.com/science/article/pii/S0950584924001563},
	doi = {10.1016/j.infsof.2024.107551},
	abstract = {Context:
Systematic Literature Reviews (SLR) have been recognized as an important research method for summarizing evidence in Software Engineering (SE). At the same, SLR still presents several problems, such as the high resource consumption (mainly human resources) and lack of effective impact on SE practitioners, although much research has already been done.
Objective:
The main goal of this paper is to explore the concept of sustainability in the SLR area, intending to contribute to understanding better and solving such problems in an integrated way. More specifically, this paper characterizes what sustainable SLR are, their core characteristics, critical factors (i.e., sensitive points in the SLR process), and guidelines for conducting such SLR.
Methods:
We performed a meta-ethnographic study to find key concepts of sustainable software systems and transpose them to sustainable SLR. For this, we systematically selected 16 studies about sustainable software systems and 14 distinguished studies about SLR. Following, we extracted the main keywords and metaphors, determined how both areas are correlated, and transposed them to obtain a set of core characteristics of sustainable SLR as well as critical factors and guidelines. Additionally, we validated them with specialists using the Delphi method.
Results:
We found 15 core characteristics that offer a broad view of sustainable SLR, 15 critical factors in the SLR process that should be carefully addressed when conducting and updating SLR, and also 16 guidelines to manage SLR from the sustainability perspective.
Conclusion:
The concept of sustainability in SLR can contribute to solving SLR problems in a more integrated way, while this work could change the mindset of the SLR community about the need to conduct sustainable SLR.},
	urldate = {2024-12-31},
	journal = {Information and Software Technology},
	author = {dos Santos, Vinicius and Iwazaki, Anderson Y. and Felizardo, Katia R. and de Souza, Érica F. and Nakagawa, Elisa Y.},
	month = dec,
	year = {2024},
	keywords = {Systematic literature review, Secondary study, SLR, Sustainability},
	pages = {107551},
	file = {ScienceDirect Snapshot:/home/atomwalk12/Zotero/storage/MCEA5L9Z/S0950584924001563.html:text/html},
}

@article{bolanos_artificial_2024,
	title = {Artificial intelligence for literature reviews: opportunities and challenges},
	volume = {57},
	issn = {1573-7462},
	shorttitle = {Artificial intelligence for literature reviews},
	howpublished = {https://doi.org/10.1007/s10462-024-10902-3},
	doi = {10.1007/s10462-024-10902-3},
	abstract = {This paper presents a comprehensive review of the use of Artificial Intelligence (AI) in Systematic Literature Reviews (SLRs). A SLR is a rigorous and organised methodology that assesses and integrates prior research on a given topic. Numerous tools have been developed to assist and partially automate the SLR process. The increasing role of AI in this field shows great potential in providing more effective support for researchers, moving towards the semi-automatic creation of literature reviews. Our study focuses on how AI techniques are applied in the semi-automation of SLRs, specifically in the screening and extraction phases. We examine 21 leading SLR tools using a framework that combines 23 traditional features with 11 AI features. We also analyse 11 recent tools that leverage large language models for searching the literature and assisting academic writing. Finally, the paper discusses current trends in the field, outlines key research challenges, and suggests directions for future research. We highlight three primary research challenges: integrating advanced AI solutions, such as large language models and knowledge graphs, improving usability, and developing a standardised evaluation framework. We also propose best practices to ensure more robust evaluations in terms of performance, usability, and transparency. Overall, this review offers a detailed overview of AI-enhanced SLR tools for researchers and practitioners, providing a foundation for the development of next-generation AI solutions in this field.},
	language = {en},
	number = {10},
	urldate = {2025-01-05},
	journal = {Artificial Intelligence Review},
	author = {Bolaños, Francisco and Salatino, Angelo and Osborne, Francesco and Motta, Enrico},
	month = aug,
	year = {2024},
	keywords = {Artificial Intelligence, Artificial intelligence, Evaluation framework, Large language models, Literature review, Natural anguage processing, Systematic literature reviews, Usability},
	pages = {259},
	file = {Full Text PDF:/home/atomwalk12/Zotero/storage/9XU7TFA7/Bolaños et al. - 2024 - Artificial intelligence for literature reviews opportunities and challenges.pdf:application/pdf},
}

@article{bolanos_artificial_2024-1,
	title = {Artificial intelligence for literature reviews: opportunities and challenges},
	volume = {57},
	issn = {1573-7462},
	shorttitle = {Artificial intelligence for literature reviews},
	howpublished = {https://doi.org/10.1007/s10462-024-10902-3},
	doi = {10.1007/s10462-024-10902-3},
	abstract = {This paper presents a comprehensive review of the use of Artificial Intelligence (AI) in Systematic Literature Reviews (SLRs). A SLR is a rigorous and organised methodology that assesses and integrates prior research on a given topic. Numerous tools have been developed to assist and partially automate the SLR process. The increasing role of AI in this field shows great potential in providing more effective support for researchers, moving towards the semi-automatic creation of literature reviews. Our study focuses on how AI techniques are applied in the semi-automation of SLRs, specifically in the screening and extraction phases. We examine 21 leading SLR tools using a framework that combines 23 traditional features with 11 AI features. We also analyse 11 recent tools that leverage large language models for searching the literature and assisting academic writing. Finally, the paper discusses current trends in the field, outlines key research challenges, and suggests directions for future research. We highlight three primary research challenges: integrating advanced AI solutions, such as large language models and knowledge graphs, improving usability, and developing a standardised evaluation framework. We also propose best practices to ensure more robust evaluations in terms of performance, usability, and transparency. Overall, this review offers a detailed overview of AI-enhanced SLR tools for researchers and practitioners, providing a foundation for the development of next-generation AI solutions in this field.},
	language = {en},
	number = {10},
	urldate = {2025-01-05},
	journal = {Artificial Intelligence Review},
	author = {Bolaños, Francisco and Salatino, Angelo and Osborne, Francesco and Motta, Enrico},
	month = aug,
	year = {2024},
	keywords = {Artificial Intelligence, Artificial intelligence, Evaluation framework, Large language models, Literature review, Natural anguage processing, Systematic literature reviews, Usability},
	pages = {259},
	file = {Full Text PDF:/home/atomwalk12/Zotero/storage/HN75PDD9/Bolaños et al. - 2024 - Artificial intelligence for literature reviews opportunities and challenges.pdf:application/pdf},
}

@article{kitchenham_procedures_nodate,
	title = {Procedures for {Performing} {Systematic} {Regviews}},
	language = {en},
	journal = {NICTA},
	year = {2004},
	author = {Kitchenham, Barbara},
	keywords = {literature-review},
	file = {PDF:/home/atomwalk12/Zotero/storage/8SUS6QW9/Kitchenham - Procedures for Performing Systematic Reviews.pdf:application/pdf},
}

@article{budgen_reporting_2018,
	title = {Reporting systematic reviews: {Some} lessons from a tertiary study},
	volume = {95},
	issn = {0950-5849},
	shorttitle = {Reporting systematic reviews},
	howpublished = {https://www.sciencedirect.com/science/article/pii/S0950584916303548},
	doi = {10.1016/j.infsof.2017.10.017},
	abstract = {Context
Many of the systematic reviews published in software engineering are related to research or methodological issues and hence are unlikely to be of direct benefit to practitioners or teachers. Those that are relevant to practice and teaching need to be presented in a form that makes their findings usable with minimum interpretation.
Objective
We have examined a sample of the many systematic reviews that have been published over a period of six years, in order to assess how well these are reported and identify useful lessons about how this might be done.
Method
We undertook a tertiary study, performing a systematic review of systematic reviews. Our study found 178 systematic reviews published in a set of major software engineering journals over the period 2010–2015. Of these, 37 provided recommendations or conclusions of relevance to education and/or practice and we used the DARE criteria as well as other attributes related to the systematic review process to analyse how well they were reported.
Results
We have derived a set of 12 ‘lessons’ that could help authors with reporting the outcomes of a systematic review in software engineering. We also provide an associated checklist for use by journal and conference referees.
Conclusion
There are several areas where better reporting is needed, including quality assessment, synthesis, and the procedures followed by the reviewers. Researchers, practitioners, teachers and journal referees would all benefit from better reporting of systematic reviews, both for clarity and also for establishing the provenance of any findings.},
	urldate = {2025-01-05},
	journal = {Information and Software Technology},
	author = {Budgen, David and Brereton, Pearl and Drummond, Sarah and Williams, Nikki},
	month = mar,
	year = {2018},
	keywords = {Provenance of findings, Reporting quality, Systematic review},
	pages = {62--74},
	file = {Accepted Version:/home/atomwalk12/Zotero/storage/SET4XTE6/Budgen et al. - 2018 - Reporting systematic reviews Some lessons from a tertiary study.pdf:application/pdf;ScienceDirect Snapshot:/home/atomwalk12/Zotero/storage/LAL2AWVY/S0950584916303548.html:text/html},
}

@article{brereton_lessons_2007,
	series = {Software {Performance}},
	title = {Lessons from applying the systematic literature review process within the software engineering domain},
	volume = {80},
	issn = {0164-1212},
	howpublished = {https://www.sciencedirect.com/science/article/pii/S016412120600197X},
	doi = {10.1016/j.jss.2006.07.009},
	abstract = {A consequence of the growing number of empirical studies in software engineering is the need to adopt systematic approaches to assessing and aggregating research outcomes in order to provide a balanced and objective summary of research evidence for a particular topic. The paper reports experiences with applying one such approach, the practice of systematic literature review, to the published studies relevant to topics within the software engineering domain. The systematic literature review process is summarised, a number of reviews being undertaken by the authors and others are described and some lessons about the applicability of this practice to software engineering are extracted. The basic systematic literature review process seems appropriate to software engineering and the preparation and validation of a review protocol in advance of a review activity is especially valuable. The paper highlights areas where some adaptation of the process to accommodate the domain-specific characteristics of software engineering is needed as well as areas where improvements to current software engineering infrastructure and practices would enhance its applicability. In particular, infrastructure support provided by software engineering indexing databases is inadequate. Also, the quality of abstracts is poor; it is usually not possible to judge the relevance of a study from a review of the abstract alone.},
	number = {4},
	urldate = {2025-01-05},
	journal = {Journal of Systems and Software},
	author = {Brereton, Pearl and Kitchenham, Barbara A. and Budgen, David and Turner, Mark and Khalil, Mohamed},
	month = apr,
	year = {2007},
	keywords = {Systematic literature review, Empirical software engineering},
	pages = {571--583},
	file = {ScienceDirect Snapshot:/home/atomwalk12/Zotero/storage/V2L67PHV/S016412120600197X.html:text/html},
}

@article{brereton_lessons_2007-1,
	title = {Lessons from applying the systematic literature review process within the software engineering domain},
	volume = {80},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {01641212},
	howpublished = {https://linkinghub.elsevier.com/retrieve/pii/S016412120600197X},
	doi = {10.1016/j.jss.2006.07.009},
	abstract = {A consequence of the growing number of empirical studies in software engineering is the need to adopt systematic approaches to assessing and aggregating research outcomes in order to provide a balanced and objective summary of research evidence for a particular topic. The paper reports experiences with applying one such approach, the practice of systematic literature review, to the published studies relevant to topics within the software engineering domain. The systematic literature review process is summarised, a number of reviews being undertaken by the authors and others are described and some lessons about the applicability of this practice to software engineering are extracted.},
	language = {en},
	number = {4},
	urldate = {2025-01-05},
	journal = {Journal of Systems and Software},
	author = {Brereton, Pearl and Kitchenham, Barbara A. and Budgen, David and Turner, Mark and Khalil, Mohamed},
	month = apr,
	year = {2007},
	pages = {571--583},
	file = {PDF:/home/atomwalk12/Zotero/storage/9U8H4DYF/Brereton et al. - 2007 - Lessons from applying the systematic literature review process within the software engineering domai.pdf:application/pdf},
}

@article{Howard2016SWIFTReviewAT,
  title={SWIFT-Review: a text-mining workbench for systematic review},
  author={Brian E. Howard and Jason R. Phillips and Kyle Miller and Arpit Tandon and Deepak Mav and Mihir R. Shah and Stephanie D Holmgren and Katherine E Pelch and Vickie R. Walker and Andrew A. Rooney and Malcolm Robert Macleod and Ruchir R. Shah and Kristina Thayer},
  journal={Systematic Reviews},
  year={2016},
  volume={5},
  url={https://api.semanticscholar.org/CorpusID:5970071}
}


@misc{noauthor_gemini_nodate,
	title = { Chat to supercharge your ideas},
	url = {https://gemini.google.com},
	author = {Google},
	year = {2023},
	abstract = {Bard is now Gemini. Get help with writing, planning, learning and more from Google AI.},
	language = {en-GB},
	urldate = {2025-01-23},
	journal = {Gemini},
	file = {Snapshot:/home/atomwalk12/Zotero/storage/TMJ32DPN/app.html:text/html},
}

@misc{notebooklm_google_2024,
	title = {Google {NotebookLM} {\textbar} {Note} {Taking} \& {Research} {Assistant} {Powered} by {AI}},
	url = {https://notebooklm.google/},
	abstract = {Use the power of AI for quick summarization and note taking, NotebookLM is your powerful virtual research assistant rooted in information you can trust.},
	language = {en-GB},
	urldate = {2025-01-23},
	author = {Google},
	year = {2023},
	file = {Snapshot:/home/atomwalk12/Zotero/storage/II36H7PV/notebooklm.google.html:text/html},
}


@misc{team_gemini_2024,
	title = {Gemini: {A} {Family} of {Highly} {Capable} {Multimodal} {Models}},
	shorttitle = {Gemini},
	url = {http://arxiv.org/abs/2312.11805},
	doi = {10.48550/arXiv.2312.11805},
	abstract = {This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of the Gemini family in cross-modal reasoning and language understanding will enable a wide variety of use cases. We discuss our approach toward post-training and deploying Gemini models responsibly to users through services including Gemini, Gemini Advanced, Google AI Studio, and Cloud Vertex AI.},
	urldate = {2025-01-23},
	publisher = {arXiv},
	author = {{Gemini Authors}},
	month = jun,
	year = {2024},
	note = {arXiv:2312.11805 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:/home/atomwalk12/Zotero/storage/G8FL5XFH/Team et al. - 2024 - Gemini A Family of Highly Capable Multimodal Models.pdf:application/pdf;Snapshot:/home/atomwalk12/Zotero/storage/4RHAQN3Z/2312.html:text/html},
}

@article{dos_santos_sustainable_2024-1,
	title = {Sustainable systematic literature reviews},
	volume = {176},
	issn = {09505849},
	howpublished = {https://linkinghub.elsevier.com/retrieve/pii/S0950584924001563},
	doi = {10.1016/j.infsof.2024.107551},
	abstract = {Objective: The main goal of this paper is to explore the concept of sustainability in the SLR area, intending to contribute to understanding better and solving such problems in an integrated way. More specifically, this paper characterizes what sustainable SLR are, their core characteristics, critical factors (i.e., sensitive points in the SLR process), and guidelines for conducting such SLR.
Methods: We performed a meta-ethnographic study to find key concepts of sustainable software systems and transpose them to sustainable SLR. For this, we systematically selected 16 studies about sustainable software systems and 14 distinguished studies about SLR. Following, we extracted the main keywords and metaphors, determined how both areas are correlated, and transposed them to obtain a set of core characteristics of sustainable SLR as well as critical factors and guidelines. Additionally, we validated them with specialists using the Delphi method.
Results: We found 15 core characteristics that offer a broad view of sustainable SLR, 15 critical factors in the SLR process that should be carefully addressed when conducting and updating SLR, and also 16 guidelines to manage SLR from the sustainability perspective.
Conclusion: The concept of sustainability in SLR can contribute to solving SLR problems in a more integrated way, while this work could change the mindset of the SLR community about the need to conduct sustainable SLR.},
	language = {en},
	urldate = {2025-01-05},
	journal = {Information and Software Technology},
	author = {Dos Santos, Vinicius and Iwazaki, Anderson Y. and Felizardo, Katia R. and De Souza, Érica F. and Nakagawa, Elisa Y.},
	month = dec,
	year = {2024},
	pages = {107551},
	file = {PDF:/home/atomwalk12/Zotero/storage/H3SLL832/Dos Santos et al. - 2024 - Sustainable systematic literature reviews.pdf:application/pdf},
}

@article{dehghani_distributed_2023,
	title = {From distributed machine to distributed deep learning: a comprehensive survey},
	volume = {10},
	issn = {2196-1115},
	shorttitle = {From distributed machine to distributed deep learning},
	howpublished = {https://doi.org/10.1186/s40537-023-00829-x},
	doi = {10.1186/s40537-023-00829-x},
	abstract = {Artificial intelligence has made remarkable progress in handling complex tasks, thanks to advances in hardware acceleration and machine learning algorithms. However, to acquire more accurate outcomes and solve more complex issues, algorithms should be trained with more data. Processing this huge amount of data could be time-consuming and require a great deal of computation. To address these issues, distributed machine learning has been proposed, which involves distributing the data and algorithm across several machines. There has been considerable effort put into developing distributed machine learning algorithms, and different methods have been proposed so far. We divide these algorithms in classification and clustering (traditional machine learning), deep learning and deep reinforcement learning groups. Distributed deep learning has gained more attention in recent years and most of the studies have focused on this approach. Therefore, we mostly concentrate on this category. Based on the investigation of the mentioned algorithms, we highlighted the limitations that should be addressed in future research.},
	language = {en},
	number = {1},
	urldate = {2025-01-05},
	journal = {Journal of Big Data},
	author = {Dehghani, Mohammad and Yazdanparast, Zahra},
	month = oct,
	year = {2023},
	keywords = {Artificial Intelligence, Artificial intelligence, Data-parallelism, Distributed deep learning, Distributed machine learning, Ditributed reinforcement learning, Machine learning, Model-parallelism},
	pages = {158},
	file = {Full Text PDF:/home/atomwalk12/Zotero/storage/FQPFJL85/Dehghani and Yazdanparast - 2023 - From distributed machine to distributed deep learning a comprehensive survey.pdf:application/pdf},
}

@inproceedings{berloco_systematic_2022,
	address = {Cham},
	title = {A {Systematic} {Review} of {Distributed} {Deep} {Learning} {Frameworks} for {Big} {Data}},
	isbn = {978-3-031-13832-4},
	doi = {10.1007/978-3-031-13832-4_21},
	abstract = {Traditional Machine Learning and Deep Learning techniques (data acquisition, preparation, model training and evaluation) take a lot of computational resources and time to produce even a simple prediction model, especially when implemented on a single machine. Intuitively, the demand for computational requirements is higher in case of management of Big Data and training of complex models. Thus, a paradigm shift from a single machine to a BD-oriented approach is required for making traditional Machine Learning and Deep Learning techniques fit to Big Data. In particular, it emerges the need for developing and deploying Big Data Analytics Infrastructures on cluster of machines. In this context, main features and principles of Distributed Deep Learning frameworks are here discussed. The main contribution of this paper is a systematic review of proposed solutions, aimed at investigating under a unifying lens their foundational elements, functional features and capabilities, despite the inherent literature fragmentation. To this, we conducted a literature search in Scopus and Google Scholar. This review also compares Distributed Deep Learning approaches according to more technical facets: implemented of parallelism techniques, supported hardware, model parameters sharing modalities, computation modalities for stochastic gradient descent and compatibility with other frameworks.},
	language = {en},
	booktitle = {Intelligent {Computing} {Methodologies}},
	publisher = {Springer International Publishing},
	author = {Berloco, Francesco and Bevilacqua, Vitoantonio and Colucci, Simona},
	editor = {Huang, De-Shuang and Jo, Kang-Hyun and Jing, Junfeng and Premaratne, Prashan and Bevilacqua, Vitoantonio and Hussain, Abir},
	year = {2022},
	keywords = {Big Data, Distributed Deep Learning, Distributed Deep Learning Frameworks, Parallel computing},
	pages = {242--256},
	file = {Full Text PDF:/home/atomwalk12/Zotero/storage/PJJTIG7E/Berloco et al. - 2022 - A Systematic Review of Distributed Deep Learning Frameworks for Big Data.pdf:application/pdf},
}

@article{langer_distributed_2020,
	title = {Distributed {Training} of {Deep} {Learning} {Models}: {A} {Taxonomic} {Perspective}},
	volume = {31},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {1045-9219, 1558-2183, 2161-9883},
	shorttitle = {Distributed {Training} of {Deep} {Learning} {Models}},
	howpublished = {https://ieeexplore.ieee.org/document/9120226/},
	doi = {10.1109/TPDS.2020.3003307},
	abstract = {Distributed deep learning systems (DDLS) train deep neural network models by utilizing the distributed resources of a cluster. Developers of DDLS are required to make many decisions to process their particular workloads in their chosen environment efficiently. The advent of GPU-based deep learning, the ever-increasing size of datasets, and deep neural network models, in combination with the bandwidth constraints that exist in cluster environments require developers of DDLS to be innovative in order to train high-quality models quickly. Comparing DDLS side-by-side is difficult due to their extensive feature lists and architectural deviations. We aim to shine some light on the fundamental principles that are at work when training deep neural networks in a cluster of independent machines by analyzing the general properties associated with training deep learning models and how such workloads can be distributed in a cluster to achieve collaborative model training. Thereby we provide an overview of the different techniques that are used by contemporary DDLS and discuss their influence and implications on the training process. To conceptualize and compare DDLS, we group different techniques into categories, thus establishing a taxonomy of distributed deep learning systems.},
	number = {12},
	urldate = {2025-01-05},
	journal = {IEEE Transactions on Parallel and Distributed Systems},
	author = {Langer, Matthias and He, Zhen and Rahayu, Wenny and Xue, Yanbo},
	month = dec,
	year = {2020},
	pages = {2802--2818},
	file = {Submitted Version:/home/atomwalk12/Zotero/storage/4649MWFD/Langer et al. - 2020 - Distributed Training of Deep Learning Models A Taxonomic Perspective.pdf:application/pdf},
}

@misc{nichols_survey_2022-1,
	title = {A {Survey} and {Empirical} {Evaluation} of {Parallel} {Deep} {Learning} {Frameworks}},
	howpublished = {http://arxiv.org/abs/2111.04949},
	doi = {10.48550/arXiv.2111.04949},
	abstract = {The field of deep learning has witnessed a remarkable shift towards extremely compute- and memory-intensive neural networks. These newer larger models have enabled researchers to advance state-of-the-art tools across a variety of fields. This phenomenon has spurred the development of algorithms for distributed training of neural networks over a larger number of hardware accelerators. In this paper, we discuss and compare current state-of-the-art frameworks for large scale distributed deep learning. First, we survey current practices in distributed learning and identify the different types of parallelism used. Then, we present empirical results comparing their performance on large image and language training tasks. Additionally, we address their statistical efficiency and memory consumption behavior. Based on our results, we discuss algorithmic and implementation portions of each framework which hinder performance.},
	urldate = {2025-01-05},
	publisher = {arXiv},
	author = {Nichols, Daniel and Singh, Siddharth and Lin, Shu-Huai and Bhatele, Abhinav},
	month = jul,
	year = {2022},
	note = {arXiv:2111.04949 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:/home/atomwalk12/Zotero/storage/U3PUQJSE/Nichols et al. - 2022 - A Survey and Empirical Evaluation of Parallel Deep Learning Frameworks.pdf:application/pdf;Snapshot:/home/atomwalk12/Zotero/storage/XYGZ2AQZ/2111.html:text/html},
}

@misc{xing_strategies_2015,
	title = {Strategies and {Principles} of {Distributed} {Machine} {Learning} on {Big} {Data}},
	howpublished = {http://arxiv.org/abs/1512.09295},
	doi = {10.48550/arXiv.1512.09295},
	abstract = {The rise of Big Data has led to new demands for Machine Learning (ML) systems to learn complex models with millions to billions of parameters, that promise adequate capacity to digest massive datasets and offer powerful predictive analytics thereupon. In order to run ML algorithms at such scales, on a distributed cluster with 10s to 1000s of machines, it is often the case that significant engineering efforts are required --- and one might fairly ask if such engineering truly falls within the domain of ML research or not. Taking the view that Big ML systems can benefit greatly from ML-rooted statistical and algorithmic insights --- and that ML researchers should therefore not shy away from such systems design --- we discuss a series of principles and strategies distilled from our recent efforts on industrial-scale ML solutions. These principles and strategies span a continuum from application, to engineering, and to theoretical research and development of Big ML systems and architectures, with the goal of understanding how to make them efficient, generally-applicable, and supported with convergence and scaling guarantees. They concern four key questions which traditionally receive little attention in ML research: How to distribute an ML program over a cluster? How to bridge ML computation with inter-machine communication? How to perform such communication? What should be communicated between machines? By exposing underlying statistical and algorithmic characteristics unique to ML programs but not typically seen in traditional computer programs, and by dissecting successful cases to reveal how we have harnessed these principles to design and develop both high-performance distributed ML software as well as general-purpose ML frameworks, we present opportunities for ML researchers and practitioners to further shape and grow the area that lies between ML and systems.},
	urldate = {2025-01-05},
	publisher = {arXiv},
	author = {Xing, Eric P. and Ho, Qirong and Xie, Pengtao and Dai, Wei},
	month = dec,
	year = {2015},
	note = {arXiv:1512.09295 [stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Distributed, Parallel, and Cluster Computing, Statistics - Machine Learning},
	file = {Preprint PDF:/home/atomwalk12/Zotero/storage/RHYCBQMX/Xing et al. - 2015 - Strategies and Principles of Distributed Machine Learning on Big Data.pdf:application/pdf;Snapshot:/home/atomwalk12/Zotero/storage/GWFZVF5U/1512.html:text/html},
}

@article{vasile_survey_nodate,
	title = {Survey on {Distributed} {Techniques} applied to {Neural} {Networks}},
	language = {en},
	author = {Vasile, Razvan Florian},
	file = {PDF:/home/atomwalk12/Zotero/storage/HE8WRHGK/Vasile - Survey on Distributed Techniques applied to Neural Networks.pdf:application/pdf},
}

}

@article{brereton_lessons_2007,
	series = {Software {Performance}},
	title = {Lessons from applying the systematic literature review process within the software engineering domain},
	volume = {80},
	issn = {0164-1212},
	howpublished = {https://www.sciencedirect.com/science/article/pii/S016412120600197X},
	doi = {10.1016/j.jss.2006.07.009},
	abstract = {A consequence of the growing number of empirical studies in software engineering is the need to adopt systematic approaches to assessing and aggregating research outcomes in order to provide a balanced and objective summary of research evidence for a particular topic. The paper reports experiences with applying one such approach, the practice of systematic literature review, to the published studies relevant to topics within the software engineering domain. The systematic literature review process is summarised, a number of reviews being undertaken by the authors and others are described and some lessons about the applicability of this practice to software engineering are extracted. The basic systematic literature review process seems appropriate to software engineering and the preparation and validation of a review protocol in advance of a review activity is especially valuable. The paper highlights areas where some adaptation of the process to accommodate the domain-specific characteristics of software engineering is needed as well as areas where improvements to current software engineering infrastructure and practices would enhance its applicability. In particular, infrastructure support provided by software engineering indexing databases is inadequate. Also, the quality of abstracts is poor; it is usually not possible to judge the relevance of a study from a review of the abstract alone.},
	number = {4},
	urldate = {2025-01-05},
	journal = {Journal of Systems and Software},
	author = {Brereton, Pearl and Kitchenham, Barbara A. and Budgen, David and Turner, Mark and Khalil, Mohamed},
	month = apr,
	year = {2007},
	keywords = {Systematic literature review, Empirical software engineering},
	pages = {571--583},
	file = {ScienceDirect Snapshot:/home/atomwalk12/Zotero/storage/V2L67PHV/S016412120600197X.html:text/html},
}

% Two papers from the project proposal below
@article{SierraCanto2010ParallelTO,
  title={Parallel Training of a Back-Propagation Neural Network Using CUDA},
  author={Xavier Sierra-Canto and Francisco Madera-Ramirez and V{\'i}ctor Uc Cetina},
  journal={2010 Ninth International Conference on Machine Learning and Applications},
  year={2010},
  pages={307-312},
  url={https://api.semanticscholar.org/CorpusID:53717101}
}

@misc{li2020pytorchdistributedexperiencesaccelerating,
      title={PyTorch Distributed: Experiences on Accelerating Data Parallel Training}, 
      author={Shen Li and Yanli Zhao and Rohan Varma and Omkar Salpekar and Pieter Noordhuis and Teng Li and Adam Paszke and Jeff Smith and Brian Vaughan and Pritam Damania and Soumith Chintala},
      year={2020},
      eprint={2006.15704},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      url={https://arxiv.org/abs/2006.15704}, 
}

@misc{keele_systematic_2007,
	author = {Keele University},
	title = {Guidelines for performing Systematic Literature Reviews in Software Engineering},
	howpublished = {\url{https://legacyfileshare.elsevier.com/promis_misc/525444systematicreviewsguide.pdf}},
	year = {2007},
	note = {[Accessed 13-01-2025]},
}


@inproceedings{krizhevsky_imagenet_2012,
	title = {{ImageNet} {Classification} with {Deep} {Convolutional} {Neural} {Networks}},
	volume = {25},
	howpublished = {https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html},
	abstract = {We trained a large, deep convolutional neural network to classify the 1.3 million high-resolution images in the LSVRC-2010 ImageNet training set into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 39.7{\textbackslash}\% and 18.9{\textbackslash}\% which is considerably better than the previous state-of-the-art results. The neural network, which has 60 million parameters and 500,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and two globally connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of convolutional nets. To reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective.},
	urldate = {2025-01-16},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	year = {2012},
	file = {Full Text PDF:/home/atomwalk12/Zotero/storage/M6RC4IZF/Krizhevsky et al. - 2012 - ImageNet Classification with Deep Convolutional Neural Networks.pdf:application/pdf},
}


@misc{kaplan_scaling_2020,
	title = {Scaling {Laws} for {Neural} {Language} {Models}},
	howpublished = {http://arxiv.org/abs/2001.08361},
	doi = {10.48550/arXiv.2001.08361},
	abstract = {We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.},
	urldate = {2025-01-17},
	publisher = {arXiv},
	author = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
	month = jan,
	year = {2020},
	note = {arXiv:2001.08361 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:/home/atomwalk12/Zotero/storage/QS7HU2T9/Kaplan et al. - 2020 - Scaling Laws for Neural Language Models.pdf:application/pdf;Snapshot:/home/atomwalk12/Zotero/storage/PF85BBSG/2001.html:text/html},
}


@book{kitchenham_evidence-based_2015,
	title = {Evidence-{Based} {Software} {Engineering} and {Systematic} {Reviews}},
	isbn = {978-1-4822-2866-3},
	abstract = {In the decade since the idea of adapting the evidence-based paradigm for software engineering was first proposed, it has become a major tool of empirical software engineering. Evidence-Based Software Engineering and Systematic Reviews provides a clear introduction to the use of an evidence-based model for software engineering research and practice.},
	language = {en},
	publisher = {CRC Press},
	author = {Kitchenham, Barbara Ann and Budgen, David and Brereton, Pearl},
	month = nov,
	year = {2015},
	note = {Google-Books-ID: bGfdCgAAQBAJ},
	keywords = {Computers / Computer Science, Computers / General, Computers / Programming / Games, Computers / Software Development \& Engineering / General, Mathematics / General},
}


@misc{huang_gpipe_2019,
	title = {{GPipe}: {Efficient} {Training} of {Giant} {Neural} {Networks} using {Pipeline} {Parallelism}},
	shorttitle = {{GPipe}},
	howpublished = {http://arxiv.org/abs/1811.06965},
	doi = {10.48550/arXiv.1811.06965},
	abstract = {Scaling up deep neural network capacity has been known as an effective approach to improving model quality for several different machine learning tasks. In many cases, increasing model capacity beyond the memory limit of a single accelerator has required developing special algorithms or infrastructure. These solutions are often architecture-specific and do not transfer to other tasks. To address the need for efficient and task-independent model parallelism, we introduce GPipe, a pipeline parallelism library that allows scaling any network that can be expressed as a sequence of layers. By pipelining different sub-sequences of layers on separate accelerators, GPipe provides the flexibility of scaling a variety of different networks to gigantic sizes efficiently. Moreover, GPipe utilizes a novel batch-splitting pipelining algorithm, resulting in almost linear speedup when a model is partitioned across multiple accelerators. We demonstrate the advantages of GPipe by training large-scale neural networks on two different tasks with distinct network architectures: (i) Image Classification: We train a 557-million-parameter AmoebaNet model and attain a top-1 accuracy of 84.4\% on ImageNet-2012, (ii) Multilingual Neural Machine Translation: We train a single 6-billion-parameter, 128-layer Transformer model on a corpus spanning over 100 languages and achieve better quality than all bilingual models.},
	urldate = {2025-01-21},
	publisher = {arXiv},
	author = {Huang, Yanping and Cheng, Youlong and Bapna, Ankur and Firat, Orhan and Chen, Mia Xu and Chen, Dehao and Lee, HyoukJoong and Ngiam, Jiquan and Le, Quoc V. and Wu, Yonghui and Chen, Zhifeng},
	month = jul,
	year = {2019},
	note = {arXiv:1811.06965 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 11 pages. Work in progress. Copyright 2018 by the authors},
	annote = {Reference Semantic Scholar: [TLDR] GPipe is introduced, a pipeline parallelism library that allows scaling any network that can be expressed as a sequence of layers by pipelining different sub-sequences of layers on separate accelerators, resulting in almost linear speedup when a model is partitioned across multiple accelerators.
},
	file = {Preprint PDF:/home/atomwalk12/Zotero/storage/VMSKDDS4/Huang et al. - 2019 - GPipe Efficient Training of Giant Neural Networks using Pipeline Parallelism.pdf:application/pdf;Snapshot:/home/atomwalk12/Zotero/storage/BABZQA3T/1811.html:text/html},
}

@misc{noauthor_tensorflowlingvo_2025,
	title = {tensorflow/lingvo},
	copyright = {Apache-2.0},
	howpublished = {https://github.com/tensorflow/lingvo},
	abstract = {Lingvo},
	urldate = {2025-01-21},
	publisher = {tensorflow},
	month = jan,
	year = {2025},
	note = {original-date: 2018-07-24T22:30:28Z},
	keywords = {asr, distributed, distributed-systems-ddl, distributed-systems-ddl-selected, distributed-systems-ddl-selected-selected, gpu-computing, language-model, lm, machine-translation, mnist, nlp, research, seq2seq, speech, speech-recognition, speech-synthesis, speech-to-text, tensorflow, translation, tts},
	annote = {GPipe},
	key = {GPipe},
}


@article{chen_mxnet_2015,
	title = {{MXNet}: {A} {Flexible} and {Efficient} {Machine} {Learning} {Library} for {Heterogeneous} {Distributed} {Systems}},
	shorttitle = {{MXNet}},
	howpublished = {https://www.semanticscholar.org/paper/MXNet%3A-A-Flexible-and-Efficient-Machine-Learning-Chen-Li/62df84d6a4d26f95e4714796c2337c9848cc13b5},
	abstract = {MXNet is a multi-language machine learning (ML) library to ease the development of ML algorithms, especially for deep neural networks. Embedded in the host language, it blends declarative symbolic expression with imperative tensor computation. It offers auto differentiation to derive gradients. MXNet is computation and memory efficient and runs on various heterogeneous systems, ranging from mobile devices to distributed GPU clusters. 
This paper describes both the API design and the system implementation of MXNet, and explains how embedding of both symbolic expression and tensor operation is handled in a unified fashion. Our preliminary experiments reveal promising results on large scale deep neural network applications using multiple GPU machines.},
	urldate = {2025-01-21},
	journal = {ArXiv},
	author = {Chen, Tianqi and Li, Mu and Li, Yutian and Lin, Min and Wang, Naiyan and Wang, Minjie and Xiao, Tianjun and Xu, Bing and Zhang, Chiyuan and Zhang, Zheng},
	month = dec,
	year = {2015},
	annote = {[TLDR] The API design and the system implementation of MXNet are described, and it is explained how embedding of both symbolic expression and tensor operation is handled in a unified fashion.},
	file = {Full Text PDF:/home/atomwalk12/Zotero/storage/3DEFVTEC/Chen et al. - 2015 - MXNet A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems.pdf:application/pdf},
}

@misc{noauthor_apachemxnet_2025,
	title = {apache/mxnet},
	copyright = {Apache-2.0},
	howpublished = {https://github.com/apache/mxnet},
	abstract = {Lightweight, Portable, Flexible Distributed/Mobile Deep Learning with Dynamic, Mutation-aware Dataflow Dep Scheduler; for Python, R, Julia, Scala, Go, Javascript and more},
	urldate = {2025-01-21},
	publisher = {The Apache Software Foundation},
	month = jan,
	year = {2025},
	note = {original-date: 2015-04-30T16:21:15Z},
	keywords = {mxnet},
	key = {MXNet},
}


@article{li_pytorch_2020,
	title = {{PyTorch} {Distributed}: {Experiences} on {Accelerating} {Data} {Parallel} {Training}},
	volume = {13},
	howpublished = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135409181&doi=10.14778%2f3415478.3415530&partnerID=40&md5=db77eb2107830a0063027223af8c38fe},
	doi = {10.14778/3415478.3415530},
	abstract = {This paper presents the design, implementation, and evaluation of the PyTorch distributed data parallel module. PyTorch is a widely-adopted scientific computing package used in deep learning research and applications. Recent advances in deep learning argue for the value of large datasets and large models, which necessitates the ability to scale out model training to more computational resources. Data parallelism has emerged as a popular solution for distributed training thanks to its straightforward principle and broad applicability. In general, the technique of distributed data parallelism replicates the model on every computational resource to generate gradients independently and then communicates those gradients at each iteration to keep model replicas consistent. Despite the conceptual simplicity of the technique, the subtle dependencies between computation and communication make it non-trivial to optimize the distributed training efficiency. As of v1.5, PyTorch natively provides several techniques to accelerate distributed data parallel, including bucketing gradients, overlapping computation with communication, and skipping gradient synchronization. Evaluations show that, when configured appropriately, the PyTorch distributed data parallel module attains near-linear scalability using 256 GPUs. © VLDB Endowment. All rights reserved.},
	number = {12},
	journal = {Proceedings of the VLDB Endowment},
	author = {Li, S. and Zhao, Y. and Varma, R. and Salpekar, O. and Noordhuis, P. and Li, T. and Paszke, A. and Smith, J. and Vaughan, B. and Damania, P. and Chintala, S.},
	year = {2020},
	keywords = {Computational resources, Data parallel, Data parallelism, Deep learning, Design evaluation, Design implementation, Distributed data, distributed-systems-ddl, distributed-systems-ddl-pytorch, distributed-systems-ddl-selected, distributed-systems-ddl-selected-selected, Iterative methods, Large dataset, Large datasets, Parallel modules, Parallel training, Program processors, Research and application},
	pages = {3005--3018},
	annote = {Export Date: 20 January 2025; Cited By: 175},
	annote = {Reference 5: This paper presents the design, implementation, and evaluation of the PyTorch distributed data parallel module. Data parallelism has emerged as a popular solution for distributed training thanks to its straightforward principle and broad applicability. In general, the technique of distributed data parallelism replicates the model on every computational resource to generate gradients independently and then communicates those gradients at each iteration to keep model replicas consistent. As of v1.5, PyTorch natively provides several techniques to accelerate distributed data parallel, including bucketing gradients, overlapping computation with communication, and skipping gradient synchronization. Evaluations show that, when configured appropriately, the PyTorch distributed data parallel module attains near-linear scalability using 256 GPUs.
},
	file = {PDF:/home/atomwalk12/Zotero/storage/3JJNHNXU/Li et al. - 2020 - PyTorch Distributed Experiences on Accelerating Data Parallel Training.pdf:application/pdf},
}

@misc{noauthor_pytorchpytorch_nodate,
	year = {2025},
	title = {pytorch/pytorch: {Tensors} and {Dynamic} neural networks in {Python} with strong {GPU} acceleration},
	howpublished = {https://github.com/pytorch/pytorch},
	urldate = {2025-01-21},
	keywords = {distributed-systems-ddl-selected, distributed-systems-ddl-selected-selected},
	key = {PyTorch},
}


@article{xie_optimal_2022,
	title = {Optimal distributed parallel algorithms for deep learning framework {Tensorflow}},
	volume = {52},
	howpublished = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111778509&doi=10.1007%2fs10489-021-02588-9&partnerID=40&md5=4b83017d376799bb5bb630f933db0ba3},
	doi = {10.1007/s10489-021-02588-9},
	abstract = {Since its release, the Tensorflow framework has been widely used in various fields due to its advantages in deep learning. However, it is still at its early state. Its native distributed implementation has difficulty in expanding for large models because it has issues of low utilization of multiple GPUs and slow distribution compared with running on single machine. It is of great significance to reduce the training time through parallel models. In view of this, we firstly provided an in-depth analysis of the implementation principle of Tensorflow and identify the bottlenecks of its native distributed parallel models to improve. Then, two optimal algorithms are designed and implemented based on data parallelism and model parallelism modes of Tensorflow. For data parallelism, the proposed algorithm is implemented to replace the native linear execution mode with pipeline execution mode. As for model parallelism, the native random partitioning mode is replaced by our proposed novel greedy algorithm. Finally, we built a homogeneous distributed cluster and a heterogeneous distributed cluster respectively to verify the effectiveness of the proposed algorithms. Through a number of comparative experiments, we showed that the proposed optimal parallel algorithms can effectively reduce model training time by an average of 26.5\%(or average 1.5x speedup than native distributed algorithms) and improve the utilization of the cluster while keeping the same accuracy level of native Tensorflow. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.},
	number = {4},
	journal = {Applied Intelligence},
	author = {Xie, Y. and He, M. and Ma, T. and Tian, W.},
	year = {2022},
	keywords = {Deep learning, Data parallelism, Clustering algorithms, Learning algorithms, Model parallelism, Training time, Program processors, Learning frameworks, Distributed clusters, Distributed parallel algorithm, Optimal distributed parallel algorithm, Optimal distributed parallel algorithms, Parallel algorithms, Parallel models, Tensorflow, distributed-systems-ddl, distributed-systems-ddl-tensorflow, distributed-systems-ddl-selected},
	pages = {3880--3900},
	annote = {Export Date: 20 January 2025; Cited By: 10},
	annote = {Reference 2: Its native distributed implementation has difficulty in expanding for large models because it has issues of low utilization of multiple GPUs and slow distribution compared with running on single machine. We firstly provided an in-depth analysis of the implementation principle of Tensorflow and identify the bottlenecks of its native distributed parallel models to improve. Two optimal algorithms are designed and implemented based on data parallelism and model parallelism modes of Tensorflow. For data parallelism, the proposed algorithm is implemented to replace the native linear execution mode with pipeline execution mode. As for model parallelism, the native random partitioning mode is replaced by our proposed novel greedy algorithm. Finally, we built a homogeneous distributed cluster and a heterogeneous distributed cluster respectively to verify the effectiveness of the proposed algorithms. Through a number of comparative experiments, we showed that the proposed optimal parallel algorithms can effectively reduce model training time by an average of 26.5\%(or average 1.5x speedup than native distributed algorithms) and improve the utilization of the cluster while keeping the same accuracy level of native Tensorflow.
},
	file = {PDF:/home/atomwalk12/Zotero/storage/RPUTUDCB/Xie et al. - 2022 - Optimal distributed parallel algorithms for deep learning framework Tensorflow.pdf:application/pdf},
}

@misc{abadi_tensorflow_2015,
	title = {{TensorFlow}, {Large}-scale machine learning on heterogeneous systems},
	copyright = {Apache-2.0},
	howpublished = {https://github.com/tensorflow/tensorflow},
	abstract = {An Open Source Machine Learning Framework for Everyone},
	urldate = {2025-01-21},
	author = {Abadi, Martín and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S. and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jozefowicz, Rafal and Jia, Yangqing and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Mané, Dan and Schuster, Mike and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Viégas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
	month = nov,
	year = {2015},
	doi = {10.5281/zenodo.4724125},
	keywords = {distributed-systems-ddl, distributed-systems-ddl-selected, distributed-systems-ddl-selected-selected},
}

@article{rasley_deepspeed_2020,
	title = {{DeepSpeed}: {System} {Optimizations} {Enable} {Training} {Deep} {Learning} {Models} with {Over} 100 {Billion} {Parameters}},
	shorttitle = {{DeepSpeed}},
	howpublished = {https://dl.acm.org/doi/10.1145/3394486.3406703},
	doi = {10.1145/3394486.3406703},
	abstract = {Explore new techniques in Microsoft's open source library called DeepSpeed, which advances large model training by improving scale, speed, cost, and usability, unlocking the ability to train 100-billion-parameter models. DeepSpeed is compatible with PyTorch. One piece of our library, called ZeRO, is a new parallelized optimizer that greatly reduces the resources needed for model and data parallelism while massively increasing the number of parameters that can be trained. Researchers have used these breakthroughs to create Turing Natural Language Generation (Turing-NLG), which at the time of its release was the largest publicly known language model at 17 billion parameters. In addition we will also go over our latest transformer kernel advancements that led the DeepSpeed team to achieve the world fastest BERT pretraining record. The Zero Redundancy Optimizer (ZeRO) is a novel memory optimization technology for large-scale distributed deep learning. ZeRO can train deep learning models with over 100 billion parameters on the current generation of GPU clusters at three to five times the throughput of the current best system. It also presents a clear path to training models with trillions of parameters, demonstrating an unprecedented leap in deep learning system technology. DeepSpeed brings state-of-the-art training techniques, such as ZeRO, optimized kernels, distributed training, mixed precision, and checkpointing, through lightweight APIs compatible with PyTorch. With just a few lines of code changes to your PyTorch model, you can leverage DeepSpeed to address underlying performance challenges and boost the speed and scale of your training.},
	language = {en},
	urldate = {2025-01-21},
	journal = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
	author = {Rasley, Jeff and Rajbhandari, Samyam and Ruwase, Olatunji and He, Yuxiong},
	month = aug,
	year = {2020},
	note = {Conference Name: KDD '20: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining
ISBN: 9781450379984
Place: Virtual Event CA USA
Publisher: ACM},
	keywords = {distributed-systems-ddl, distributed-systems-ddl-deepspeed, distributed-systems-ddl-selected, distributed-systems-ddl-selected-selected},
	pages = {3505--3506},
	annote = {[TLDR] New techniques in Microsoft's open source library called DeepSpeed, which advances large model training by improving scale, speed, cost, and usability, unlocking the ability to train 100-billion-parameter models, are explored.},
	file = {PDF:/home/atomwalk12/Zotero/storage/9DRCQC3S/Rasley et al. - 2020 - DeepSpeed System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameter.pdf:application/pdf},
}

@misc{noauthor_microsoftdeepspeed_2025,
	author = {Microsoft},
	title = {microsoft/{DeepSpeed}},
	copyright = {Apache-2.0},
	howpublished = {https://github.com/microsoft/DeepSpeed},
	abstract = {DeepSpeed is a deep learning optimization library that makes distributed training and inference easy, efficient, and effective.},
	urldate = {2025-01-21},
	publisher = {Microsoft},
	month = jan,
	year = {2025},
	note = {original-date: 2020-01-23T18:35:18Z},
	keywords = {billion-parameters, compression, data-parallelism, deep-learning, distributed-systems-ddl, distributed-systems-ddl-deepspeed, distributed-systems-ddl-selected, distributed-systems-ddl-selected-selected, gpu, inference, machine-learning, mixture-of-experts, model-parallelism, pipeline-parallelism, pytorch, trillion-parameters, zero},
}


@misc{sergeev_horovod_2018,
	title = {Horovod: fast and easy distributed deep learning in {TensorFlow}},
	shorttitle = {Horovod},
	howpublished = {http://arxiv.org/abs/1802.05799},
	doi = {10.48550/arXiv.1802.05799},
	abstract = {Training modern deep learning models requires large amounts of computation, often provided by GPUs. Scaling computation from one GPU to many can enable much faster training and research progress but entails two complications. First, the training library must support inter-GPU communication. Depending on the particular methods employed, this communication may entail anywhere from negligible to significant overhead. Second, the user must modify his or her training code to take advantage of inter-GPU communication. Depending on the training library's API, the modification required may be either significant or minimal. Existing methods for enabling multi-GPU training under the TensorFlow library entail non-negligible communication overhead and require users to heavily modify their model-building code, leading many researchers to avoid the whole mess and stick with slower single-GPU training. In this paper we introduce Horovod, an open source library that improves on both obstructions to scaling: it employs efficient inter-GPU communication via ring reduction and requires only a few lines of modification to user code, enabling faster, easier distributed training in TensorFlow. Horovod is available under the Apache 2.0 license at https://github.com/uber/horovod},
	urldate = {2025-01-21},
	publisher = {arXiv},
	author = {Sergeev, Alexander and Balso, Mike Del},
	month = feb,
	year = {2018},
	note = {arXiv:1802.05799 [cs]},
	keywords = {Computer Science - Machine Learning, distributed-systems-ddl, distributed-systems-ddl-selected-selected, distributed-systems-ddl-snowballed, Statistics - Machine Learning},
	file = {Preprint PDF:/home/atomwalk12/Zotero/storage/UP4SSA4G/Sergeev and Balso - 2018 - Horovod fast and easy distributed deep learning in TensorFlow.pdf:application/pdf;Snapshot:/home/atomwalk12/Zotero/storage/G96PZGXD/1802.html:text/html},
}

@misc{noauthor_horovodhorovod_2025,
	author = {Uber},
	title = {horovod/horovod},
	howpublished = {https://github.com/horovod/horovod},
	abstract = {Distributed training framework for TensorFlow, Keras, PyTorch, and Apache MXNet.},
	urldate = {2025-01-21},
	publisher = {Horovod},
	month = jan,
	year = {2025},
	note = {original-date: 2017-08-09T19:39:59Z},
	keywords = {baidu, deep-learning, deeplearning, distributed-systems-ddl, distributed-systems-ddl-selected-selected, distributed-systems-ddl-snowballed, keras, machine-learning, machinelearning, mpi, mxnet, pytorch, ray, spark, tensorflow, uber},
}


@article{jiang_unified_nodate,
	journal = {Proceedings of the 14th USENIX Symposium on Operating Systems Design and Implementation},
	year = {2020},
	title = {A {Unified} {Architecture} for {Accelerating} {Distributed} {DNN} {Training} in {Heterogeneous} {GPU}/{CPU} {Clusters}},
	abstract = {Data center clusters that run DNN training jobs are inherently heterogeneous. They have GPUs and CPUs for computation and network bandwidth for distributed training. However, existing distributed DNN training architectures, all-reduce and Parameter Server (PS), cannot fully utilize such heterogeneous resources. In this paper, we present a new distributed DNN training architecture called BytePS. BytePS can leverage spare CPU and bandwidth resources in the cluster to accelerate distributed DNN training tasks running on GPUs. It provides a communication framework that is both proved optimal and uniﬁed – existing all-reduce and PS become two special cases of BytePS. To achieve the proved optimality in practice, BytePS further splits the functionalities of a parameter optimizer. It introduces a Summation Service abstraction for aggregating gradients, which is common for all the optimizers. Summation Service can be accelerated by AVX instructions and can be efﬁciently run on CPUs, while DNN model-related optimizer algorithms are run on GPUs for computation acceleration. BytePS can accelerate DNN training for major frameworks including TensorFlow, PyTorch and MXNet. For representative DNN training jobs with up to 256 GPUs, BytePS outperforms the state-of-the-art open source all-reduce and PS by up to 84\% and 245\%, respectively.},
	language = {en},
	author = {Jiang, Yimin and Lan, Chang and Yi, Bairen and Cui, Yong and Guo, Chuanxiong},
	keywords = {distributed-systems-ddl, distributed-systems-ddl-selected-selected},
	file = {PDF:/home/atomwalk12/Zotero/storage/QE7UACZ4/Jiang et al. - A Unified Architecture for Accelerating Distributed DNN Training in Heterogeneous GPUCPU Clusters.pdf:application/pdf},
}

@misc{noauthor_bytedancebyteps_2025,
	author = {{ByteDance}},
	title = {bytedance/byteps},
	howpublished = {https://github.com/bytedance/byteps},
	abstract = {A high performance and generic framework for distributed DNN training},
	urldate = {2025-01-22},
	publisher = {Bytedance Inc.},
	month = jan,
	year = {2025},
	note = {original-date: 2019-06-25T07:00:13Z},
	keywords = {deep-learning, distributed-systems-ddl, distributed-systems-ddl-byteps, distributed-systems-ddl-selected-selected, distributed-training, keras, machine-learning, mxnet, pytorch, tensorflow},
}


@misc{abadi_tensorflow_2016,
	title = {{TensorFlow}: {Large}-{Scale} {Machine} {Learning} on {Heterogeneous} {Distributed} {Systems}},
	shorttitle = {{TensorFlow}},
	howpublished = {http://arxiv.org/abs/1603.04467},
	doi = {10.48550/arXiv.1603.04467},
	abstract = {TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.},
	urldate = {2025-01-22},
	publisher = {arXiv},
	author = {Abadi, Martín and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S. and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Mane, Dan and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Viegas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
	month = mar,
	year = {2016},
	note = {arXiv:1603.04467 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning, distributed-systems-ddl, distributed-systems-ddl-selected-selected, distributed-systems-ddl-snowballed},
	annote = {Comment: Version 2 updates only the metadata, to correct the formatting of Mart{\textbackslash}'in Abadi's name},
	file = {Preprint PDF:/home/atomwalk12/Zotero/storage/5BA54RAE/Abadi et al. - 2016 - TensorFlow Large-Scale Machine Learning on Heterogeneous Distributed Systems.pdf:application/pdf;Snapshot:/home/atomwalk12/Zotero/storage/W9VMLV5B/1603.html:text/html},
}


@misc{noauthor_jax-mljax_2025,
	author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James Johnson and Chris Leary and Dougal Maclaurin and George Necula and Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao Zhang},
	title = {jax-ml/jax},
	copyright = {Apache-2.0},
	url = {https://github.com/jax-ml/jax},
	abstract = {Composable transformations of Python+NumPy programs: differentiate, vectorize, JIT to GPU/TPU, and more},
	urldate = {2025-01-22},
	publisher = {jax-ml},
	month = jan,
	year = {2025},
	note = {original-date: 2018-10-25T21:25:02Z},
	keywords = {distributed-systems-ddl, distributed-systems-ddl-selected-selected, distributed-systems-ddl-snowballed, jax},
}

@article{frostig_compiling_nodate,
	journal = {arXiv},
	year = {2018},
	title = {Compiling machine learning programs via high-level tracing},
	abstract = {We describe JAX, a domain-specific tracing JIT compiler for generating high-performance accelerator code from pure Python and Numpy machine learning programs. JAX uses the XLA compiler infrastructure to generate optimized code for the program subroutines that are most favorable for acceleration, and these optimized subroutines can be called and orchestrated by arbitrary Python. Because the system is fully compatible with Autograd, it allows forward- and reverse-mode automatic differentiation of Python functions to arbitrary order. Because JAX supports structured control flow, it can generate code for sophisticated machine learning algorithms while maintaining high performance. We show that by combining JAX with Autograd and Numpy we get an easily programmable and highly performant ML system that targets CPUs, GPUs, and TPUs, capable of scaling to multi-core Cloud TPUs.},
	language = {en},
	author = {Frostig, Roy and Johnson, Matthew James and Leary, Chris},
	keywords = {distributed-systems-ddl, distributed-systems-ddl-selected-selected, distributed-systems-ddl-snowballed},
	file = {PDF:/home/atomwalk12/Zotero/storage/3TJUWJHJ/Frostig et al. - Compiling machine learning programs via high-level tracing.pdf:application/pdf},
}


@misc{li_colossal-ai_2023,
	title = {Colossal-{AI}: {A} {Unified} {Deep} {Learning} {System} {For} {Large}-{Scale} {Parallel} {Training}},
	shorttitle = {Colossal-{AI}},
	url = {http://arxiv.org/abs/2110.14883},
	doi = {10.48550/arXiv.2110.14883},
	abstract = {The success of Transformer models has pushed the deep learning model scale to billions of parameters. Due to the limited memory resource of a single GPU, However, the best practice for choosing the optimal parallel strategy is still lacking, since it requires domain expertise in both deep learning and parallel computing. The Colossal-AI system addressed the above challenge by introducing a unified interface to scale your sequential code of model training to distributed environments. It supports parallel training methods such as data, pipeline, tensor, and sequence parallelism, as well as heterogeneous training methods integrated with zero redundancy optimizer. Compared to the baseline system, Colossal-AI can achieve up to 2.76 times training speedup on large-scale models.},
	urldate = {2025-01-22},
	publisher = {arXiv},
	author = {Li, Shenggui and Liu, Hongxin and Bian, Zhengda and Fang, Jiarui and Huang, Haichen and Liu, Yuliang and Wang, Boxiang and You, Yang},
	month = oct,
	year = {2023},
	note = {arXiv:2110.14883 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning},
	file = {Preprint PDF:/home/atomwalk12/Zotero/storage/JVITUQ25/Li et al. - 2023 - Colossal-AI A Unified Deep Learning System For Large-Scale Parallel Training.pdf:application/pdf;Snapshot:/home/atomwalk12/Zotero/storage/5URDZC59/2110.html:text/html},
}

@misc{noauthor_hpcaitechcolossalai_2025,
	title = {hpcaitech/{ColossalAI}},
	copyright = {Apache-2.0},
	url = {https://github.com/hpcaitech/ColossalAI},
	abstract = {Making large AI models cheaper, faster and more accessible},
	urldate = {2025-01-22},
	author = {HPC-AI Tech},
	publisher = {HPC-AI Tech},
	month = jan,
	year = {2025},
	note = {original-date: 2021-10-28T16:19:44Z},
	keywords = {ai, big-model, data-parallelism, deep-learning, distributed-computing, foundation-models, heterogeneous-training, hpc, inference, large-scale, model-parallelism, pipeline-parallelism},
}

@misc{wolf_huggingfaces_2020,
	title = {{HuggingFace}'s {Transformers}: {State}-of-the-art {Natural} {Language} {Processing}},
	shorttitle = {{HuggingFace}'s {Transformers}},
	url = {http://arxiv.org/abs/1910.03771},
	doi = {10.48550/arXiv.1910.03771},
	abstract = {Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. {\textbackslash}textit\{Transformers\} is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. {\textbackslash}textit\{Transformers\} is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at {\textbackslash}url\{https://github.com/huggingface/transformers\}.},
	urldate = {2025-01-22},
	publisher = {arXiv},
	author = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, Rémi and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and Platen, Patrick von and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Scao, Teven Le and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander M.},
	month = jul,
	year = {2020},
	note = {arXiv:1910.03771 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 8 pages, 4 figures, more details at https://github.com/huggingface/transformers},
	file = {Preprint PDF:/home/atomwalk12/Zotero/storage/LLD3DIDW/Wolf et al. - 2020 - HuggingFace's Transformers State-of-the-art Natural Language Processing.pdf:application/pdf;Snapshot:/home/atomwalk12/Zotero/storage/WN99XIQ3/1910.html:text/html},
}

@misc{noauthor_huggingfaceaccelerate_2025,
	title = {huggingface/accelerate},
	copyright = {Apache-2.0},
	url = {https://github.com/huggingface/accelerate},
	abstract = {🚀 A simple way to launch, train, and use PyTorch models on almost any device and distributed configuration, automatic mixed precision (including fp8), and easy-to-configure FSDP and DeepSpeed support},
	urldate = {2025-01-22},
	author = {Hugging Face},
	publisher = {Hugging Face},
	month = jan,
	year = {2025},
	note = {original-date: 2020-10-30T13:27:12Z},
}

@misc{falcon_pytorch_2019,
	title = {{PyTorch} {Lightning}},
	copyright = {Apache-2.0},
	url = {https://github.com/Lightning-AI/lightning},
	abstract = {Pretrain, finetune ANY AI model of ANY size on multiple GPUs, TPUs with zero code changes.},
	urldate = {2025-01-22},
	author = {Falcon, William and {The PyTorch Lightning team}},
	month = mar,
	year = {2019},
	doi = {10.5281/zenodo.3828935},
}

@misc{shoeybi_megatron-lm_2020,
	title = {Megatron-{LM}: {Training} {Multi}-{Billion} {Parameter} {Language} {Models} {Using} {Model} {Parallelism}},
	shorttitle = {Megatron-{LM}},
	url = {http://arxiv.org/abs/1909.08053},
	doi = {10.48550/arXiv.1909.08053},
	abstract = {Recent work in language modeling demonstrates that training large transformer models advances the state of the art in Natural Language Processing applications. However, very large models can be quite difficult to train due to memory constraints. In this work, we present our techniques for training very large transformer models and implement a simple, efficient intra-layer model parallel approach that enables training transformer models with billions of parameters. Our approach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model parallelism, and can be fully implemented with the insertion of a few communication operations in native PyTorch. We illustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 GPUs. We sustain 15.1 PetaFLOPs across the entire application with 76\% scaling efficiency when compared to a strong single GPU baseline that sustains 39 TeraFLOPs, which is 30\% of peak FLOPs. To demonstrate that large language models can further advance the state of the art (SOTA), we train an 8.3 billion parameter transformer language model similar to GPT-2 and a 3.9 billion parameter model similar to BERT. We show that careful attention to the placement of layer normalization in BERT-like models is critical to achieving increased performance as the model size grows. Using the GPT-2 model we achieve SOTA results on the WikiText103 (10.8 compared to SOTA perplexity of 15.8) and LAMBADA (66.5\% compared to SOTA accuracy of 63.2\%) datasets. Our BERT model achieves SOTA results on the RACE dataset (90.9\% compared to SOTA accuracy of 89.4\%).},
	urldate = {2025-01-22},
	publisher = {arXiv},
	author = {Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
	month = mar,
	year = {2020},
	note = {arXiv:1909.08053 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/home/atomwalk12/Zotero/storage/T3BLVW33/Shoeybi et al. - 2020 - Megatron-LM Training Multi-Billion Parameter Language Models Using Model Parallelism.pdf:application/pdf;Snapshot:/home/atomwalk12/Zotero/storage/WNHHGISL/1909.html:text/html},
}

@misc{noauthor_nvidiamegatron-lm_2025,
	title = {{NVIDIA}/{Megatron}-{LM}},
	url = {https://github.com/NVIDIA/Megatron-LM},
	abstract = {Ongoing research training transformer models at scale},
	urldate = {2025-01-22},
	publisher = {NVIDIA Corporation},
	month = jan,
	year = {2025},
	note = {original-date: 2019-03-21T16:15:52Z},
	keywords = {large-language-models, model-para, transformers},
	author = {NVIDIA Corporation},
}


@Misc{FairScale2021,
  author =       {{FairScale authors}},
  title =        {FairScale:  A general purpose modular PyTorch library for high performance and large scale training},
  howpublished = {\url{https://github.com/facebookresearch/fairscale}},
  year =         {2021}
}

@misc{karakus_amazon_2021,
	title = {Amazon {SageMaker} {Model} {Parallelism}: {A} {General} and {Flexible} {Framework} for {Large} {Model} {Training}},
	shorttitle = {Amazon {SageMaker} {Model} {Parallelism}},
	url = {http://arxiv.org/abs/2111.05972},
	doi = {10.48550/arXiv.2111.05972},
	abstract = {With deep learning models rapidly growing in size, systems-level solutions for large-model training are required. We present Amazon SageMaker model parallelism, a software library that integrates with PyTorch, and enables easy training of large models using model parallelism and other memory-saving features. In contrast to existing solutions, the implementation of the SageMaker library is much more generic and flexible, in that it can automatically partition and run pipeline parallelism over arbitrary model architectures with minimal code change, and also offers a general and extensible framework for tensor parallelism, which supports a wider range of use cases, and is modular enough to be easily applied to new training scripts. The library also preserves the native PyTorch user experience to a much larger degree, supporting module re-use and dynamic graphs, while giving the user full control over the details of the training step. We evaluate performance over GPT-3, RoBERTa, BERT, and neural collaborative filtering, and demonstrate competitive performance over existing solutions.},
	urldate = {2025-01-22},
	publisher = {arXiv},
	author = {Karakus, Can and Huilgol, Rahul and Wu, Fei and Subramanian, Anirudh and Daniel, Cade and Cavdar, Derya and Xu, Teng and Chen, Haohan and Rahnama, Arash and Quintela, Luis},
	month = nov,
	year = {2021},
	note = {arXiv:2111.05972 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning},
	annote = {Comment: 24 pages. Submitted for review},
	file = {Preprint PDF:/home/atomwalk12/Zotero/storage/JXINKX8T/Karakus et al. - 2021 - Amazon SageMaker Model Parallelism A General and Flexible Framework for Large Model Training.pdf:application/pdf;Snapshot:/home/atomwalk12/Zotero/storage/6G8FQV5L/2111.html:text/html},
}

@misc{noauthor_awsamazon-sagemaker-examples_2025,
	title = {aws/amazon-sagemaker-examples},
	copyright = {Apache-2.0},
	url = {https://github.com/aws/amazon-sagemaker-examples},
	abstract = {Example 📓 Jupyter notebooks that demonstrate how to build, train, and deploy machine learning models using 🧠 Amazon SageMaker.},
	urldate = {2025-01-22},
	publisher = {Amazon Web Services},
	month = jan,
	year = {2025},
	author = {Amazon Web Services},
	note = {original-date: 2017-10-23T05:55:22Z},
	keywords = {aws, data-science, deep-learning, examples, inference, jupyter-notebook, machine-learning, mlops, reinforcement-learning, sagemaker, training},
}

@misc{noauthor_azureazureml-examples_2025,
	title = {Azure/azureml-examples},
	copyright = {MIT},
	url = {https://github.com/Azure/azureml-examples},
	abstract = {Official community-driven Azure Machine Learning examples, tested with GitHub Actions.},
	urldate = {2025-01-22},
	publisher = {Microsoft Azure},
	author = {Microsoft Azure},
	month = jan,
	year = {2025},
	note = {original-date: 2020-08-21T18:04:26Z},
	keywords = {azure, azure-machine-learning, azureml, data-science, ml},
}

@misc{noauthor_googlecloudplatformgenerative-ai_2025,
	title = {{GoogleCloudPlatform}/generative-ai},
	copyright = {Apache-2.0},
	url = {https://github.com/GoogleCloudPlatform/generative-ai},
	abstract = {Sample code and notebooks for Generative AI on Google Cloud, with Gemini on Vertex AI},
	urldate = {2025-01-22},
	publisher = {Google Cloud Platform},
	month = jan,
	year = {2025},
	note = {original-date: 2023-05-05T12:31:07Z},
	keywords = {gemini, gemini-api, generative-ai, google, google-cloud, google-gemini, langchain, llm, palm-api, vertex-ai, vertex-ai-gemini-api, vertexai},
}

@misc{lepikhin_gshard_2020,
	title = {{GShard}: {Scaling} {Giant} {Models} with {Conditional} {Computation} and {Automatic} {Sharding}},
	shorttitle = {{GShard}},
	url = {http://arxiv.org/abs/2006.16668},
	doi = {10.48550/arXiv.2006.16668},
	abstract = {Neural network scaling has been critical for improving the model quality in many real-world machine learning applications with vast amounts of training data and compute. Although this trend of scaling is affirmed to be a sure-fire approach for better model quality, there are challenges on the path such as the computation cost, ease of programming, and efficient implementation on parallel devices. GShard is a module composed of a set of lightweight annotation APIs and an extension to the XLA compiler. It provides an elegant way to express a wide range of parallel computation patterns with minimal changes to the existing model code. GShard enabled us to scale up multilingual neural machine translation Transformer model with Sparsely-Gated Mixture-of-Experts beyond 600 billion parameters using automatic sharding. We demonstrate that such a giant model can efficiently be trained on 2048 TPU v3 accelerators in 4 days to achieve far superior quality for translation from 100 languages to English compared to the prior art.},
	urldate = {2025-01-22},
	publisher = {arXiv},
	author = {Lepikhin, Dmitry and Lee, HyoukJoong and Xu, Yuanzhong and Chen, Dehao and Firat, Orhan and Huang, Yanping and Krikun, Maxim and Shazeer, Noam and Chen, Zhifeng},
	month = jun,
	year = {2020},
	note = {arXiv:2006.16668 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:/home/atomwalk12/Zotero/storage/NUIVKAZX/Lepikhin et al. - 2020 - GShard Scaling Giant Models with Conditional Computation and Automatic Sharding.pdf:application/pdf;Snapshot:/home/atomwalk12/Zotero/storage/RJKL5CGM/2006.html:text/html},
}

@misc{moritz_ray_2018,
	title = {Ray: {A} {Distributed} {Framework} for {Emerging} {AI} {Applications}},
	shorttitle = {Ray},
	url = {http://arxiv.org/abs/1712.05889},
	doi = {10.48550/arXiv.1712.05889},
	abstract = {The next generation of AI applications will continuously interact with the environment and learn from these interactions. These applications impose new and demanding systems requirements, both in terms of performance and flexibility. In this paper, we consider these requirements and present Ray---a distributed system to address them. Ray implements a unified interface that can express both task-parallel and actor-based computations, supported by a single dynamic execution engine. To meet the performance requirements, Ray employs a distributed scheduler and a distributed and fault-tolerant store to manage the system's control state. In our experiments, we demonstrate scaling beyond 1.8 million tasks per second and better performance than existing specialized systems for several challenging reinforcement learning applications.},
	urldate = {2025-01-22},
	publisher = {arXiv},
	author = {Moritz, Philipp and Nishihara, Robert and Wang, Stephanie and Tumanov, Alexey and Liaw, Richard and Liang, Eric and Elibol, Melih and Yang, Zongheng and Paul, William and Jordan, Michael I. and Stoica, Ion},
	month = sep,
	year = {2018},
	note = {arXiv:1712.05889 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: 17 pages, 14 figures, 13th USENIX Symposium on Operating Systems Design and Implementation, 2018},
	file = {Preprint PDF:/home/atomwalk12/Zotero/storage/789QX42R/Moritz et al. - 2018 - Ray A Distributed Framework for Emerging AI Applications.pdf:application/pdf;Snapshot:/home/atomwalk12/Zotero/storage/Z6L76EKF/1712.html:text/html},
}

@misc{noauthor_ray-projectray_2025,
	title = {ray-project/ray},
	copyright = {Apache-2.0},
	url = {https://github.com/ray-project/ray},
	abstract = {Ray is an AI compute engine. Ray consists of a core distributed runtime and a set of AI Libraries for accelerating ML workloads.},
	urldate = {2025-01-22},
	publisher = {ray-project},
	author = {ray-project},
	month = jan,
	year = {2025},
	note = {original-date: 2016-10-25T19:38:30Z},
	keywords = {automl, data-science, deep-learning, deployment, distributed, hyperparameter-optimization, hyperparameter-search, java, llm-serving, machine-learning, model-selection, optimization, parallel, python, pytorch, ray, reinforcement-learning, rllib, serving, tensorflow},
}

@misc{sdgilley_azure_nodate,
	title = {Azure {Machine} {Learning} documentation},
	url = {https://learn.microsoft.com/en-us/azure/machine-learning/?view=azureml-api-2},
	abstract = {Learn how to train and deploy models and manage the ML lifecycle (MLOps) with Azure Machine Learning. Tutorials, code examples, API references, and more.},
	language = {en-us},
	year = {2021},
	urldate = {2025-01-22},
	author = {sdgilley},
	file = {Snapshot:/home/atomwalk12/Zotero/storage/KIQW3TQ2/machine-learning.html:text/html},
}


@misc{noauthor_amazon_nodate,
	title = {Amazon {SageMaker} {Documentation}},
	url = {https://docs.aws.amazon.com/sagemaker/},
	author = {Amazon Web Services},
	year = {2021},
	file = {Amazon SageMaker Documentation:/home/atomwalk12/Zotero/storage/VWW6G38L/sagemaker.html:text/html},
}

@misc{noauthor_fairscale_nodate,
	title = {{FairScale} {Documentation} {\textbar} {FairScale} documentation},
	url = {https://fairscale.readthedocs.ioindex.html},
	abstract = {API docs for FairScale. FairScale is a PyTorch extension library for high performance and large scale training.},
	language = {en},
	urldate = {2025-01-22},
	author = {FairScale},
	year = {2021},
	file = {Snapshot:/home/atomwalk12/Zotero/storage/5MLSVDSS/latest.html:text/html},
}
@misc{noauthor_overview_nodate,
	title = {Overview {Lightning} {AI}},
	url = {https://lightning.ai/docs/overview/getting-started},
	abstract = {The all-in-one platform for AI development. Code together. Prototype. Train. Scale. Serve. From your browser - with zero setup. From the creators of PyTorch Lightning.},
	language = {en},
	year = {2010},
	author = {Lightning AI},
	urldate = {2025-01-22},
	file = {Snapshot:/home/atomwalk12/Zotero/storage/R3ZKYXXR/getting-started.html:text/html},
}


@misc{noauthor_vertex_nodate,
	title = {Vertex {AI} documentation},
	url = {https://cloud.google.com/vertex-ai/docs},
	abstract = {Documentation for Vertex AI, a suite of machine learning tools that enables developers to train high-quality models specific to their business needs.},
	language = {en},
	author = {Google Cloud},
	year = {2021},
	urldate = {2025-01-22},
	file = {Snapshot:/home/atomwalk12/Zotero/storage/59SL92FV/docs.html:text/html},
}

@misc{noauthor_googlecloudplatformvertex-ai-samples_2025,
	title = {{GoogleCloudPlatform}/vertex-ai-samples},
	copyright = {Apache-2.0},
	url = {https://github.com/GoogleCloudPlatform/vertex-ai-samples},
	abstract = {Notebooks, code samples, sample apps, and other resources that demonstrate how to use, develop and manage machine learning and generative AI workflows using Google Cloud Vertex AI.},
	urldate = {2025-01-22},
	publisher = {Google Cloud Platform},
	author = {Google Cloud Platform},
	month = jan,
	year = {2025},
	note = {original-date: 2021-05-27T00:06:43Z},
	keywords = {automl, colab, colab-enterprise, gemini, gemini-api, genai, generative-ai, google-cloud-platform, ml, mlops, model, model-garden, notebook, pipeline, predictions, samples, vertex-ai, vertexai, workbench},
}
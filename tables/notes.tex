% ======== CORE THEME 1: SCALABILITY ========
\section*{Scalability Relationships}
\subsection*{Motivation (MF1)}
\textbf{MF1. Scalability.} The connection is that scalability is a major shared motivating factor for both DNNs and GPU programming.
The increasing scale of data and complexity of DNNs necessitates scalable solutions. GPU programming is
motivated by providing the tools and optimizations needed to achieve this scalability, enabling DNNs to
handle larger workloads, improve productivity, and become more cost-effective.

\subsection*{Critical Factors (CF2)}
\textbf{CF2. Scalability.} Scalability is achieved by implementing a modular programming style. DNN frameworks
abstract away the distributed infrastructure complexity -- by being able to easily select distributed
strategies when executing the code -- while ML frameworks leveraging GPU acceleration to hide
low-level hardware details, allowing developers to focus on the application logic.
% Related to: MF1 (motivation), LF3 (limitation)

\subsection*{Limitations (LF3)}
\textbf{LF3. Communication Overhead and Scalability.} Another limitation is the communication overhead and its impact on scalability. Both areas struggle
with this, leading to performance bottlenecks that are challenging to overcome. There are not universally
optimal solutions, as the best approaches are dependent on model architectures and hardware configurations.
Community involvement is essential for progress as this can promote innovation.
% Emerges from: CF2 implementation challenges

% ======== CORE THEME 2: HARDWARE COMMUNICATION ======== 
\section{Hardware Communication Challenges}
\subsection*{Motivation (MF4)}
\textbf{MF4. Heterogeneous hardware.} While DNNs are motivated to use heterogenous hardware to ensure broader applicability and performance, GPU programming
acknowledge its importance by providing C APIs for CPU-GPU communication. Nonetheless, there do exist limitations
due to latency and sub-optimal bandwidth utilization in both domains. A related concern is that GPU libraries like cuDNN do not provide
integrated support for multi-GPU training, which must be achieved manually by the user. This highlights existing challenges
in both domains.

\subsection*{Critical Factors (CF4)}
\textbf{CF4. Network and hardware communication.} Multi-GPU training is particularly relevant in DNNs. There exist multiple algorithms to minimize
network latency, however GPU programming frameworks still struggle with multi-GPU communication
training, as libraries like cuDNN leave this management to the user. This indicates an ongoing
challenge in this area.
% Direct progression from MF4 motivation

% ======== CORE THEME 3: PERFORMANCE OPTIMIZATION ========
\section{Performance Considerations}
\subsection*{Motivation (MF2)}
\textbf{MF2. Complexity and performance.} The key shared motivator is to manage computational complexity while at the same time ensure higher accuracy
in common applications (i.e. NLP tasks). The increased accuracy is due to the guarantee that performance
is likely to increase thanks to the scaling laws that neural networks exhibit.
GPU programming directly provides the necessary primitives to facilitate the scaling laws and DNNs build on
top of them to improve performance.

\subsection*{Critical Factors (CF3)}
\textbf{CF3. Performance.} Performance is a key concern for both DNNs and GPU programming. DNNs are motivated by the need to
minimize network bandwidth latency and achieve better scalability, while GPU programming provides
optimized primitives by implementing large-matrix operations. To do this, thorough
knowledge of the GPU architecture is necessary.
% Implements MF2 motivations

\subsection*{Evaluation (EM2)}
\textbf{EM2. Model architectures.} In some evaluating scenarios, special care was payed to assess performance across different architectures.
For DNNs, this involves assessing scalability with increasingly complex model architectures. On the other hand,
for GPU programming, evaluation is geared towards optimizing performance by providing efficient primitives
for the most popular operations (convolutions, self-attention, fully connected layers, etc.).
% Assesses CF3 implementations

% ======== CORE THEME 4: COMMUNITY & ECOSYSTEM ========
\section{Community and Tooling Landscape}
\subsection*{Motivation (MF3, MF6, MF7)}
\textbf{MF3. Critical in many domains.}
GPU programming itself might be considered a more specialized domain, but its motivation is connected to the
critical applicability of DNNs across many fields. GPU programming enable DNNs to function efficiently in
these domains by providing the building blocks. A specific example involves Nvidia providing the optimized libraries,
as they can optimize the code better than the general community due to better knowledge of the underlying GPU architecture.

\textbf{MF6. Leveraging existing tools...}
DNN development benefits from open-source frameworks, promoting community-driven innovation. Conversely, GPU programming,
while often proprietary, builds on-top of other low-level libraries like cuBLAS. This shows a reliance on other
proprietary software, which ensures high-level performance. This can stifle innovation in the long term due
to the lack of competition.

\textbf{MF7. Cross-framework use...}
Open-source DNN framework aim for cross-framework compatibility and usability to foster community innovation.
GPU programming libraries show a trade-off between low-level, highly optimized primitives (like cuDNN), and offering
user-friendly interfaces (like CuPy and Torch7). Community involvement is important in both areas, however
less so for GPU programming due to the proprietary nature of the frameworks.

\subsection*{Critical Factors (CF1)}
\textbf{CF1. Paradigms, programming ease...}
DNNs are generally flexible and often use popular interpreted programming languages like Python to
promote ease of use. On the other hand, GPU programming usually relies on C++ and CUDA, which are
critical in areas where speed is a concern. Many GPU programming libraries provide bindings to
popular languages like Python to promote broad user involvement. This shows that community is
important in both areas, however GPU programming must rely on low-level languages to squeeze out
the best performance.
% Connects to MF7's cross-framework focus

% ======== CORE THEME 5: USABILITY & FLEXIBILITY ========
\section{Usability Tradeoffs}
\subsection*{Motivation (MF5)}
\textbf{MF5. Applications...} A shared motivation is to simplify development and improve the practical utility of both DNNs and GPU programming.
Both fields are driven by the need to make life easier for developers to leverage parallel hardware effectively,
and the motivation is to fulfill practical requirements of simplified deployment and reproducible research.

\subsection*{Critical Factors (CF5)}
\textbf{CF5. Ease of use and hardware flexibility...}
The main challenge here resides between ease of use to the developer and hardware flexibility.
Considering the broad community of developers, DNN libraries prioritize modularity and ease of
extension to facilitate broader community involvement. Some GPU programming libraries sacrifice ease of use for lower-level control
and potentially higher performance (cuDNN), while others strive for more user-friendly APIs (CuPy, Caffe).

\subsection*{Limitations (LF1)}
\textbf{LF1. Usability...}
DNN libraries attempt to improve usability through common APIs to facilitate broader
reproducibility. On the other hand, GPU programming frameworks have performance optimizations as
the main objective, which sacrifices ease of use and hides internal implementation details.
To circumvent this, many GPU frameworks provide profiling tools and specialized debuggers to aid
developer productivity.
% Result of CF5 tensions

% ======== CORE THEME 6: DEPLOYMENT & EVALUATION ========
\section{Deployment Strategies}
\subsection*{Evaluation (EM1, EM3, EM4)}
\textbf{EM1. Deployment...}
DNN libraries follow a staged deployment process for safe evaluation (i.e. Google products), which
offers a degree of safety in real world scenarios. Conversely, GPU programming frameworks are
designed with configurable deployment in mind, providing features like compile-time flags to
integrate more easily into ML frameworks.

\textbf{EM3. Task domains...}
Evaluation metrics are strongly dependent on the task domains. DNNs are assessed across
a broad range of deep learning applications, while GPU programming libraries are geared towards
a more narrow field  (like cuDNN), with others being more general purpose (like CuPy in scientific
computing).

\textbf{EM4. Evaluation...}
DNN evaluation focuses on overall performance gains and broad applicability in different domains (vision,
NLP, etc.). Conversely, GPU programming focuses on the potential performance gains achievable through
hardware-specific optimizations. Both fields are continuously refined when new hardware architectures
become available and new deep learning algorithms are developed.

% ======== CORE THEME 7: ALGORITHMIC CHALLENGES ========
\section{Algorithmic Limitations}
\subsection*{Limitations (LF2)}
\textbf{LF2. Algorithmic limitations...}
The primary algorithmic limitation concerns memory management in both domains.
DNNs pose problems related to data and model parallelism, while GPU programming faces issues
related to matrix multiplication algorithms and hyperparameter choices being suboptimal in some edge cases
(i.e. small batch size).
% Connects to CF3 (Performance)

% ======== UPDATED CROSS-THEME CONNECTIONS ========
\begin{itemize}
	\item \textbf{MF1 → CF2 → LF3}: Scalability motivation (MF1) leads to modular implementation approaches (CF2), ultimately resulting in communication limitations (LF3)
	\item \textbf{MF4 → CF4 → LF3}: Hardware heterogeneity motivation leads to multi-GPU challenges (CF4), ultimately creating scalability limitations (LF3)
	\item \textbf{MF2 → CF3 → EM2}: Performance motivation requires architectural knowledge (CF3), evaluated through model-specific metrics (EM2)
	\item \textbf{MF6 → CF1 → LF1}: Tool leverage (MF6) enables programming paradigms (CF1) but creates usability limitations (LF1)
	\item \textbf{MF5 → CF5 → EM1}: Application motivation (MF5) drives flexibility needs (CF5) evaluated through deployment (EM1)
	\item \textbf{MF3 → EM3}: Domain criticality (MF3) influences task-specific evaluation (EM3)
	\item \textbf{CF3 → LF2}: Performance requirements (CF3) reveal algorithmic limitations (LF2)
\end{itemize}

% ======== PRESERVED ORIGINAL STRUCTURE ========
\include{tables/translation_motivating_factors}
\include{tables/translation_critical_factors}
\include{tables/translation_evaluation_metrics}




% ======== ORIGINAL STRUCTURE ========
\subsection{M.6 -- Relationship between concepts}

\textbf{MF1. Scalability.}
The connection is that scalability is a major shared motivating factor for both DNNs and GPU programming.
The increasing scale of data and complexity of DNNs necessitates scalable solutions. GPU programming is
motivated by providing the tools and optimizations needed to achieve this scalability, enabling DNNs to
handle larger workloads, improve productivity, and become more cost-effective.

\textbf{MF2. Complexity and performance.}
The key shared motivator is to manage computational complexity while at the same time ensure higher accuracy
in common applications (i.e. NLP tasks). The increased accuracy is due to the guarantee that performance
is likely to increase thanks to the scaling laws that neural networks exhibit.
GPU programming directly provides the necessary primitives to facilitate the scaling laws and DNNs build on 
top of them to improve performance.

\textbf{MF3. Critical in many domains.}
GPU programming itself might be considered a more specialized domain, but its motivation is connected to the
critical applicability of DNNs across many fields. GPU programming enable DNNs to function efficiently in
these domains by providing the building blocks. A specific example involves Nvidia providing the optimized libraries,
as they can optimize the code better than the general community due to better knowledge of the underlying GPU architecture.

\textbf{MF4. Heterogeneous hardware.}
While DNNs are motivated to use heterogenous hardware to ensure broader applicability and performance, GPU programming
acknowledge its importance by providing C APIs for CPU-GPU communication. Nonetheless, there do exist limitations
due to latency and sub-optimal bandwidth utilization in both domains. A related concern is that GPU libraries like cuDNN do not provide
integrated support for multi-GPU training, which must be achieved manually by the user. This highlights existing challenges
in both domains.

\textbf{MF5. Applications.}
A shared motivation is to simplify development and improve the practical utility of both DNNs and GPU programming.
Both fields are driven by the need to make life easier for developers to leverage parallel hardware effectively,
and the motivation is to fulfill practical requirements of simplified deployment and reproducible research.

\textbf{MF6. Leveraging existing tools.}
DNN development benefits from open-source frameworks, promoting community-driven innovation. Conversely, GPU programming,
while often proprietary, builds on-top of other low-level libraries like cuBLAS. This shows a reliance on other
proprietary software, which ensures high-level performance. This can stifle innovation in the long term due
to the lack of competition.

\textbf{MF7. Cross-framework use.}
Open-source DNN framework aim for cross-framework compatibility and usability to foster community innovation.
GPU programming libraries show a trade-off between low-level, highly optimized primitives (like cuDNN), and offering
user-friendly interfaces (like CuPy and Torch7). Community involvement is important in both areas, however
less so for GPU programming due to the proprietary nature of the frameworks.

\paragraph{CF1. Paradigms, programming ease.}
DNNs are generally flexible and often use popular interpreted programming languages like Python to
promote ease of use. On the other hand, GPU programming usually relies on C++ and CUDA, which are
critical in areas where speed is a concern. Many GPU programming libraries provide bindings to
popular languages like Python to promote broad user involvement. This shows that community is
important in both areas, however GPU programming must rely on low-level languages to squeeze out
the best performance.

\textbf{CF2. Scalability.}
Scalability is achieved by implementing a modular programming style. DNN frameworks
abstract away the distributed infrastructure complexity -- by being able to easily select distributed
strategies when executing the code -- while ML frameworks leveraging GPU acceleration to hide
low-level hardware details, allowing developers to focus on the application logic.

\include{tables/translation_motivating_factors}
\include{tables/translation_critical_factors}
\include{tables/translation_evaluation_metrics}

\textbf{CF3. Performance.}
Performance is a key concern for both DNNs and GPU programming. DNNs are motivated by the need to
minimize network bandwidth latency and achieve better scalability, while GPU programming provides
optimized primitives by implementing large-matrix operations. To do this, thorough
knowledge of the GPU architecture is necessary.

\textbf{CF4. Network and hardware communication.}
Multi-GPU training is particularly relevant in DNNs. There exist multiple algorithms to minimize
network latency, however GPU programming frameworks still struggle with multi-GPU communication
training, as libraries like cuDNN leave this management to the user. This indicates an ongoing
challenge in this area.

\textbf{CF5. Ease of use and hardware flexibility.}
The main challenge here resides between ease of use to the developer and hardware flexibility.
Considering the broad community of developers, DNN libraries prioritize modularity and ease of
extension to facilitate broader community involvement. Some GPU programming libraries sacrifice ease of use for lower-level control
and potentially higher performance (cuDNN), while others strive for more user-friendly APIs (CuPy, Caffe).

\paragraph{EM1. Deployment.}
DNN libraries follow a staged deployment process for safe evaluation (i.e. Google products), which
offers a degree of safety in real world scenarios. Conversely, GPU programming frameworks are
designed with configurable deployment in mind, providing features like compile-time flags to
integrate more easily into ML frameworks.

\textbf{EM2. Model architectures.}
In some evaluating scenarios, special care was payed to assess performance across different architectures.
For DNNs, this involves assessing scalability with increasingly complex model architectures. On the other hand,
for GPU programming, evaluation is geared towards optimizing performance by providing efficient primitives
for the most popular operations (convolutions, self-attention, fully connected layers, etc.).

\textbf{EM3. Task domains.}
Evaluation metrics are strongly dependent on the task domains. DNNs are assessed across
a broad range of deep learning applications, while GPU programming libraries are geared towards
a more narrow field  (like cuDNN), with others being more general purpose (like CuPy in scientific
computing).

\textbf{EM4. Evaluation}
DNN evaluation focuses on overall performance gains and broad applicability in different domains (vision,
NLP, etc.). Conversely, GPU programming focuses on the potential performance gains achievable through
hardware-specific optimizations. Both fields are continuously refined when new hardware architectures
become available and new deep learning algorithms are developed.

\textbf{LF1. Usability.}
DNN libraries attempt to improve usability through common APIs to facilitate broader
reproducibility. On the other hand, GPU programming frameworks have performance optimizations as
the main objective, which sacrifices ease of use and hides internal implementation details.
To circumvent this, many GPU frameworks provide profiling tools and specialized debuggers to aid
developer productivity.

\textbf{LF2. Algorithmic limitations.}
The primary algorithmic limitation concerns memory management in both domains.
DNNs pose problems related to data and model parallelism, while GPU programming faces issues
related to matrix multiplication algorithms and hyperparameter choices being suboptimal in some edge cases
(i.e. small batch size).

\textbf{LF3. Communication Overhead and Scalability.}
Another limitation is the communication overhead and its impact on scalability. Both areas struggle
with this, leading to performance bottlenecks that are challenging to overcome. There are not universally
optimal solutions, as the best approaches are dependent on model architectures and hardware configurations.
Community involvement is essential for progress as this can promote innovation.

\clearpage
\onecolumn

{\footnotesize
	\begin{longtable}{|l|p{5cm}|p{5cm}|p{5cm}|}
		\caption{Translations of the evaluation metrics}\label{tab:translations_evaluation_metrics}   \\

		\toprule
		ID & Distributed Neural Networks & GPU Programming & Translation \\
		\midrule
		\endfirsthead

		\multicolumn{4}{c}{Table \thetable{} -- continued from previous page}           \\
		\toprule
		ID & Distributed Neural Networks & GPU Programming & Translation \\
		\midrule
		\endhead
		\midrule
    EM1
        & \textbullet\ Evaluation was initially performed behind closed doors for internal processes (speech recognition systems) and subsequently for external applications (Google Search). \cellref{D301}
        & \textbullet\ In many frameworks, the GPU libraries can be switched on and off at compile time using a single flag. \cellref{G1014} \newline
          \textbullet\ Some are designed for both research and industry, run on both CPU and GPU, have bindings for both Python and Matlab. For model architecture portability, Protocol Buffer files are used. \cellref{G3041}
        & \uline{\textbf{Deployment:}} \newline
          \textbullet\ Large companies like Google employ a staged deployment approach: first testing internally (e.g., speech recognition) before rolling out to external applications (e.g., Search), which enables safe evaluation of new technologies. \newline
          \textbullet\ Framework designers can rollback frameworks through compile-time flags. Although CUDA code is generally written in C, Python and Matlab bindings ensure portability.
        \\
        \midrule

    EM2
        & \textbullet\ The evaluation was done by scaling complex networks -- based on Mixture of Experts -- to 600B parameters using automatic sharding. \cellref{D305}
        & \textbullet\ The cuDNN library is assessed by measuring time and memory usage for convolutional layers. 
        Mini-batch performance is also assessed.
        Scalability is not as much of a concern as different GPU architectures are benchmarked instead of assessing performance across GPU clusters. \cellref{G3011}
        & \uline{\textbf{Model Architectures:}} \newline
          \textbullet\ Scalability testing is a concern for DNNs, as these are the type of problems that are encountered in the real world. \newline
          \textbullet\ However, for GPU programming, performance is measured by optimizing resource usage on a single GPU. 
          The difficulty stands in optimizing performance as new NN architectures are developed.
        \\
        \midrule

    EM3
        & \textbullet\ Evaluation tasks include: image classification \cellref{D303}, machine translation \cellref{D303}, \cellref{D305}, 
          NLP \cellref{D306} \cellref{D311}, RL \cellref{D308}.\newline
          \textbullet\ MoE models can be scaled up to 600 billion parameters for machine translation.
        & \textbullet\ CuDNN can be used in deep learning, CNNs, speech and language. \cellref{G3012} \newline
          \textbullet\ CuPy can be extended to scientific computing and probabilistic modelling. \cellref{G3061}
        & \uline{\textbf{Task Doamins:}} \newline
          \textbullet\ DNN libraries focus on solving deep learning tasks. \newline
          \textbullet\ CuDNN was designed specifically for deep learning. However, CuPy can be used in broader domains.
        \\
        \midrule

    EM4
        & \textbullet\ Impressive improvements in performance over older methods. \cellref{D304} \newline
          \textbullet\ Evaluation is done against vision and NLP tasks \cellref{D306} and RL \cellref{D308}. \newline
          \textbullet\ Wikipedia dataset is often used for evaluation. \cellref{D307} \newline
          \textbullet\ Scaling gives consistent improvements in performance. \cellref{D311}
        & \textbullet\ The evaluation metric involves assessing performance and matrix multiplication. The speedup is impressive reaching up to 36\% improvements. \cellref{G3013} \newline
          \textbullet\ CuDNN is assessed against popular libraries like cuda-convnet2 and Caffe. It achieves portability against different GPU architectures. \cellref{G3013} \newline
          \textbullet\ Qualitative evaluation can offer valuable insights into the performance. \cellref{G3051}
        & \uline{\textbf{Evaluation:}} \newline
          \textbullet\ Potential for greater gains through hardware-specific optimizations. \newline
          \textbullet\ Broad applicability of DNNs, while GPU programming is focused on improving specific architectures. \newline
          \textbullet\ Consistent scaling suggests promising future applications in DNNs, while advancements in GPU programming focus on specific architectures.
        \\
        \midrule
		\bottomrule
	\end{longtable}
}

\twocolumn


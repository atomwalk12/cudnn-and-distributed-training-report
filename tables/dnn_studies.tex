\begin{table*}[th!]
	\centering
	\caption{The DNN papers included in the review. 1 (data), 2 (model), 3 (pipeline)}
	\label{tab:dnn_papers}
	\begin{tabular}{llp{8.4cm}lllc}
		\hline
		\small \textbf{\#} & \small \textbf{Ref.}                    & \small \textbf{Title}                                                                                                               & \small \textbf{Type} & \small \textbf{Year} & \small \textbf{Citations} & \small \textbf{Stars}                                                \\[1ex]
		\hline
		\small DNN1        & \small \cite{abadi_tensorflow_2016}     & \small TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems                                                & \small Data          & \small 2016          & \small 9998               & \small 187k \cite{abadi_tensorflow_2015}                             \\[1ex]
		\small DNN2        & \small \cite{chen_mxnet_2015}           & \small MXNet: A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems                               & \small Hybrid        & \small 2015          & \small 2214               & \small 20.8k \cite{noauthor_apachemxnet_2025}                        \\[1ex]
		\small DNN3        & \small \cite{huang_gpipe_2019}          & \small GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism                                                & \small Pipeline      & \small 2018          & \small 1446               & \small 2.8k \cite{noauthor_tensorflowlingvo_2025}                    \\[1ex]
		\small DNN4        & \small \cite{jiang_unified_nodate}      & \small BytePS: A Unified Architecture for Accelerating Distributed DNN Training in Heterogeneous GPU/CPU Clusters                   & \small Data          & \small 2020          & \small 338                & \small 3.7k \cite{noauthor_bytedancebyteps_2025}                     \\[1ex]
		\small DNN5        & \small \cite{lepikhin_gshard_2020}      & \small GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding                                             & \small Model         & \small 2020          & \small 931                & \small 2.8k \cite{noauthor_tensorflowlingvo_2025}                    \\[1ex]
		\small DNN6        & \small \cite{li_pytorch_2020}           & \small PyTorch Distributed: Experiences on Accelerating Data Parallel Training                                                      & \small Hybrid        & \small 2020          & \small 175                & \small 86.1k \cite{noauthor_pytorchpytorch_nodate}                   \\[1ex]
		\small DNN7        & \small \cite{li_colossal-ai_2023}       & \small Colossal AI: A Unified Deep Learning System for Large-Scale Parallel Training                                                & \small Hybrid        & \small 2023          & \small 118                & \small 39k \cite{noauthor_hpcaitechcolossalai_2025}                  \\[1ex]
		\small DNN8        & \small \cite{moritz_ray_2018}           & \small Ray: A distributed Framework for Emerging AI Applications                                                                    & \small Hybrid        & \small 2018          & \small 1108               & \small 35k \cite{noauthor_ray-projectray_2025}                       \\[1ex]
		\small DNN9        & \small \cite{rasley_deepspeed_2020}     & \small DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters                        & \small Hybrid        & \small 2020          & \small 1059               & \small 36.3k \cite{noauthor_microsoftdeepspeed_2025}                 \\[1ex]
		\small DNN10       & \small \cite{sergeev_horovod_2018}      & \small Horovod: fast and easy distributed deep learning in TensorFlow                                                               & \small Data          & \small 2018          & \small 1152               & \small 14.3k \cite{noauthor_horovodhorovod_2025}                     \\[1ex]
		\small DNN11       & \small \cite{shoeybi_megatron-lm_2020}  & \small Megatron-LM: Training Multi-Billion Parameter Models for Natural Language Processing                                         & \small Hybrid        & \small 2020          & \small 1578               & \small 11.2k \cite{noauthor_nvidiamegatron-lm_2025}                  \\[1ex]
		\small DNN12       & \small \cite{wolf_huggingfaces_2020}    & \small HuggingFace's Transformers: State-of-the-art Natural Language Processing                                                     & \small Data          & \small 2020          & \small 1444               & \small 8.2k \cite{noauthor_huggingfaceaccelerate_2025}               \\[1ex]
		\small DNN13       & \small \cite{noauthor_overview_nodate}  & \small Pytorch Lightning: The lightweight PyTorch wrapper for high-performance AI research. Scale your models, not the boilerplate. & \small Data          & \small 2019          & \small N/A                & \small 28.8k \cite{falcon_pytorch_2019}                              \\[1ex]
		\small DNN14       & \small \cite{noauthor_fairscale_nodate} & \small FairScale:  A general purpose modular PyTorch library for high performance and large scale training                          & \small Hybrid        & \small 2021          & \small N/A                & \small 3.2k \cite{FairScale2021}                                     \\[1ex]
		\small DNN15       & \small \cite{noauthor_amazon_nodate}    & \small Amazon SageMaker Platform                                                                                                    & \small Data          & \small 2017          & \small N/A                & \small 10.3k \cite{noauthor_awsamazon-sagemaker-examples_2025}       \\[1ex]
		\small DNN16       & \small \cite{sdgilley_azure_nodate}     & \small Microsoft AzureML Platform                                                                                                   & \small Data          & \small 2021          & \small N/A                & \small 1.8k \cite{noauthor_azureazureml-examples_2025}               \\[1ex]
		\small DNN17       & \small \cite{noauthor_vertex_nodate}    & \small Google Vertex AI Platform                                                                                                    & \small Data          & \small 2021          & \small N/A                & \small 178 \cite{noauthor_googlecloudplatformvertex-ai-samples_2025} \\[1ex]
		\small DNN18       & \small \cite{frostig_compiling_nodate}  & \small Jax: Compiling machine learning programs via high-level tracing                                                              & \small Hybrid        & \small 2018          & \small N/A                & \small 31k \cite{noauthor_jax-mljax_2025}                            \\[1ex]
		\hline
	\end{tabular}
\end{table*}

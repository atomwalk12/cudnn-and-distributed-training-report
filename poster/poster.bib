

@misc{noauthor_wayback_2023,
	title = {Stanford University: How to Read a Paper},
	howpublished = {\url{https://web.archive.org/web/20231216162503/https://web.stanford.edu/class/ee384m/Handouts/HowtoReadPaper.pdf}},
	urldate = {2025-02-24},
	file = {PDF Snapshot:/home/atomwalk12/Zotero/storage/UY4A3EY9/2023 - Wayback Machine.pdf:application/pdf},
}


@misc{nguyen_machine_2019,
	title = {Machine {Learning} and {Deep} {Learning} frameworks and libraries for large-scale data mining: a survey},
	volume = {52},
	issn = {1573-7462},
	shorttitle = {Machine {Learning} and {Deep} {Learning} frameworks and libraries for large-scale data mining},
	url = {https://doi.org/10.1007/s10462-018-09679-z},
	doi = {10.1007/s10462-018-09679-z},
	abstract = {The combined impact of new computing resources and techniques with an increasing avalanche of large datasets, is transforming many research areas and may lead to technological breakthroughs that can be used by billions of people. In the recent years, Machine Learning and especially its subfield Deep Learning have seen impressive advances. Techniques developed within these two fields are now able to analyze and learn from huge amounts of real world examples in a disparate formats. While the number of Machine Learning algorithms is extensive and growing, their implementations through frameworks and libraries is also extensive and growing too. The software development in this field is fast paced with a large number of open-source software coming from the academy, industry, start-ups or wider open-source communities. This survey presents a recent time-slide comprehensive overview with comparisons as well as trends in development and usage of cutting-edge Artificial Intelligence software. It also provides an overview of massive parallelism support that is capable of scaling computation effectively and efficiently in the era of Big Data.},
	language = {en},
	number = {1},
	urldate = {2025-01-25},
	journal = {Artificial Intelligence Review},
	author = {Nguyen, Giang and Dlugolinsky, Stefan and Bobák, Martin and Tran, Viet and López García, Álvaro and Heredia, Ignacio and Malík, Peter and Hluchý, Ladislav},
	month = jun,
	year = {2019},
	keywords = {Artificial Intelligence, Graphics processing unit (GPU), Parallel processing, Deep Learning, Machine Learning, Artificial Intelligence software, Intensive computing, Large-scale data mining, to-check},
	pages = {77--124},
	file = {Full Text PDF:/home/atomwalk12/Zotero/storage/TRD4F4EK/Nguyen et al. - 2019 - Machine Learning and Deep Learning frameworks and libraries for large-scale data mining a survey.pdf:application/pdf},
}

@misc{dehghani_distributed_2023,
	title = {From distributed machine to distributed deep learning: a comprehensive survey},
	volume = {10},
	issn = {2196-1115},
	shorttitle = {From distributed machine to distributed deep learning},
	url = {https://doi.org/10.1186/s40537-023-00829-x},
	doi = {10.1186/s40537-023-00829-x},
	abstract = {Artificial intelligence has made remarkable progress in handling complex tasks, thanks to advances in hardware acceleration and machine learning algorithms. However, to acquire more accurate outcomes and solve more complex issues, algorithms should be trained with more data. Processing this huge amount of data could be time-consuming and require a great deal of computation. To address these issues, distributed machine learning has been proposed, which involves distributing the data and algorithm across several machines. There has been considerable effort put into developing distributed machine learning algorithms, and different methods have been proposed so far. We divide these algorithms in classification and clustering (traditional machine learning), deep learning and deep reinforcement learning groups. Distributed deep learning has gained more attention in recent years and most of the studies have focused on this approach. Therefore, we mostly concentrate on this category. Based on the investigation of the mentioned algorithms, we highlighted the limitations that should be addressed in future research.},
	language = {en},
	number = {1},
	urldate = {2025-01-05},
	journal = {Journal of Big Data},
	author = {Dehghani, Mohammad and Yazdanparast, Zahra},
	month = oct,
	year = {2023},
	keywords = {Artificial Intelligence, Artificial intelligence, Data-parallelism, Distributed deep learning, Distributed machine learning, Ditributed reinforcement learning, Machine learning, Model-parallelism},
	pages = {158},
	file = {Full Text PDF:/home/atomwalk12/Zotero/storage/FQPFJL85/Dehghani and Yazdanparast - 2023 - From distributed machine to distributed deep learning a comprehensive survey.pdf:application/pdf},
}

@misc{chahal_hitchhikers_2018,
	title = {A {Hitchhiker}'s {Guide} {On} {Distributed} {Training} of {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1810.11787},
	doi = {10.48550/arXiv.1810.11787},
	abstract = {Deep learning has led to tremendous advancements in the ﬁeld of Artiﬁcial Intelligence. One caveat however is the substantial amount of compute needed to train these deep learning models. Training a benchmark dataset like ImageNet on a single machine with a modern GPU can take upto a week, distributing training on multiple machines has been observed to drastically bring this time down. Recent work has brought down ImageNet training time to a time as low as 4 minutes by using a cluster of 2048 GPUs. This paper surveys the various algorithms and techniques used to distribute training and presents the current state of the art for a modern distributed training framework. More speciﬁcally, we explore the synchronous and asynchronous variants of distributed Stochastic Gradient Descent, various All Reduce gradient aggregation strategies and best practices for obtaining higher throughout and lower latency over a cluster such as mixed precision training, large batch training and gradient compression.},
	language = {en},
	urldate = {2024-12-27},
	publisher = {arXiv},
	author = {Chahal, Karanbir and Grover, Manraj Singh and Dey, Kuntal},
	month = oct,
	year = {2018},
	keywords = {Computer Science - Machine Learning, Computer Science - Distributed, Parallel, and Cluster Computing, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {PDF:/home/atomwalk12/Zotero/storage/7HZX2TRH/Chahal et al. - 2018 - A Hitchhiker's Guide On Distributed Training of Deep Neural Networks.pdf:application/pdf},
}

@misc{berloco_systematic_2022,
	address = {Cham},
	title = {A {Systematic} {Review} of {Distributed} {Deep} {Learning} {Frameworks} for {Big} {Data}},
	isbn = {978-3-031-13832-4},
	doi = {10.1007/978-3-031-13832-4_21},
	abstract = {Traditional Machine Learning and Deep Learning techniques (data acquisition, preparation, model training and evaluation) take a lot of computational resources and time to produce even a simple prediction model, especially when implemented on a single machine. Intuitively, the demand for computational requirements is higher in case of management of Big Data and training of complex models. Thus, a paradigm shift from a single machine to a BD-oriented approach is required for making traditional Machine Learning and Deep Learning techniques fit to Big Data. In particular, it emerges the need for developing and deploying Big Data Analytics Infrastructures on cluster of machines. In this context, main features and principles of Distributed Deep Learning frameworks are here discussed. The main contribution of this paper is a systematic review of proposed solutions, aimed at investigating under a unifying lens their foundational elements, functional features and capabilities, despite the inherent literature fragmentation. To this, we conducted a literature search in Scopus and Google Scholar. This review also compares Distributed Deep Learning approaches according to more technical facets: implemented of parallelism techniques, supported hardware, model parameters sharing modalities, computation modalities for stochastic gradient descent and compatibility with other frameworks.},
	language = {en},
	booktitle = {Intelligent {Computing} {Methodologies}},
	publisher = {Springer International Publishing},
	author = {Berloco, Francesco and Bevilacqua, Vitoantonio and Colucci, Simona},
	editor = {Huang, De-Shuang and Jo, Kang-Hyun and Jing, Junfeng and Premaratne, Prashan and Bevilacqua, Vitoantonio and Hussain, Abir},
	year = {2022},
	keywords = {Big Data, Distributed Deep Learning, Distributed Deep Learning Frameworks, Parallel computing},
	pages = {242--256},
	file = {Full Text PDF:/home/atomwalk12/Zotero/storage/PJJTIG7E/Berloco et al. - 2022 - A Systematic Review of Distributed Deep Learning Frameworks for Big Data.pdf:application/pdf},
}